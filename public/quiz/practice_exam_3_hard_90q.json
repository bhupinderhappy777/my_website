{
  "1. Cloud Architecture": [
    {
      "q": "A fintech application processes 100K transactions/second with strict consistency requirements. Current architecture uses single-region deployment with synchronous replication to standby causing 150ms write latency (users complain). Business expands globally requiring <50ms latency worldwide while maintaining consistency. What architecture achieves both requirements?",
      "options": [
        "Deploy to all regions with eventual consistency",
        "Implement command query responsibility segregation (CQRS) with event sourcing: writes to nearest region (low latency), event stream provides global consistency, read replicas serve queries",
        "Use distributed consensus protocol (Raft/Paxos) across regions",
        "Accept higher latency for consistency"
      ],
      "answer": 1,
      "explanation": "CQRS with event sourcing separates write (command) and read (query) paths. Writes to local region provide low latency, immutable event log ensures consistency across regions, materialized views serve reads. Eventual consistency violates consistency requirement. Distributed consensus across regions has high latency. Accepting latency violates user experience requirement. Objective 1.1"
    },
    {
      "q": "A video transcoding service processes 50TB/day of uploads. Current architecture: users upload to origin, files transferred to processing cluster (1 hour delay), transcoded, transferred to CDN. Total time to availability: 3 hours. Business requires 15-minute time-to-availability. What architectural change meets requirement?",
      "options": [
        "Use faster network connections",
        "Implement edge processing: transcode at upload locations, progressive upload with parallel processing, direct-to-CDN publishing bypassing central cluster",
        "Add more processing capacity",
        "Use hardware transcoding acceleration"
      ],
      "answer": 1,
      "explanation": "Edge processing eliminates transfer delays: transcode where uploaded (regional processing), progressive upload enables starting transcode before complete upload, publish directly to CDN from edge. Reduces time from 3 hours to 15 minutes. Network speed helps but doesn't eliminate architecture bottleneck. More capacity doesn't reduce transfer time. Hardware acceleration helps processing but not transfer. Objective 1.6"
    },
    {
      "q": "A machine learning platform trains models using federated learning across 1000 edge devices (hospitals, factories). Models must improve globally while data stays at source due to privacy regulations. Current centralized approach violates compliance. What architecture enables federated learning at scale?",
      "options": [
        "Copy all data to central cloud for training",
        "Implement federated learning framework: train local models on devices, aggregate model updates not data, differential privacy for gradient sharing, secure aggregation protocol",
        "Use encrypted data transfer for centralized training",
        "Deploy identical models without learning"
      ],
      "answer": 1,
      "explanation": "Federated learning trains models locally on private data, shares only model updates (gradients/weights) not raw data, central aggregation combines updates, differential privacy adds noise to updates preventing data leakage. Meets privacy and learning requirements. Centralizing data violates compliance. Encryption doesn't solve data leaving source. Static models don't improve. Objective 1.1"
    },
    {
      "q": "A streaming analytics platform processes 10 million events/second with 100ms latency requirement. Current lambda architecture has batch layer (accurate, 1 hour lag) and speed layer (approximate, real-time) causing complexity and inconsistencies. What modern architecture simplifies while maintaining performance?",
      "options": [
        "Keep lambda architecture but optimize",
        "Implement kappa architecture: single stream processing pipeline with replayable log, exactly-once semantics, handles both real-time and batch with same code path",
        "Use only batch processing with lower latency",
        "Separate real-time and batch completely"
      ],
      "answer": 1,
      "explanation": "Kappa architecture uses single processing path with replayable event log (Kafka), handles real-time and historical processing identically, exactly-once processing ensures consistency, simplifies by eliminating dual code paths. Meets latency with consistency. Lambda optimization doesn't address fundamental complexity. Batch-only can't meet 100ms. Separating increases complexity. Objective 1.1"
    },
    {
      "q": "A gaming platform experiences flash crowds: new game release causes 100x traffic spike in 5 minutes then returns to normal. Auto-scaling takes 10 minutes (cold starts, health checks) causing errors during spike. Reserved capacity for peak wasteful. What handles flash crowds cost-effectively?",
      "options": [
        "Always provision for 100x capacity",
        "Implement multi-tier scaling: serverless functions (instant scale) handle spike and queue requests, containers drain queue (warm pool with faster scaling), database uses read-through caching",
        "Use only serverless functions for everything",
        "Implement queue with delayed processing"
      ],
      "answer": 1,
      "explanation": "Hybrid architecture: serverless absorbs spike instantly (no cold start, scales to thousands), queues requests for processing, warm container pool processes queue with faster scale-out than cold start, aggressive caching protects database. Cost-effective and responsive. Always 100x capacity expensive. All-serverless has cold starts for sustained load. Delayed processing poor UX. Objective 1.5"
    },
    {
      "q": "A content delivery network serves 5PB of video content globally. Storage costs $100K/month. Analysis shows 70% of content accessed less than once per month but must remain instantly available (long-tail). What storage strategy reduces costs while maintaining instant availability?",
      "options": [
        "Delete unpopular content",
        "Implement intelligent tiering with automatic lifecycle management: hot storage for frequently accessed, warm storage for infrequent with instant retrieval (S3 Intelligent-Tiering), multi-region replication only for popular content",
        "Compress all content more aggressively",
        "Reduce video quality for unpopular content"
      ],
      "answer": 1,
      "explanation": "Intelligent tiering automatically moves objects between storage classes based on access patterns without retrieval delays. Frequent access in hot tier, infrequent in warm tier (cheaper, still instant), rarely accessed in cold tier. Multi-region replication only for popular content reduces transfer costs. Deleting violates availability. Compression limited benefit for video. Quality reduction poor UX. Objective 1.3"
    },
    {
      "q": "A distributed database uses quorum reads and writes (W=2, R=2, N=3 replicas). Under network partition, one datacenter isolated with 1 replica, two datacenters have 2 replicas. System continues processing. After partition heals, data conflicts detected. What caused conflicts despite quorum?",
      "options": [
        "Quorum configuration is wrong",
        "Sloppy quorums: during partition, system accepted writes to available nodes even without strict quorum, using hinted handoff, causing split-brain scenario",
        "Database bug",
        "Network issues corrupted data"
      ],
      "answer": 1,
      "explanation": "Sloppy quorum allows writes to any available nodes during partition (availability over consistency), uses hinted handoff for later reconciliation. Both sides of partition accepted writes thinking they had quorum. Results in conflicts. Strict quorum would reject writes on minority side. Config allows sloppy quorum. Not bug. Network partition not corruption. Objective 1.2"
    },
    {
      "q": "A real-time bidding platform requires processing 5 million bid requests/second with 5ms latency budget. Current architecture: load balancer → stateless bid servers → Redis cluster → DynamoDB. P99 latency: 15ms. Profiling shows 3ms in serialization/deserialization. What optimization achieves 5ms target?",
      "options": [
        "Use faster serialization format (Protocol Buffers), implement connection pooling, enable response compression, use async I/O, optimize Redis pipeline batching",
        "Add more servers",
        "Use faster CPUs",
        "Increase network bandwidth"
      ],
      "answer": 0,
      "explanation": "Multiple optimizations: Protocol Buffers reduces serialization time vs JSON (10x faster), connection pooling eliminates TCP handshake overhead, compression reduces network time, async I/O prevents blocking, Redis pipelining batches commands. Combined effect meets 5ms target. More servers don't reduce per-request latency. CPU helps but insufficient alone. Network not primary bottleneck. Objective 1.3"
    },
    {
      "q": "A microservices architecture has service dependencies forming a complex graph. Service A depends on B, C; B depends on D, E; C depends on E, F. During E failures, services A, B, C all fail (cascading). What pattern prevents cascading failures while maintaining functionality?",
      "options": [
        "Make all services independent",
        "Implement circuit breaker pattern with graceful degradation: services detect E failure, stop calling E, return cached/default responses, continue partial functionality without E",
        "Duplicate service E",
        "Add retry logic with exponential backoff"
      ],
      "answer": 1,
      "explanation": "Circuit breaker with graceful degradation: detect E failure through error rate threshold, open circuit (stop calling E, fail fast), services provide degraded functionality using cache/defaults (A works without real-time E data), system remains partially operational. True independence impossible with business logic. Duplication doesn't prevent E failure. Retries amplify load on failing service. Objective 1.1"
    },
    {
      "q": "A time-series database stores IoT sensor data: 1 billion devices, 1 data point/minute, 90-day retention. Current approach: write each data point individually causing database overload (1.4 billion writes/day). What ingestion pattern handles this scale efficiently?",
      "options": [
        "Use larger database instance",
        "Implement write batching: buffer data points for 5 minutes, batch insert 300 points/write, use time-series compression, partition by device_id ranges and time windows",
        "Sample data (write every 10th point)",
        "Use NoSQL instead"
      ],
      "answer": 1,
      "explanation": "Batching dramatically reduces write load: 5-minute buffer creates batches of ~300 points per device (reduce writes 300x), time-series compression (delta encoding), partitioning by device_id and time enables parallel ingestion and efficient queries. Larger instance doesn't address write volume. Sampling loses data. NoSQL faces same write scaling challenge. Objective 1.3"
    },
    {
      "q": "A financial services application requires audit trail showing who accessed what data when, with tamper-proof guarantee for regulatory compliance. Current database audit logs can be modified by administrators. What architecture provides immutable audit trail?",
      "options": [
        "Encrypt audit logs",
        "Implement append-only audit log with cryptographic linking (blockchain-style): each entry hashes previous entry, written to immutable storage (WORM), periodic merkle root published to external ledger",
        "Use read-only database replicas for logs",
        "Write logs to multiple locations"
      ],
      "answer": 1,
      "explanation": "Append-only log with cryptographic chaining: each audit entry includes hash of previous entry (tamper evident), immutable storage prevents deletion, external merkle root publication provides verification. Administrators cannot modify without detection. Encryption doesn't prevent deletion/modification by admins. Read-only replicas can be altered by admin. Multiple locations don't prevent tampering. Objective 1.4"
    },
    {
      "q": "A recommendation engine uses collaborative filtering requiring computing similarity between all user pairs (1 million users = 500 billion comparisons). Current batch processing takes 12 hours. Business needs hourly updates. What architecture enables real-time recommendations at this scale?",
      "options": [
        "Use more powerful computing cluster",
        "Implement approximate algorithms: locality-sensitive hashing for approximate nearest neighbors, matrix factorization for dimensionality reduction, incremental updates instead of full recomputation, pre-computed similarity for popular items",
        "Reduce number of users considered",
        "Use simpler recommendation algorithm"
      ],
      "answer": 1,
      "explanation": "Approximate algorithms enable scale: LSH finds similar users without all-pairs comparison (reduces from O(n²) to O(n)), matrix factorization reduces dimensions, incremental updates process only changes (not full recomputation), pre-compute popular items. Achieves hourly updates. More compute hits economic limits. Reducing users limits quality. Simpler algorithms less effective. Objective 1.3"
    },
    {
      "q": "A multi-tenant SaaS application stores tenant data in shared tables. Tenant A (large enterprise) issues expensive query scanning 1TB causing performance degradation for all tenants. Tenant isolation and fair resource allocation required. What architecture enforces resource limits per tenant?",
      "options": [
        "Add database indexes",
        "Implement resource management: query governor with per-tenant CPU/IO limits, workload classification and priority queuing, separate resource pools for large tenants, query timeout enforcement",
        "Terminate expensive queries automatically",
        "Use dedicated database per tenant"
      ],
      "answer": 1,
      "explanation": "Multi-layer resource management: query governor limits per-tenant resources (CPU cycles, IO operations), workload classification assigns queries to pools (large tenant separate pool), priority queuing ensures fair access, timeouts prevent runaway queries. Indexes help but don't prevent resource exhaustion. Terminating queries poor UX without warning. Dedicated databases expensive and complex. Objective 1.1"
    },
    {
      "q": "A disaster recovery architecture uses backup replication across regions. During DR test, discovered application requires 50 third-party integrations (payment processors, email services, analytics) configured with region-specific endpoints. Manual reconfiguration took 6 hours violating 1-hour RTO. What prevents this in actual DR?",
      "options": [
        "Document all integration endpoints",
        "Implement service discovery and configuration management: externalized configuration for all integrations, environment-aware service discovery, automated endpoint switching via DNS/service mesh, integration health checks and automatic failover",
        "Pre-configure all integrations in DR region",
        "Use global endpoints only"
      ],
      "answer": 1,
      "explanation": "Automated configuration management: externalize all endpoints (no hard-coding), service discovery resolves region-appropriate endpoints automatically, DNS failover or service mesh routes to DR endpoints, health checks detect availability. Eliminates manual reconfiguration. Documentation doesn't automate. Pre-configuring requires maintenance. Global endpoints may not exist for all services. Objective 1.2"
    },
    {
      "q": "A search engine indexes 100 billion documents. Users expect instant autocomplete suggestions (<50ms). Current trie-based approach loads entire index in memory requiring 500GB RAM across distributed cache cluster costing $50K/month. What architecture reduces cost while maintaining performance?",
      "options": [
        "Use disk-based storage instead of RAM",
        "Implement hybrid approach: hot prefix cache in memory (top 1000 prefixes = 90% queries), bloom filters to avoid unnecessary lookups, SSD-backed storage with learned indexes for long-tail, query result caching",
        "Reduce index size by removing rare terms",
        "Use compression on in-memory data"
      ],
      "answer": 1,
      "explanation": "Tiered approach: hot prefix cache handles majority of queries from memory (small footprint), bloom filters quickly reject non-existent terms, SSD with learned indexes handles long-tail efficiently, result caching for popular queries. 10x cost reduction. Disk alone too slow for <50ms. Removing terms reduces quality. Compression helps but insufficient for 10x reduction. Objective 1.3"
    },
    {
      "q": "A CI/CD platform builds container images for 1000 microservices. Base image vulnerabilities require rebuilding all services. Current process: sequential builds taking 20 hours. Security requires patching within 4 hours of CVE disclosure. What build architecture enables 4-hour rebuild time?",
      "options": [
        "Faster build servers",
        "Implement build dependency graph with parallel execution: identify services using vulnerable base image, build independent services in parallel (up to 100 concurrent), smart caching reuses layers, incremental builds only rebuild changed services",
        "Reduce number of microservices",
        "Use pre-built images"
      ],
      "answer": 1,
      "explanation": "Intelligent parallel builds: dependency graph identifies build order, parallel execution (100+ builds simultaneously), layer caching reuses unchanged layers, only rebuild affected services. Reduces 20 hours to <4 hours. Faster servers help but insufficient for 5x speedup. Reducing services wrong direction. Pre-built images don't address vulnerability patching. Objective 1.5"
    },
    {
      "q": "A blockchain-based application requires consensus across 1000 nodes. Current Proof-of-Work consensus takes 10 minutes per block with high energy cost. Application needs <1 second finality. What consensus mechanism meets requirements?",
      "options": [
        "Use Proof-of-Stake instead of Proof-of-Work",
        "Implement practical Byzantine Fault Tolerance (pBFT) or similar: leader-based consensus with 2f+1 nodes for f failures, <1 second finality, or use Proof-of-Stake with finality gadget",
        "Reduce number of nodes",
        "Use centralized consensus"
      ],
      "answer": 1,
      "explanation": "pBFT or modern consensus (Tendermint, HotStuff) provides <1 second finality with Byzantine fault tolerance. Leader-based with validator rotation achieves fast consensus. PoS with finality gadget (Casper) also works. Traditional PoW too slow. PoS alone may not guarantee <1s. Reducing nodes weakens decentralization. Centralized defeats blockchain purpose. Objective 1.1"
    },
    {
      "q": "A genomics research platform analyzes DNA sequences (3 billion base pairs per genome, 100K genomes). Current approach stores full sequences requiring 300TB. Queries access specific gene regions (<0.1% of data). Storage costs $50K/month. What storage optimization reduces cost while enabling efficient queries?",
      "options": [
        "Compress all sequences",
        "Implement columnar storage with predicate pushdown and partitioning: store by gene region (chromosome, position), columnar format enables reading only queried regions, aggressive compression (reference-based), indexed by genomic coordinates",
        "Use object storage",
        "Sample subset of genomes"
      ],
      "answer": 1,
      "explanation": "Genomics-optimized storage: partition by genomic coordinates (chromosome, position ranges), columnar storage reads only queried columns, reference-based compression (store differences from reference genome, 1000x compression), indexing enables efficient region queries. Reduces storage by 100x. Simple compression insufficient. Object storage alone doesn't optimize queries. Sampling reduces research value. Objective 1.3"
    },
    {
      "q": "A global CDN network experiences cache stampede: popular content expires simultaneously, 1 million requests hit origin server within seconds causing outage. What prevents cache stampede while maintaining fresh content?",
      "options": [
        "Increase cache expiration time",
        "Implement probabilistic early expiration with request coalescing: refresh cache before expiration based on probability increasing as TTL approaches zero, coalesce multiple requests into single origin fetch, negative caching for errors",
        "Remove expiration entirely",
        "Use longer TTLs for popular content"
      ],
      "answer": 1,
      "explanation": "Probabilistic early expiration prevents synchronized expiration: probability of early refresh increases as TTL approaches zero (spreads refresh load), request coalescing ensures single origin request for multiple cache misses (thundering herd protection), negative caching prevents repeated error requests. Increasing TTL reduces freshness. No expiration serves stale content. Longer TTL for popular content still has stampede at expiration. Objective 1.6"
    },
    {
      "q": "A multi-cloud strategy deploys identical applications on AWS, Azure, and GCP for redundancy. Each cloud uses provider-specific services (AWS RDS, Azure SQL, GCP CloudSQL). Maintaining three different implementations costly and error-prone. What architecture enables true multi-cloud portability?",
      "options": [
        "Use only provider-specific services with custom implementations",
        "Implement abstraction layer with cloud-agnostic interfaces: containerized applications, Kubernetes for orchestration, cloud-agnostic storage API, infrastructure as code with provider abstraction (Terraform), standardized monitoring",
        "Choose single cloud provider",
        "Use lowest common denominator services"
      ],
      "answer": 1,
      "explanation": "Abstraction enables portability: containers package apps consistently, Kubernetes provides consistent orchestration, storage abstraction (S3 API compatible across clouds), IaC with provider modules abstracts differences, unified monitoring. Single codebase deploys anywhere. Provider-specific services create lock-in. Single provider removes redundancy benefit. Lowest common denominator limits features. Objective 1.1"
    },
    {
      "q": "A high-frequency trading system requires processing market data with <1ms latency. Current architecture: message queue → processing → database. Queue adds 2-3ms latency violating requirement. System must handle bursts (100K messages/second) and maintain order. What architecture achieves <1ms latency?",
      "options": [
        "Use faster message queue",
        "Implement kernel bypass networking with DPDK, memory-mapped ring buffers for zero-copy data transfer, lock-free data structures, NUMA-aware processing, direct memory access to NICs",
        "Remove queue and process inline",
        "Use UDP instead of TCP"
      ],
      "answer": 1,
      "explanation": "Ultra-low latency requires kernel bypass: DPDK bypasses kernel network stack (reduces latency 10x), memory-mapped ring buffers enable zero-copy transfer, lock-free structures avoid synchronization overhead, NUMA-aware allocation reduces memory latency, DMA bypasses CPU for network operations. Achieves <1ms. Faster queue insufficient for sub-ms. Inline processing doesn't handle bursts. UDP loses reliability. Objective 1.3"
    }
  ],
  "2. Deployment": [
    {
      "q": "A global application deployment requires releasing to 20 regions sequentially (avoid simultaneous global outage risk). Each region takes 30 minutes, total 10 hours. Business requires <2 hour global deployment. Regional rollbacks must be independent. What deployment strategy achieves this?",
      "options": [
        "Deploy all regions simultaneously",
        "Implement wave-based deployment with blast radius limiting: deploy to 5 regions in parallel per wave (4 waves), automated canary validation in each region, automatic wave progression or halt on errors, independent rollback per region",
        "Reduce deployment time per region",
        "Use blue-green in each region simultaneously"
      ],
      "answer": 1,
      "explanation": "Wave deployment balances speed and safety: parallel deployment within waves (5 regions simultaneously), sequential waves limit blast radius, automated canary in each region validates before full rollout, waves progress automatically on success or halt on failure, per-region rollback maintains isolation. Completes in ~2 hours. All-simultaneous risks global outage. Per-region optimization insufficient. Blue-green in all regions lacks staged rollout benefit. Objective 2.3"
    },
    {
      "q": "A stateful application migration from VMs to Kubernetes requires moving 500GB of session state without downtime. Sessions must remain valid during migration. Current approach: drain VMs (1 hour downtime). What zero-downtime migration strategy works?",
      "options": [
        "Accept 1 hour downtime",
        "Implement dual-write pattern: run old and new systems in parallel, write session data to both (Redis cluster shared), gradually shift read traffic to new system, validate consistency, decommission old system",
        "Copy session data then switchover",
        "Use session replication between VM and Kubernetes"
      ],
      "answer": 1,
      "explanation": "Dual-write enables zero-downtime: both systems write to shared session store (Redis cluster), gradually shift reads from VM to K8s with traffic percentage, validate session handling in K8s, eventual full cutover. Sessions remain valid. Downtime violates requirement. Copy-then-switchover loses sessions created during copy. Built-in replication between platforms doesn't exist. Objective 2.1"
    },
    {
      "q": "A Kubernetes deployment uses HorizontalPodAutoscaler based on CPU. During traffic spike, pods scale out but application still slow. Investigation shows pods are CPU-bound during startup (JVM warmup 2 minutes). By time pods ready, spike over. What autoscaling approach handles this?",
      "options": [
        "Use faster startup times",
        "Implement predictive autoscaling with warm pool: ML predicts traffic patterns and pre-scales, maintain warm pool of pre-started pods (ReadinessGate controls traffic), use custom metrics (queue depth, request latency) not just CPU",
        "Increase CPU limits",
        "Scale based on memory instead"
      ],
      "answer": 1,
      "explanation": "Predictive autoscaling with warm pool addresses slow startup: ML-based prediction pre-scales before spike, warm pool keeps pre-started pods ready (not receiving traffic until warmed up via ReadinessGate), custom metrics provide earlier signals than CPU. Eliminates startup lag. Faster startup helps but JVM warmup inherent. CPU limits don't address startup. Memory not relevant metric. Objective 2.3"
    },
    {
      "q": "Infrastructure as Code drift detection shows 200 resources manually modified in production. Team wants to import existing state to IaC without destroying/recreating resources (would cause downtime). What import strategy is safest?",
      "options": [
        "Delete resources and recreate from IaC",
        "Use terraform import for each resource, validate with terraform plan showing no changes, create separate IaC branch per resource group, test plans extensively before applying",
        "Ignore drift and only manage new resources",
        "Use cloud provider export tools"
      ],
      "answer": 1,
      "explanation": "Safe import process: import resources individually or in small groups, terraform plan validates import accuracy (should show 0 changes), branch per resource group enables isolated testing, extensive validation before any applies. Eliminates risk of unexpected changes. Recreating causes downtime. Ignoring drift defeats IaC purpose. Export tools may miss configuration details. Objective 2.2"
    },
    {
      "q": "A canary deployment shows slightly higher error rate in canary (0.3%) vs production (0.2%). Errors are non-deterministic, impossible to reproduce. Canary runs for 2 hours, still 0.3% errors. 100K requests processed. What's the rigorous statistical approach to decision?",
      "options": [
        "Rollback immediately (any increase is bad)",
        "Apply statistical significance testing: calculate confidence interval for error rate difference, determine if difference is statistically significant (not random variation), consider error severity and business impact, establish error budget threshold",
        "Continue canary to gather more data indefinitely",
        "Deploy to 100% (0.3% acceptable)"
      ],
      "answer": 1,
      "explanation": "Statistical rigor prevents false positives: hypothesis test determines if 0.1% difference is statistically significant (considering sample size, variance), confidence intervals quantify uncertainty, error budget (from SLO) provides objective threshold. 100K requests provides good sample. Immediate rollback may reject valid releases. Indefinite canary delays release. Deploying without analysis risky. Objective 2.3"
    },
    {
      "q": "A database schema migration adds non-nullable column requiring data backfill for 1 billion rows. Migration takes 6 hours during which writes fail. Zero-downtime deployment required. What migration sequence enables zero-downtime?",
      "options": [
        "Add column as nullable, backfill data asynchronously, make non-nullable after backfill, deploy code using new column",
        "Take scheduled maintenance window",
        "Add column with default value",
        "Migrate to new database with full schema"
      ],
      "answer": 0,
      "explanation": "Multi-phase zero-downtime migration: Phase 1: add column as nullable (fast), Phase 2: background job backfills data (no blocking), Phase 3: verify complete backfill, Phase 4: alter to non-nullable, Phase 5: deploy code using column. Enables continuous writes. Maintenance window violates zero-downtime. Default value may not work for all business logic. New database requires complex data migration. Objective 2.3"
    },
    {
      "q": "A microservices deployment has complex dependency version matrix: Service A v2 requires Service B v3, but Service C v2 requires Service B v2. Deploying Service A breaks Service C. What dependency management approach resolves this?",
      "options": [
        "Deploy all services simultaneously",
        "Implement API versioning with backwards compatibility: Service B v3 supports both old and new APIs, services explicitly specify required API version via headers, gradual migration of consumers, deprecation policy for old versions",
        "Keep services on same versions",
        "Use feature flags in Service B"
      ],
      "answer": 1,
      "explanation": "API versioning enables independent deployment: Service B v3 implements both old and new API contracts, consumers specify version via header/path, Service C continues using v2 API while Service A uses v3, gradual migration of consumers, eventual v2 deprecation. Breaks tight coupling. Simultaneous deployment doesn't scale. Version locking prevents independent deployment. Feature flags don't address incompatible APIs. Objective 2.3"
    },
    {
      "q": "A Helm chart deploys 50 Kubernetes resources. Post-install hook fails causing helm install to timeout. Kubernetes resources are created but marked as failed. Manual cleanup of resources error-prone. What Helm practice prevents resource leaks?",
      "options": [
        "Disable hooks entirely",
        "Implement proper hook cleanup: use helm-hook-delete-policy, set hook weight for execution order, use Jobs for hooks with TTLSecondsAfterFinished, implement atomic installation with --atomic flag for automatic cleanup on failure",
        "Manually track all resources",
        "Use kubectl apply instead of Helm"
      ],
      "answer": 1,
      "explanation": "Proper Helm hook management: delete-policy cleans up hook resources, hook weights ensure correct order, Jobs with TTL auto-delete after completion, --atomic flag rolls back all resources on failure (no leak). Prevents manual cleanup. Disabling hooks loses functionality. Manual tracking doesn't scale. kubectl loses Helm templating. Objective 2.2"
    },
    {
      "q": "A Docker image build for production uses multi-stage build. Final image size 2GB (includes build tools, source code, test frameworks). Security scan shows 50 vulnerabilities in build tools. What build optimization addresses security and size?",
      "options": [
        "Use smaller base image",
        "Implement proper multi-stage build: builder stage with all tools, final stage copies only runtime artifacts, use distroless or scratch base for final stage, .dockerignore excludes unnecessary files, build tools never reach final image",
        "Remove vulnerabilities from build tools",
        "Scan only final stage"
      ],
      "answer": 1,
      "explanation": "Correct multi-stage pattern: builder stage has tools/dependencies, final stage COPY --from=builder only runtime binaries, distroless/scratch base has no OS packages (no vulnerabilities), .dockerignore prevents build context bloat. Reduces image to 100-200MB, zero build tool vulnerabilities. Smaller base helps but insufficient. Fixing build tools doesn't reduce size. Scanning only final stage correct but need proper build first. Objective 2.2"
    },
    {
      "q": "GitOps deployment watches Git repository. Developer accidentally commits secrets in configuration YAML. Secrets are encrypted but encryption key also in same repository. Automated deployment exposes secrets. What prevents this security anti-pattern?",
      "options": [
        "Encrypt secrets with external key",
        "Implement external secret management: use SealedSecrets or External Secrets Operator, secrets stored in vault/secrets manager, only encrypted references in Git, deployment time secret injection, pre-commit hooks prevent accidental secret commits",
        "Use private repository",
        "Manual review before deployment"
      ],
      "answer": 1,
      "explanation": "Proper secret management: SealedSecrets/External Secrets store secrets externally (Vault, cloud secrets manager), Git contains only encrypted references, secrets injected at deployment time, pre-commit hooks scan for secrets. Keys never in Git. Encryption with external key helps but practices needed. Private repo doesn't prevent exposure to repo access. Manual review doesn't scale. Objective 2.2"
    },
    {
      "q": "A blue-green deployment with 1000 application instances switches traffic from blue to green. 50 instances in green fail health checks after traffic switch (didn't fail before traffic). Users experience errors. What pre-switch validation would have caught this?",
      "options": [
        "Better health checks",
        "Implement production traffic replay or synthetic testing: replay production traffic patterns to green environment before switch, test under realistic load including peak patterns, validate database connections under load, test third-party integrations",
        "Longer warmup period",
        "Canary deployment instead"
      ],
      "answer": 1,
      "explanation": "Production-realistic validation: traffic replay sends actual production patterns to green pre-switch (catches load-dependent issues), synthetic tests validate end-to-end flows, realistic load tests database connection pools, integration tests verify external dependencies. Catches issues before user impact. Better health checks help but may miss issues. Warmup doesn't test under load. Canary still needs validation. Objective 2.3"
    },
    {
      "q": "A configuration management tool (Ansible) applies desired state every 30 minutes. Developers need emergency changes applied immediately without waiting. Running Ansible manually on 500 servers risky. What controlled approach enables immediate changes?",
      "options": [
        "Reduce sync interval to 1 minute",
        "Implement push-based change with approval workflow: approved changes trigger immediate targeted Ansible run against affected servers, change request system tracks approvals, limited scope execution reduces risk, still maintains IaC principles",
        "Allow SSH access for emergency changes",
        "Disable configuration management during emergencies"
      ],
      "answer": 1,
      "explanation": "Controlled push mechanism: change request system provides workflow/approvals, approved changes trigger immediate targeted Ansible execution (specific playbook, specific hosts), maintains IaC (change in code first), audit trail preserved. Fast and safe. 1-minute interval wasteful and still has delay. Direct SSH bypasses IaC. Disabling CM creates drift. Objective 2.2"
    },
    {
      "q": "A Kubernetes deployment strategy requires deploying to dev, then staging, then production automatically if tests pass. Current approach: manual kubectl apply to each environment. What automated progressive delivery implementation is safest?",
      "options": [
        "Script kubectl apply with conditionals",
        "Implement GitOps with progressive delivery: ArgoCD with ApplicationSet for environment promotion, automated testing gates between environments, approval requirement for production, environment-specific value overlays via Kustomize",
        "CI/CD pipeline with manual approvals",
        "Helm with different values files"
      ],
      "answer": 1,
      "explanation": "GitOps progressive delivery: ArgoCD ApplicationSet manages multi-environment deployment, automatic promotion on passing tests (dev to staging), required approval gate for staging to production, Kustomize handles environment differences, Git remains single source of truth. Safe automation. Scripts fragile and lack visibility. Manual CI/CD gates don't automate dev to staging. Helm alone doesn't provide progressive delivery. Objective 2.3"
    },
    {
      "q": "A service mesh deployment (Istio) shows 30% higher latency after enabling. Application previously 50ms, now 65ms (30% increase). Business considers removing service mesh. What optimization reduces overhead while keeping mesh benefits?",
      "options": [
        "Remove service mesh",
        "Optimize service mesh configuration: use native sidecars with eBPF if available, enable protocol sniffing to avoid unnecessary proxying, optimize Envoy threading and memory, use iptables optimization, implement ambient mesh architecture",
        "Increase application server capacity",
        "Disable all service mesh features"
      ],
      "answer": 1,
      "explanation": "Service mesh optimization: native sidecars (eBPF) reduce overhead vs traditional sidecars, protocol sniffing bypasses proxy for non-mTLS traffic, Envoy tuning optimizes performance, ambient mesh (Istio recent versions) uses shared node proxy reducing per-pod overhead. Maintains benefits with less than 5ms overhead. Removing mesh loses observability and security. More capacity doesn't address overhead. Disabling features defeats purpose. Objective 2.3"
    },
    {
      "q": "A database migration uses logical replication from source to target. During cutover, discovered 1000 transactions in replication lag (30 seconds behind). Cutting over now causes data loss. Application cannot tolerate downtime. What cutover process prevents data loss?",
      "options": [
        "Wait for replication to catch up then cut over",
        "Implement two-phase cutover: enable read-only mode on source while replication completes, verify zero lag, switch application to target, re-enable writes on target, validate consistency",
        "Accept data loss for 1000 transactions",
        "Use transactional cutover with distributed transaction"
      ],
      "answer": 1,
      "explanation": "Two-phase safe cutover: Phase 1: read-only on source (stops new changes), replication completes (catches up), Phase 2: verify zero lag, Phase 3: switch application to target, Phase 4: enable writes. No data loss, minimal read-only window. Waiting doesn't stop new writes. Accepting loss violates requirement. Distributed transactions don't apply to migration scenario. Objective 2.1"
    },
    {
      "q": "A container registry stores 10,000 images with 100 versions each (1 million total images). Builds push new images but never clean old images. Storage costs $20K/month and growing. What image lifecycle policy optimizes storage?",
      "options": [
        "Delete all old images",
        "Implement tag-based retention policy: keep last N versions per image, keep all images with semantic version tags, delete images older than 90 days without tags, keep production-tagged images indefinitely, automated cleanup job",
        "Manually review and delete monthly",
        "Compress images"
      ],
      "answer": 1,
      "explanation": "Smart retention policy: keep last N versions (e.g., 10) ensures recent builds available, semantic version tags preserved (releases), untagged old images cleaned (failed builds), production tag protects deployed images, automation prevents manual work. Reduces storage 80%+. Deleting all breaks rollback capability. Manual review doesn't scale. Compression limited benefit. Objective 2.2"
    },
    {
      "q": "A Terraform workspace manages 500 resources across multiple cloud providers. Terraform plan takes 15 minutes making development slow. Resource dependencies are complex. What Terraform optimization reduces plan time?",
      "options": [
        "Use faster computer",
        "Implement workspace splitting: separate workspace per application with clear boundaries, use remote state data sources for cross-workspace dependencies, enable parallelism flag, use targeted applies for specific resources during development",
        "Reduce number of resources",
        "Remove provider plugins"
      ],
      "answer": 1,
      "explanation": "Workspace optimization: split into logical boundaries (per-app workspaces, shared infrastructure workspace), remote state data sources link workspaces, parallelism flag increases concurrent operations, targeted operations during dev. Reduces plan time 5-10x. Faster computer helps marginally. Reducing resources may not be possible. Removing providers breaks functionality. Objective 2.2"
    }
  ],
  "3. Security": [
    {
      "q": "A zero-day vulnerability affects base OS in all container images (500 services). Patch available but redeploying all services takes 8 hours. Business requires protection within 1 hour. What immediate mitigation works while redeployment in progress?",
      "options": [
        "Stop all containers",
        "Deploy runtime security with eBPF or seccomp profiles blocking vulnerable syscalls, WAF rules blocking exploitation attempts, network segmentation isolating vulnerable services, IDS signatures for attack detection",
        "Accept risk for 8 hours",
        "Deploy canary rollout over 8 hours"
      ],
      "answer": 1,
      "explanation": "Runtime protection provides immediate defense: eBPF/seccomp blocks exploitation at syscall level, WAF stops web-based attacks, segmentation limits blast radius, IDS provides visibility. Works while permanent fix (redeployment) progresses. Stopping breaks business. Accepting risk violates security. Canary doesn't speed up remediation. Objective 3.3"
    },
    {
      "q": "A compliance audit finds encryption keys rotated annually but access logs show keys accessed 50K times/day. Key compromise would expose all data encrypted with that key. What key management strategy limits exposure per-key?",
      "options": [
        "Rotate keys monthly",
        "Implement envelope encryption with frequent DEK rotation: new data encryption key per day or per customer, master key encrypts DEKs, DEK compromise limits exposure to single day or customer, automated rotation",
        "Use longer keys",
        "Encrypt keys with additional layer"
      ],
      "answer": 1,
      "explanation": "Envelope encryption with frequent DEK rotation minimizes blast radius: daily or per-customer DEKs mean compromise affects limited data scope, master key encrypts DEKs (rarely accessed), automated rotation sustainable. Better than rotating master key frequently (requires re-encrypting everything). Monthly KEK rotation doesn't limit per-key exposure. Key length doesn't limit exposure scope. Double encryption adds complexity without limiting scope. Objective 3.4"
    },
    {
      "q": "Penetration test shows attacker pivoted from compromised web server to database server using service account with elevated privileges. Network segmentation exists but account had access to all zones. What prevents lateral movement?",
      "options": [
        "Stronger passwords",
        "Implement least privilege with service-specific credentials: unique credentials per service with minimum necessary permissions, network policy enforcement (zero-trust), just-in-time privilege elevation for admin tasks, service account lifecycle management",
        "Better network segmentation",
        "MFA for service accounts"
      ],
      "answer": 1,
      "explanation": "Multi-layer defense: unique credentials per service limits blast radius, minimum permissions (least privilege) restricts what compromised account can access, network policies provide defense-in-depth (even with credentials, network blocks access), JIT elevation for admin tasks. Password strength doesn't prevent privileged account abuse. Segmentation existed but credentials bypassed. MFA difficult for service accounts and doesn't limit privileges. Objective 3.2"
    },
    {
      "q": "Application requires FIPS 140-2 Level 3 validated cryptography. Cloud provider HSM offers Level 2. Using Level 2 violates compliance. On-premises HSM increases latency (100ms) violating performance SLA. What architecture meets both requirements?",
      "options": [
        "Accept compliance violation",
        "Deploy FIPS Level 3 HSM appliances in cloud regions, use low-latency connections, implement hybrid model with local crypto operations for performance-critical paths and HSM for key management",
        "Relocate all to on-premises",
        "Request waiver for Level 2"
      ],
      "answer": 1,
      "explanation": "Hybrid approach: FIPS Level 3 HSM appliances deployed in cloud regions provide compliance, low-latency network connections (cloud interconnect) minimize latency, local crypto operations use keys from HSM but perform operations locally (meeting performance), key management centralized in HSM. Compliance violation not acceptable. Fully on-premises loses cloud benefits. Waiver may not be granted. Objective 3.4"
    },
    {
      "q": "DDoS attack uses application-layer attacks that bypass network-level protections. Attackers use valid credentials stolen from credential stuffing. Rate limiting per IP ineffective (distributed). What multilayer defense mitigates this?",
      "options": [
        "Higher rate limits",
        "Implement behavior analytics with risk-based authentication: ML detects anomalous patterns (impossible travel, unusual access patterns), step-up authentication for risky operations, device fingerprinting, CAPTCHA challenges for suspicious activity",
        "Block all login attempts",
        "Require MFA for every request"
      ],
      "answer": 1,
      "explanation": "Behavior-based defense: ML learns normal patterns and flags anomalies (login from new country 1 hour after US login), risk-based auth requires additional verification for unusual activity, device fingerprinting identifies known devices, CAPTCHA raises attacker cost. Addresses sophisticated attacks with valid credentials. Higher limits don't prevent abuse. Blocking all breaks service. Constant MFA poor UX. Objective 3.3"
    },
    {
      "q": "API authentication uses JWT tokens with 24-hour expiration. Tokens stored in localStorage. XSS vulnerability discovered. Attackers can steal tokens and impersonate users for 24 hours even after password change. What token strategy mitigates this?",
      "options": [
        "Use sessionStorage instead",
        "Implement refresh token rotation: short-lived access tokens (5 min) in memory only, httpOnly refresh tokens, token binding to TLS channel, immediate revocation capability, fingerprint validation"
      ],
      "answer": 1,
      "explanation": "Secure token pattern: short-lived access tokens limit exposure window, memory-only storage prevents XSS theft, httpOnly refresh tokens inaccessible to JavaScript, token binding ties to TLS session, immediate revocation on compromise, fingerprint adds device validation. sessionStorage has same XSS risk as localStorage. Objective 3.6"
    },
    {
      "q": "Container security scan shows critical vulnerability in package that is not actually used by application (dependency of dependency). Deployment blocked. Removing entire dependency breaks application. What addresses false positive blocking?",
      "options": [
        "Override security scan",
        "Implement vulnerability contextual analysis: use software composition analysis to map which code paths actually reach vulnerable package, implement runtime protection (AppArmor/SELinux profiles) blocking exploitation, document risk acceptance with compensating controls",
        "Wait for patch",
        "Use different base image"
      ],
      "answer": 1,
      "explanation": "Contextual analysis determines actual risk: SCA shows if vulnerable code reachable from application, runtime protection blocks exploitation even if reachable, document analysis and compensating controls for risk acceptance. Better than blanket override or blocking deployments indefinitely. Waiting may take months. Different image may have other issues. Objective 3.3"
    },
    {
      "q": "Compliance requires immutable infrastructure (no SSH, no patches). Security vuln discovered requiring immediate patch. Rebuilding and redeploying AMI takes 6 hours. Business demands faster response. What balances immutability and agility?",
      "options": [
        "Allow SSH for emergencies",
        "Implement hot-patchable base images with atomic updates: prebuild common patch scenarios, blue-green deployment of patched images, automated testing pipeline for rapid validation, exception process for critical vulnerabilities with reduced testing",
        "Abandon immutable infrastructure",
        "Accept 6-hour response time"
      ],
      "answer": 1,
      "explanation": "Optimized immutable process: prebuild common patches (OS, common libraries) reduces build time, automated testing pipeline validates quickly, blue-green enables fast deployment with instant rollback, exception process with risk-based reduced testing for critical vulnerabilities. Maintains immutability principle with faster response. SSH violates immutability. Abandoning loses benefits. 6-hour unacceptable for critical vulnerabilities. Objective 3.3"
    },
    {
      "q": "Database contains PII requiring encryption at rest. Application must search encrypted data (name, email). Standard encryption prevents searching. Performance requirement: queries under 100ms. What encryption scheme enables search on encrypted data?",
      "options": [
        "Decrypt for search",
        "Implement deterministic encryption for searchable fields or order-preserving encryption with field-level encryption, use hash-based indexes for exact match, consider searchable encryption schemes, encrypt non-searchable fields with probabilistic encryption"
      ],
      "answer": 1,
      "explanation": "Searchable encryption techniques: deterministic encryption (same plaintext = same ciphertext) enables exact match without decryption, order-preserving encryption enables range queries, hash indexes for email lookup, probabilistic encryption for high-security fields not requiring search. Balances security and functionality. Decrypting for search defeats purpose. Objective 3.4"
    },
    {
      "q": "SIEM system generates 50K security events per hour. Security team of 3 can investigate 20 events per day. Alert fatigue causing real threats missed. What approach makes SIEM actionable?",
      "options": [
        "Reduce alert sensitivity",
        "Implement security orchestration automation: ML-based threat prioritization, automated enrichment and triage, automated response for known attack patterns, correlation to reduce noise, risk-based alerting with severity classification"
      ],
      "answer": 1,
      "explanation": "Security automation (SOAR) makes SIEM actionable: ML prioritizes based on threat intelligence and context, automated enrichment adds context (user, asset, past behavior), automated playbooks handle known patterns, correlation groups related events reducing 1000s to 10s, risk scoring focuses on critical items. Maintains security with realistic human capacity. Reducing sensitivity increases false negatives. Objective 3.3"
    },
    {
      "q": "Advanced security scenario 11: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 12: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 13: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 14: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 15: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 16: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    },
    {
      "q": "Advanced security scenario 17: Multi-cloud environment with different security tools per provider creating visibility gaps. What unified security approach provides consistent visibility and control?",
      "options": [
        "Use provider-native tools only",
        "Implement cloud security posture management (CSPM) with unified policy engine, centralized logging and SIEM, cloud-agnostic security controls (service mesh, OPA policies), security as code with common policy framework"
      ],
      "answer": 1,
      "explanation": "Unified security across clouds: CSPM provides single pane for multi-cloud security posture, centralized SIEM aggregates all logs, cloud-agnostic controls (Istio, OPA) work everywhere, security as code defines policies once and deploys to all clouds. Provider tools alone create silos. Objective 3.2"
    }
  ],
  "4. Operations": [
    {
      "q": "Auto-scaling uses CPU and memory metrics. Application shows healthy metrics (CPU 40%, memory 50%) but users report slowness. Investigation shows thread pool exhaustion (all threads blocked on external API). What custom metric prevents this?",
      "options": [
        "Increase thread pool size",
        "Implement active thread count monitoring and scale on thread utilization: custom metric tracks thread pool utilization, scale before exhaustion, implement circuit breaker for external API, monitor queue depth",
        "Add more CPU",
        "Use async processing"
      ],
      "answer": 1,
      "explanation": "Thread pool utilization as scaling metric detects this bottleneck: scale when 80% threads active (before exhaustion), queue depth provides early signal, circuit breaker prevents cascading failure from slow API. CPU/memory don't reflect thread state. Increasing threads masks issue. More CPU doesn't help thread blocking. Async helps but need scaling metric first. Objective 4.2"
    },
    {
      "q": "RTO requires 1-hour recovery but DR test shows 4-hour actual recovery (infrastructure ready in 1 hour, application configuration takes 3 hours). What DR practice was missing?",
      "options": [
        "Faster manual recovery",
        "Automate entire recovery: infrastructure as code for all resources, configuration management for application setup, orchestrated recovery playbooks (RunBooks), pre-deployed but scaled-to-zero resources, regular testing",
        "Better documentation",
        "More DR staff"
      ],
      "answer": 1,
      "explanation": "Full automation meets RTO: IaC deploys infrastructure, config management automates application setup (eliminating 3-hour manual configuration), orchestrated playbooks coordinate recovery steps, pre-deployed infrastructure scaled to minimum reduces startup time. Testing validates automation. Faster manual still takes 3 hours. Documentation doesn't automate. More staff doesn't reduce 3-hour configuration time. Objective 4.3"
    },
    {
      "q": "Monitoring shows all infrastructure metrics healthy but business metrics (conversion rate, revenue) down 20%. What observability gap exists and how to address it?",
      "options": [
        "Infrastructure monitoring is sufficient",
        "Implement full-stack observability: business metrics alongside technical metrics, user journey monitoring, synthetic transactions testing critical paths, distributed tracing correlating business and technical events, real user monitoring (RUM)"
      ],
      "answer": 1,
      "explanation": "Business-aware observability connects technical and business metrics: track revenue/conversions alongside latency/errors, user journey monitoring shows where users drop off, synthetic tests validate critical business flows, distributed tracing correlates failed transactions with technical issues, RUM shows actual user experience. Infrastructure alone misses business impact. Objective 4.1"
    },
    {
      "q": "Database backup tested successfully in isolated environment. Production restore attempt fails due to missing table permissions, incompatible database version extensions, and cross-region endpoint differences. What backup validation prevents this?",
      "options": [
        "More frequent backup tests",
        "Implement production-like restore testing: restore to environment matching production configuration exactly, validate application connectivity and permissions, test with actual application workload, automate restore testing monthly, document dependencies and prerequisites"
      ],
      "answer": 1,
      "explanation": "Production-realistic restore testing: test environment must match production config (versions, extensions, permissions), test actual application connectivity (not just database restore), validate with real workload patterns, automated regular testing catches config drift. Isolated testing missed production-specific dependencies. Frequency doesn't address environment mismatch. Objective 4.3"
    },
    {
      "q": "Cost optimization initiative reduces cloud spend 40% by rightsizing instances and using reserved capacity. Three weeks later, P99 latency increased from 200ms to 2 seconds during peak traffic. What was the optimization mistake?",
      "options": [
        "Optimization was too aggressive",
        "Failed to consider headroom for bursts: rightsizing based on average utilization without peak capacity buffer, reserved capacity doesn't auto-scale like on-demand, removed autoscaling buffer, cost optimization without performance testing under load"
      ],
      "answer": 1,
      "explanation": "Cost optimization must preserve performance: maintain headroom for traffic bursts (peak not average), reserved instances need additional on-demand or spot for autoscaling, buffer capacity for unexpected spikes, load test after optimization to validate performance. Average-based sizing causes peak degradation. Objective 4.2"
    },
    {
      "q": "Operations scenario 6: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 7: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 8: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 9: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 10: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 11: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 12: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 13: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 14: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    },
    {
      "q": "Operations scenario 15: Application deployed across 3 availability zones. Zone failure causes complete application outage despite multi-AZ deployment. What configuration error caused this?",
      "options": [
        "Need more zones",
        "Single point of failure in architecture: database primary in failed zone without automatic failover, load balancer not actually distributing across zones, shared service dependency in single zone, insufficient remaining capacity in 2 zones for full load"
      ],
      "answer": 1,
      "explanation": "Multi-AZ failures indicate architectural issues: database without auto-failover creates SPOF, load balancer misconfiguration routes all traffic to one zone, shared services (cache, queue) in failed zone block all requests, N+1 capacity planning ensures 2 zones handle full load. More zones don't fix architecture issues. Objective 4.2"
    }
  ],
  "5. DevOps Fundamentals": [
    {
      "q": "CI pipeline runs full test suite (2000 tests, 30 minutes) on every commit blocking developers. 95% of tests unaffected by typical commits. What testing strategy maintains quality while improving speed?",
      "options": [
        "Remove slow tests",
        "Implement test impact analysis: map tests to code changes, run only affected tests on commits (5 minutes), full suite on main branch merges and nightly, ML predicts test relevance",
        "Parallelize tests more",
        "Reduce test coverage"
      ],
      "answer": 1,
      "explanation": "Test impact analysis optimizes CI: code change analysis maps to affected tests, run only relevant subset (5 min vs 30 min), full suite on merge ensures nothing missed, ML improves over time. Maintains quality with faster feedback. Removing tests reduces coverage. Parallelization helps but runs unnecessary tests. Reducing coverage risks bugs. Objective 5.3"
    },
    {
      "q": "Git repository shows 500K lines of code changed in single commit message 'various fixes'. Code review impossible, rollback risky. What development practice prevents this?",
      "options": [
        "Mandatory code reviews",
        "Implement atomic commits with conventional commit messages: small focused commits (single feature/fix), commit message conventions with ticket references, pre-commit hooks enforcing commit size limits, squash only when appropriate, rebase workflow for clean history"
      ],
      "answer": 1,
      "explanation": "Atomic commit practices: small commits enable easy review and rollback, conventional messages provide context (feat:, fix:, docs:), size limits prevent bulk commits (catch in pre-commit), squash preserves history during cleanup. Makes codebase maintainable. Reviews help but need reviewable commits first. Objective 5.1"
    },
    {
      "q": "Configuration management tool (Puppet/Chef) applies configuration but servers show drift within hours. Investigation shows operations team making manual changes for troubleshooting then forgetting to update code. How to prevent drift?",
      "options": [
        "More frequent configuration runs",
        "Implement configuration as code workflow: all changes must go through code repository first, automated drift detection and alerting, read-only production access (changes via automation only), change approval workflow integrated with CM tool"
      ],
      "answer": 1,
      "explanation": "Enforce configuration as code discipline: code-first policy (no manual changes), read-only production prevents drift at source, automated drift detection alerts to manual changes, approval workflow ensures changes reviewed. Prevents drift cause not symptoms. Frequent runs don't prevent manual changes. Objective 5.2"
    },
    {
      "q": "CI/CD pipeline for 50 microservices uses shared Jenkins server. Pipeline jobs frequently queued waiting for executors (30-minute wait times). Adding more executors to Jenkins expensive. What architecture improves CI/CD scalability cost-effectively?",
      "options": [
        "Buy more Jenkins executors",
        "Implement elastic CI/CD: use cloud-based auto-scaling workers (spot instances or Kubernetes pods), pipeline jobs spawn ephemeral build agents, scale from 0 to 100s based on demand, pay only for build time, shared artifact caching"
      ],
      "answer": 1,
      "explanation": "Elastic CI/CD architecture: ephemeral build agents scale instantly with workload (no queuing), spot instances or K8s pods cost-effective, scale to zero when idle (cost savings), artifact caching speeds builds. Eliminates queuing and rightsizes cost. More executors fixed cost whether used or not. Objective 5.3"
    },
    {
      "q": "DevOps practice 5: Deployment pipeline has 15 manual approval gates causing 2-day release cycle. Business wants daily releases without sacrificing control. What automated governance enables this?",
      "options": [
        "Remove all approval gates",
        "Implement automated quality gates with risk-based approvals: automated security scanning, test coverage requirements, performance benchmarks, automated approvals for low-risk changes, manual approval only for high-risk changes, audit trail for all decisions"
      ],
      "answer": 1,
      "explanation": "Automated governance: quality gates check security/testing/performance automatically (fast, consistent), risk-based routing sends only high-risk changes for human approval, low-risk auto-approved with audit trail. Maintains control with speed. Removing gates loses governance. Objective 5.3"
    },
    {
      "q": "DevOps practice 6: Deployment pipeline has 15 manual approval gates causing 2-day release cycle. Business wants daily releases without sacrificing control. What automated governance enables this?",
      "options": [
        "Remove all approval gates",
        "Implement automated quality gates with risk-based approvals: automated security scanning, test coverage requirements, performance benchmarks, automated approvals for low-risk changes, manual approval only for high-risk changes, audit trail for all decisions"
      ],
      "answer": 1,
      "explanation": "Automated governance: quality gates check security/testing/performance automatically (fast, consistent), risk-based routing sends only high-risk changes for human approval, low-risk auto-approved with audit trail. Maintains control with speed. Removing gates loses governance. Objective 5.3"
    },
    {
      "q": "DevOps practice 7: Deployment pipeline has 15 manual approval gates causing 2-day release cycle. Business wants daily releases without sacrificing control. What automated governance enables this?",
      "options": [
        "Remove all approval gates",
        "Implement automated quality gates with risk-based approvals: automated security scanning, test coverage requirements, performance benchmarks, automated approvals for low-risk changes, manual approval only for high-risk changes, audit trail for all decisions"
      ],
      "answer": 1,
      "explanation": "Automated governance: quality gates check security/testing/performance automatically (fast, consistent), risk-based routing sends only high-risk changes for human approval, low-risk auto-approved with audit trail. Maintains control with speed. Removing gates loses governance. Objective 5.3"
    },
    {
      "q": "DevOps practice 8: Deployment pipeline has 15 manual approval gates causing 2-day release cycle. Business wants daily releases without sacrificing control. What automated governance enables this?",
      "options": [
        "Remove all approval gates",
        "Implement automated quality gates with risk-based approvals: automated security scanning, test coverage requirements, performance benchmarks, automated approvals for low-risk changes, manual approval only for high-risk changes, audit trail for all decisions"
      ],
      "answer": 1,
      "explanation": "Automated governance: quality gates check security/testing/performance automatically (fast, consistent), risk-based routing sends only high-risk changes for human approval, low-risk auto-approved with audit trail. Maintains control with speed. Removing gates loses governance. Objective 5.3"
    },
    {
      "q": "DevOps practice 9: Deployment pipeline has 15 manual approval gates causing 2-day release cycle. Business wants daily releases without sacrificing control. What automated governance enables this?",
      "options": [
        "Remove all approval gates",
        "Implement automated quality gates with risk-based approvals: automated security scanning, test coverage requirements, performance benchmarks, automated approvals for low-risk changes, manual approval only for high-risk changes, audit trail for all decisions"
      ],
      "answer": 1,
      "explanation": "Automated governance: quality gates check security/testing/performance automatically (fast, consistent), risk-based routing sends only high-risk changes for human approval, low-risk auto-approved with audit trail. Maintains control with speed. Removing gates loses governance. Objective 5.3"
    }
  ],
  "6. Troubleshooting": [
    {
      "q": "Application intermittently slow (P99: 10 seconds) but P50 normal (100ms). Only 1% of requests affected. Logs show no errors. Distributed tracing shows no single slow service. What troubleshooting approach identifies cause?",
      "options": [
        "Increase logging",
        "Analyze tail latency systematically: check for garbage collection pauses, examine thread dumps during slow requests, look for lock contention, check for resource exhaustion (connection pools, file descriptors), correlate with infrastructure events",
        "Restart application",
        "Add more capacity"
      ],
      "answer": 1,
      "explanation": "Tail latency requires systematic analysis: GC pauses affect random requests, lock contention causes occasional blocking, resource exhaustion hits threshold, infrastructure events (network blips) cause periodic delays. Thread dumps during slow period reveal state. Logging may miss intermittent issues. Restart temporary. Capacity doesn't address root cause. Objective 6.1"
    },
    {
      "q": "Database queries slow during specific time windows (2-3 PM, 7-8 PM). Outside these windows performance normal. Database metrics show no resource constraints. Application unchanged. What troubleshooting approach identifies cause?",
      "options": [
        "Check application logs",
        "Correlate with external factors: check backup/maintenance windows, analyze query patterns during slow times, check for batch jobs or scheduled tasks, look for external system dependencies (partner APIs, data feeds), network utilization patterns"
      ],
      "answer": 1,
      "explanation": "Time-based issues suggest external factors: scheduled backups compete for I/O, partner batch feeds increase load, external APIs slow during their peak, network congestion at certain times, batch processing overlaps with interactive workload. Correlation analysis reveals patterns. Application unchanged rules out code. Database metrics may not show external dependencies. Objective 6.1"
    },
    {
      "q": "Kubernetes pod stuck terminating for 30 minutes (Terminating state). Pod deletion forced but pod recreated immediately. No errors in pod logs. What diagnostic steps identify root cause?",
      "options": [
        "Delete the deployment",
        "Check for finalizers blocking deletion, investigate PersistentVolumeClaims attachments, examine PreStop hooks hanging, check for node issues preventing pod cleanup, investigate controller continuously recreating pod"
      ],
      "answer": 1,
      "explanation": "Stuck terminating pods indicate: finalizers blocking graceful deletion, PVC detachment failing, PreStop hook hanging (timeout), node kubelet issues, controller (deployment/statefulset) recreating pod. Systematic check of each. Deleting deployment doesn't address stuck pod mechanism. Objective 6.5"
    },
    {
      "q": "Application response time P50 normal (100ms) but P99 very high (10 seconds). Occurs randomly affecting 1% of requests. No correlation with time, load, or user. Distributed tracing shows no single slow service. What systematic troubleshooting identifies cause?",
      "options": [
        "Add more logging",
        "Analyze tail latency: capture thread dumps during slow requests, analyze GC pause times, check for lock contention patterns, examine connection pool exhaustion, look for network retransmission patterns, analyze slow query logs for occasional database locks"
      ],
      "answer": 1,
      "explanation": "Tail latency requires multi-faceted analysis: thread dumps show blocking, GC pause logs reveal stop-the-world pauses, lock analysis shows contention, pool monitoring catches exhaustion, network analysis reveals retransmissions, database logs show occasional locks. Combination of occasional events. Logging alone may miss timing. Objective 6.3"
    },
    {
      "q": "SSL/TLS connection failures affect 10% of clients randomly. Server certificate valid, intermediate chain correct. Failed clients can connect to other SSL sites. Packet capture shows SSL handshake failure during cipher negotiation. What causes selective failures?",
      "options": [
        "Server certificate issue",
        "Cipher suite mismatch: server configured with limited modern ciphers, older clients lacking compatible ciphers, TLS version incompatibility (server supports only TLS 1.3, older clients use TLS 1.2), SNI issues with older clients"
      ],
      "answer": 1,
      "explanation": "Selective TLS failures indicate compatibility: cipher suite mismatch (server allows only modern ciphers, some clients don't support them), TLS version requirements (TLS 1.3 only server, old clients), SNI support varies by client age. Configure server with broader cipher suite support for compatibility. Server cert valid (some clients work). Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 5: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 6: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 7: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 8: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 9: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    },
    {
      "q": "Troubleshooting scenario 10: Multi-region application experiences latency issues only in Europe region despite identical infrastructure. What investigation approach identifies geographic-specific issues?",
      "options": [
        "Check application code",
        "Analyze region-specific factors: network path tracing between regions, DNS resolution times by region, CDN performance by region, regional cloud provider performance, check for region-specific configurations or data sovereignty rules affecting architecture"
      ],
      "answer": 1,
      "explanation": "Geographic troubleshooting: network tracing shows routing differences, DNS may have region-specific issues, CDN hit rates vary by region, cloud provider regional variations exist, configurations may differ by region. Identical infrastructure claim may miss region-specific settings. Application code likely same across regions. Objective 6.2"
    }
  ]
}