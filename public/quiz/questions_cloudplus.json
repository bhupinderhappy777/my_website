{
  "1. Cloud Architecture": [
    {
      "q": "A company needs to deploy applications without managing the underlying infrastructure or runtime environment. Which service model should they use?",
      "options": [
        "IaaS",
        "PaaS",
        "SaaS",
        "FaaS"
      ],
      "answer": 1,
      "explanation": "PaaS (Platform as a Service) provides a platform for application deployment without managing infrastructure or runtime. Developers focus on code while the provider manages OS, middleware, and runtime. Objective 1.1"
    },
    {
      "q": "In which cloud service model is the customer responsible for patching the operating system?",
      "options": [
        "SaaS",
        "PaaS",
        "IaaS",
        "FaaS"
      ],
      "answer": 2,
      "explanation": "In IaaS (Infrastructure as a Service), customers have full control and responsibility for the OS, including patching, updates, and security configurations. Objective 1.1"
    },
    {
      "q": "Which service model provides pre-built software applications accessible via web browser?",
      "options": [
        "IaaS",
        "PaaS",
        "SaaS",
        "DaaS"
      ],
      "answer": 2,
      "explanation": "SaaS (Software as a Service) delivers complete applications over the internet. Users access software via browser without managing infrastructure, platforms, or application maintenance. Examples: Office 365, Salesforce. Objective 1.1"
    },
    {
      "q": "A developer wants to run code in response to events without provisioning servers. Which service model is MOST appropriate?",
      "options": [
        "IaaS",
        "PaaS",
        "SaaS",
        "FaaS"
      ],
      "answer": 3,
      "explanation": "FaaS (Function as a Service) or serverless computing executes code in response to events without server management. The provider handles all infrastructure scaling automatically. Objective 1.1"
    },
    {
      "q": "According to the shared responsibility model, who is responsible for data encryption in IaaS?",
      "options": [
        "Cloud provider only",
        "Customer only",
        "Shared between both",
        "Third-party vendor"
      ],
      "answer": 1,
      "explanation": "In IaaS, the customer is responsible for data encryption, access controls, and data security. The provider secures physical infrastructure while customers secure their data and applications. Objective 1.1"
    },
    {
      "q": "Which cloud service model provides the LEAST control over the underlying infrastructure?",
      "options": [
        "IaaS",
        "PaaS",
        "SaaS",
        "All provide equal control"
      ],
      "answer": 2,
      "explanation": "SaaS provides the least infrastructure control since the provider manages everything from hardware to applications. Users only control application settings and data. Objective 1.1"
    },
    {
      "q": "A company uses Office 365 for email and productivity. Under the shared responsibility model, who manages email server patching?",
      "options": [
        "Customer",
        "Microsoft",
        "Shared responsibility",
        "Third-party auditor"
      ],
      "answer": 1,
      "explanation": "In SaaS, the provider (Microsoft) handles all infrastructure, platform, and application maintenance including patching. Customers only manage user access and data. Objective 1.1"
    },
    {
      "q": "Which service model allows customers to install and manage their own database software?",
      "options": [
        "SaaS",
        "PaaS database service",
        "IaaS",
        "FaaS"
      ],
      "answer": 2,
      "explanation": "IaaS provides virtual machines where customers can install and fully manage their own database software, including patches, configurations, and performance tuning. Objective 1.1"
    },
    {
      "q": "In PaaS, which component is the customer typically responsible for?",
      "options": [
        "Operating system patches",
        "Network infrastructure",
        "Application code",
        "Hardware maintenance"
      ],
      "answer": 2,
      "explanation": "In PaaS, customers are responsible for their application code and data while the provider manages OS, runtime, middleware, and infrastructure. Objective 1.1"
    },
    {
      "q": "Which service model is BEST suited for rapid application development with minimal infrastructure management?",
      "options": [
        "IaaS",
        "PaaS",
        "SaaS",
        "On-premises"
      ],
      "answer": 1,
      "explanation": "PaaS enables rapid development by providing pre-configured development environments, databases, and middleware. Developers can focus on coding without infrastructure concerns. Objective 1.1"
    },
    {
      "q": "What is the primary difference between RTO and RPO?",
      "options": [
        "RTO is time to recover, RPO is acceptable data loss",
        "RPO is time to recover, RTO is acceptable data loss",
        "They are the same metric",
        "RTO applies to hardware, RPO to software"
      ],
      "answer": 0,
      "explanation": "RTO (Recovery Time Objective) defines maximum acceptable downtime. RPO (Recovery Point Objective) defines maximum acceptable data loss measured in time. Objective 1.2"
    },
    {
      "q": "A company requires near-zero data loss and can tolerate 4 hours of downtime. Which metrics apply?",
      "options": [
        "High RTO, High RPO",
        "Low RTO, Low RPO",
        "High RTO, Low RPO",
        "Low RTO, High RPO"
      ],
      "answer": 2,
      "explanation": "Near-zero data loss requires low RPO (minutes). Tolerating 4 hours downtime means higher RTO. Different applications may have different recovery requirements. Objective 1.2"
    },
    {
      "q": "Which disaster recovery site type can resume operations immediately with no data loss?",
      "options": [
        "Cold site",
        "Warm site",
        "Hot site",
        "Cloud site"
      ],
      "answer": 2,
      "explanation": "Hot sites are fully operational duplicates with real-time data replication, enabling immediate failover with zero/minimal data loss. They are the most expensive DR option. Objective 1.2"
    },
    {
      "q": "What is cloud bursting?",
      "options": [
        "Moving all workloads to cloud permanently",
        "Temporarily using public cloud when private cloud capacity is exceeded",
        "Shutting down cloud resources during low demand",
        "Distributing data across multiple clouds"
      ],
      "answer": 1,
      "explanation": "Cloud bursting automatically scales applications to public cloud when on-premises or private cloud capacity is insufficient, then scales back when demand decreases. Objective 1.2"
    },
    {
      "q": "Which disaster recovery site has hardware and infrastructure but requires data restoration?",
      "options": [
        "Hot site",
        "Warm site",
        "Cold site",
        "No site"
      ],
      "answer": 1,
      "explanation": "Warm sites have hardware and network infrastructure ready but require data restoration from backups. They balance cost and recovery time between hot and cold sites. Objective 1.2"
    },
    {
      "q": "An application requires 99.99% uptime. What is the maximum acceptable downtime per year?",
      "options": [
        "52.56 minutes",
        "8.76 hours",
        "3.65 days",
        "36.5 days"
      ],
      "answer": 0,
      "explanation": "99.99% uptime allows 0.01% downtime = 52.56 minutes per year. This is often called 'four nines' availability. Higher availability requires redundancy and failover mechanisms. Objective 1.2"
    },
    {
      "q": "What is the primary purpose of availability zones in cloud architecture?",
      "options": [
        "Cost reduction",
        "Compliance requirements",
        "Fault isolation and high availability",
        "Faster processing speeds"
      ],
      "answer": 2,
      "explanation": "Availability zones are physically separated data centers within a region, providing fault isolation. Deploying across zones protects against facility-level failures. Objective 1.2"
    },
    {
      "q": "Which disaster recovery site is the MOST cost-effective but has the longest recovery time?",
      "options": [
        "Hot site",
        "Warm site",
        "Cold site",
        "Mobile site"
      ],
      "answer": 2,
      "explanation": "Cold sites have only basic facilities (space, power, cooling) but no hardware or data. They are cheapest but require hardware procurement and data restoration, resulting in longest RTO. Objective 1.2"
    },
    {
      "q": "Edge computing is BEST used for applications requiring:",
      "options": [
        "Maximum storage capacity",
        "Low latency and local processing",
        "Lowest cost",
        "Centralized management"
      ],
      "answer": 1,
      "explanation": "Edge computing processes data close to the source, reducing latency critical for IoT, real-time analytics, and applications requiring immediate response. Objective 1.2"
    },
    {
      "q": "What does multicloud tenancy refer to?",
      "options": [
        "Multiple customers sharing one cloud",
        "One customer using multiple cloud providers",
        "Multiple applications in one cloud",
        "Multiple regions in one provider"
      ],
      "answer": 1,
      "explanation": "Multicloud tenancy means using services from multiple cloud providers (AWS, Azure, GCP) to avoid vendor lock-in, improve resilience, or leverage best-of-breed services. Objective 1.2"
    },
    {
      "q": "A company's RPO is 15 minutes. How frequently should backups occur?",
      "options": [
        "Every 24 hours",
        "Every hour",
        "Every 15 minutes or less",
        "Once per week"
      ],
      "answer": 2,
      "explanation": "RPO defines maximum acceptable data loss. With 15-minute RPO, backups must occur at least every 15 minutes to ensure no more than 15 minutes of data can be lost. Objective 1.2"
    },
    {
      "q": "Which availability monitoring approach provides the MOST comprehensive view?",
      "options": [
        "Internal monitoring only",
        "External monitoring only",
        "Both internal and external monitoring",
        "Manual checks only"
      ],
      "answer": 2,
      "explanation": "Combining internal monitoring (infrastructure health) with external monitoring (end-user experience) provides complete visibility of actual service availability and performance. Objective 1.2"
    },
    {
      "q": "What is the primary benefit of deploying applications across multiple regions?",
      "options": [
        "Cost savings",
        "Geographic redundancy and disaster recovery",
        "Faster development",
        "Simpler management"
      ],
      "answer": 1,
      "explanation": "Multi-region deployment protects against region-wide outages, provides geographic redundancy for DR, and can improve performance for globally distributed users. Objective 1.2"
    },
    {
      "q": "An e-commerce site experiences 10x traffic during holidays. Which approach BEST handles this?",
      "options": [
        "Maintain peak capacity year-round",
        "Use cloud bursting",
        "Reduce functionality during peaks",
        "Queue all requests"
      ],
      "answer": 1,
      "explanation": "Cloud bursting temporarily scales to public cloud during demand spikes, avoiding the cost of maintaining peak capacity year-round while ensuring availability. Objective 1.2"
    },
    {
      "q": "What is the relationship between availability zones and regions?",
      "options": [
        "They are the same",
        "Multiple zones exist within one region",
        "Multiple regions exist within one zone",
        "No relationship"
      ],
      "answer": 1,
      "explanation": "A region is a geographic area containing multiple isolated availability zones. Zones provide fault tolerance within a region while regions provide geographic separation. Objective 1.2"
    },
    {
      "q": "Which VPN type provides site-to-site connectivity between on-premises and cloud?",
      "options": [
        "Client VPN",
        "Remote access VPN",
        "Site-to-site VPN",
        "SSL VPN"
      ],
      "answer": 2,
      "explanation": "Site-to-site VPN creates persistent, encrypted tunnels between networks (on-premises to cloud), enabling resources in both locations to communicate securely. Objective 1.3"
    },
    {
      "q": "What is the primary advantage of a dedicated connection over VPN?",
      "options": [
        "Lower cost",
        "Easier setup",
        "Consistent performance and higher bandwidth",
        "Better encryption"
      ],
      "answer": 2,
      "explanation": "Dedicated connections (AWS Direct Connect, Azure ExpressRoute) provide predictable bandwidth, lower latency, and consistent performance compared to internet-based VPNs. Objective 1.3"
    },
    {
      "q": "Which component distributes traffic based on application-layer information like HTTP headers?",
      "options": [
        "Network load balancer",
        "Application load balancer",
        "DNS",
        "Router"
      ],
      "answer": 1,
      "explanation": "Application load balancers operate at Layer 7, routing based on content like URL paths, HTTP headers, or cookies, enabling advanced traffic management for web applications. Objective 1.3"
    },
    {
      "q": "What is the primary function of a CDN?",
      "options": [
        "Encrypt data",
        "Cache content at edge locations",
        "Load balance applications",
        "Monitor network traffic"
      ],
      "answer": 1,
      "explanation": "Content Delivery Networks cache static content (images, videos, files) at geographically distributed edge locations, reducing latency and origin server load. Objective 1.3"
    },
    {
      "q": "Which network component provides traffic filtering between VPC subnets?",
      "options": [
        "Router",
        "Internet gateway",
        "Network ACL",
        "NAT gateway"
      ],
      "answer": 2,
      "explanation": "Network ACLs (Access Control Lists) are stateless firewalls operating at the subnet level, controlling inbound and outbound traffic with allow/deny rules. Objective 1.3"
    },
    {
      "q": "What is VPC peering used for?",
      "options": [
        "Internet access",
        "Connecting two VPCs for private communication",
        "Load balancing",
        "DNS resolution"
      ],
      "answer": 1,
      "explanation": "VPC peering creates private network connections between two VPCs, allowing resources to communicate using private IP addresses without traversing the internet. Objective 1.3"
    },
    {
      "q": "Which routing protocol is commonly used for dynamic routing between cloud and on-premises networks?",
      "options": [
        "RIP",
        "OSPF",
        "BGP",
        "EIGRP"
      ],
      "answer": 2,
      "explanation": "BGP (Border Gateway Protocol) is the standard for routing between autonomous systems, commonly used for hybrid cloud connectivity and multi-cloud networking. Objective 1.3"
    },
    {
      "q": "What does a transit gateway enable?",
      "options": [
        "Internet access for private subnets",
        "Centralized hub for connecting multiple VPCs and on-premises networks",
        "Content caching",
        "Load balancing"
      ],
      "answer": 1,
      "explanation": "Transit gateways act as central hubs, simplifying network topology by connecting multiple VPCs, on-premises networks, and remote offices through a single gateway. Objective 1.3"
    },
    {
      "q": "Which load balancer type is BEST for TCP/UDP traffic requiring extreme performance?",
      "options": [
        "Application load balancer",
        "Network load balancer",
        "Classic load balancer",
        "Gateway load balancer"
      ],
      "answer": 1,
      "explanation": "Network load balancers operate at Layer 4 (TCP/UDP), providing ultra-low latency and handling millions of requests per second, ideal for performance-critical applications. Objective 1.3"
    },
    {
      "q": "What is the purpose of an application gateway?",
      "options": [
        "Connect VPCs",
        "Provide web application firewall and SSL termination",
        "Route between subnets",
        "Monitor traffic"
      ],
      "answer": 1,
      "explanation": "Application gateways combine load balancing, web application firewall, and SSL/TLS termination, providing comprehensive Layer 7 security and traffic management. Objective 1.3"
    },
    {
      "q": "Which VLAN benefit is MOST important in cloud environments?",
      "options": [
        "Physical separation",
        "Logical network segmentation",
        "Increased bandwidth",
        "Reduced latency"
      ],
      "answer": 1,
      "explanation": "VLANs provide logical network segmentation without physical separation, enabling network isolation for security, compliance, and multi-tenancy in shared infrastructure. Objective 1.3"
    },
    {
      "q": "What is Software-Defined Networking (SDN)?",
      "options": [
        "Hardware-based routing",
        "Separating control plane from data plane for programmable networks",
        "Traditional switch configuration",
        "Manual routing"
      ],
      "answer": 1,
      "explanation": "SDN separates network control (decision-making) from forwarding (data plane), enabling centralized, programmable network management through software controllers. Objective 1.3"
    },
    {
      "q": "A company needs to route traffic between multiple VPCs in a star topology. Which solution is MOST efficient?",
      "options": [
        "VPC peering between all VPCs",
        "Transit gateway",
        "Multiple VPN connections",
        "Internet gateway"
      ],
      "answer": 1,
      "explanation": "Transit gateway provides hub-and-spoke topology, eliminating the need for complex mesh VPC peering while simplifying routing and management. Objective 1.3"
    },
    {
      "q": "What is the primary difference between public and private subnets?",
      "options": [
        "IP address ranges",
        "Internet gateway access",
        "Size",
        "Region"
      ],
      "answer": 1,
      "explanation": "Public subnets have routes to internet gateways enabling direct internet access, while private subnets lack internet gateway routes and require NAT for outbound internet access. Objective 1.3"
    },
    {
      "q": "Which component enables instances in private subnets to access the internet?",
      "options": [
        "Internet gateway",
        "NAT gateway",
        "VPN gateway",
        "Virtual private gateway"
      ],
      "answer": 1,
      "explanation": "NAT (Network Address Translation) gateway allows instances in private subnets to initiate outbound internet connections while preventing inbound internet-initiated connections. Objective 1.3"
    },
    {
      "q": "Static routes are BEST used when:",
      "options": [
        "Network topology changes frequently",
        "Simple, predictable network paths exist",
        "Automatic failover is required",
        "Complex routing protocols are needed"
      ],
      "answer": 1,
      "explanation": "Static routes work well for simple, stable network topologies where routes don't change frequently. Dynamic routing protocols are better for complex, changing environments. Objective 1.3"
    },
    {
      "q": "What do route tables define?",
      "options": [
        "DNS resolution",
        "Traffic filtering rules",
        "Network paths for traffic routing",
        "Load balancing algorithms"
      ],
      "answer": 2,
      "explanation": "Route tables contain rules determining where network traffic is directed. Each subnet must be associated with a route table defining traffic flow. Objective 1.3"
    },
    {
      "q": "Which firewall type operates at the VPC level?",
      "options": [
        "Host-based firewall",
        "Network ACL",
        "Application firewall",
        "Proxy firewall"
      ],
      "answer": 1,
      "explanation": "Network ACLs are stateless firewalls operating at VPC subnet boundaries, providing an additional security layer beyond security groups. Objective 1.3"
    },
    {
      "q": "A global application needs to route users to the nearest region. Which service is MOST appropriate?",
      "options": [
        "Load balancer",
        "CDN with global routing",
        "VPN",
        "NAT gateway"
      ],
      "answer": 1,
      "explanation": "CDNs with global routing capabilities direct users to the nearest edge location or region based on latency, improving performance and user experience. Objective 1.3"
    },
    {
      "q": "What is the primary benefit of using BGP for cloud networking?",
      "options": [
        "Simpler configuration",
        "Dynamic route discovery and redundancy",
        "Lower cost",
        "Faster speeds"
      ],
      "answer": 1,
      "explanation": "BGP enables dynamic route discovery, automatic failover, and path selection across multiple connections, critical for reliable hybrid and multi-cloud networking. Objective 1.3"
    },
    {
      "q": "Which storage tier is optimized for frequently accessed data requiring low latency?",
      "options": [
        "Hot",
        "Warm",
        "Cold",
        "Archive"
      ],
      "answer": 0,
      "explanation": "Hot tier storage provides highest performance and lowest latency for frequently accessed data, but at higher cost per GB compared to cooler tiers. Objective 1.4"
    },
    {
      "q": "Archive storage is BEST suited for:",
      "options": [
        "Database workloads",
        "Active application data",
        "Compliance data accessed once per year",
        "Operating system files"
      ],
      "answer": 2,
      "explanation": "Archive tier is optimized for long-term retention of rarely accessed data with retrieval times of hours. Lowest cost but highest retrieval time and cost. Objective 1.4"
    },
    {
      "q": "What is the primary advantage of SSD over HDD?",
      "options": [
        "Lower cost",
        "Higher capacity",
        "Faster performance and lower latency",
        "Better for cold storage"
      ],
      "answer": 2,
      "explanation": "SSDs use flash memory providing significantly faster random I/O, lower latency, and better performance for databases and applications compared to spinning disk HDDs. Objective 1.4"
    },
    {
      "q": "Which storage type is BEST for unstructured data like images and videos?",
      "options": [
        "Block storage",
        "File storage",
        "Object storage",
        "Database storage"
      ],
      "answer": 2,
      "explanation": "Object storage is optimized for unstructured data, providing massive scalability, metadata management, and cost-effective storage for media files, backups, and archives. Objective 1.4"
    },
    {
      "q": "Block storage is typically used for:",
      "options": [
        "Website static content",
        "Email attachments",
        "Virtual machine disks and databases",
        "Log files"
      ],
      "answer": 2,
      "explanation": "Block storage provides low-level, raw storage volumes that can be formatted with file systems, making it ideal for VM disks, databases requiring high IOPS, and boot volumes. Objective 1.4"
    },
    {
      "q": "Which storage type supports concurrent access from multiple servers using NFS or SMB protocols?",
      "options": [
        "Block storage",
        "Object storage",
        "File storage",
        "Archive storage"
      ],
      "answer": 2,
      "explanation": "File storage provides shared file systems accessible by multiple servers simultaneously using standard protocols (NFS, SMB/CIFS), ideal for shared application data. Objective 1.4"
    },
    {
      "q": "A company needs to store 5TB of frequently accessed database files. Which storage combination is MOST appropriate?",
      "options": [
        "Object storage on HDD",
        "Block storage on SSD",
        "File storage on tape",
        "Archive storage on SSD"
      ],
      "answer": 1,
      "explanation": "Databases require high IOPS and low latency, making block storage on SSD the optimal choice for performance. Object storage is better for unstructured data. Objective 1.4"
    },
    {
      "q": "What is the primary performance implication of choosing cold storage?",
      "options": [
        "Lower cost, higher retrieval time",
        "Higher cost, lower retrieval time",
        "Same as hot storage",
        "No performance impact"
      ],
      "answer": 0,
      "explanation": "Cold and archive tiers have significantly lower storage costs but higher retrieval times (minutes to hours) and potential retrieval fees, suitable for infrequently accessed data. Objective 1.4"
    },
    {
      "q": "Which storage tier should be used for backup data accessed quarterly?",
      "options": [
        "Hot",
        "Warm",
        "Cold",
        "Archive"
      ],
      "answer": 2,
      "explanation": "Cold storage balances cost and accessibility for infrequently accessed data (quarterly/monthly access). Cheaper than hot/warm but faster retrieval than archive tier. Objective 1.4"
    },
    {
      "q": "Object storage provides which unique capability?",
      "options": [
        "File-level locking",
        "Bootable volumes",
        "Massive scalability with HTTP/S access",
        "Shared file system"
      ],
      "answer": 2,
      "explanation": "Object storage offers virtually unlimited scalability, accessed via HTTP/S APIs (REST), with flat namespace and rich metadata, making it ideal for cloud-native applications. Objective 1.4"
    },
    {
      "q": "A company stores 100TB of archived financial records. They are accessed twice per year. Which tier minimizes cost?",
      "options": [
        "Hot",
        "Warm",
        "Cold",
        "Archive"
      ],
      "answer": 3,
      "explanation": "Archive tier provides lowest storage cost for rarely accessed data. Despite higher retrieval costs and times (hours), total cost is lowest when access is infrequent. Objective 1.4"
    },
    {
      "q": "What is the primary cost implication of hot storage?",
      "options": [
        "Lowest storage cost, highest retrieval cost",
        "Highest storage cost, lowest retrieval cost",
        "Same as cold storage",
        "No cost difference"
      ],
      "answer": 1,
      "explanation": "Hot tier has highest per-GB storage cost but minimal/no retrieval fees and charges, making it cost-effective for frequently accessed data despite higher base cost. Objective 1.4"
    },
    {
      "q": "Which disk type is MOST cost-effective for large sequential workloads with minimal random I/O?",
      "options": [
        "Premium SSD",
        "Standard SSD",
        "Standard HDD",
        "Ultra disk"
      ],
      "answer": 2,
      "explanation": "HDDs are cost-effective for sequential workloads (backups, archives, big data) where high random I/O performance isn't required. Much cheaper per GB than SSDs. Objective 1.4"
    },
    {
      "q": "Block storage volumes can be:",
      "options": [
        "Accessed via HTTP only",
        "Attached to multiple servers simultaneously",
        "Formatted with file systems",
        "Only used for archival"
      ],
      "answer": 2,
      "explanation": "Block storage provides raw storage volumes that can be partitioned, formatted with any file system (NTFS, ext4, XFS), and used like local disks attached to single instances. Objective 1.4"
    },
    {
      "q": "Which storage type is BEST for a shared content management system accessed by multiple web servers?",
      "options": [
        "Block storage",
        "Object storage",
        "File storage",
        "Local SSD"
      ],
      "answer": 2,
      "explanation": "File storage (NFS/SMB) enables multiple servers to concurrently read/write shared files, essential for CMS, shared application data, and collaborative workloads. Objective 1.4"
    },
    {
      "q": "What is the primary benefit of using cloud-managed services?",
      "options": [
        "More control over infrastructure",
        "Reduced operational overhead",
        "Lower cost always",
        "Better performance always"
      ],
      "answer": 1,
      "explanation": "Cloud-managed services (managed databases, serverless) reduce operational overhead by handling patching, backups, scaling, allowing focus on application development. Objective 1.5"
    },
    {
      "q": "Which architecture pattern breaks applications into small, independent services?",
      "options": [
        "Monolithic",
        "Microservices",
        "Client-server",
        "Peer-to-peer"
      ],
      "answer": 1,
      "explanation": "Microservices architecture decomposes applications into small, loosely coupled services that can be developed, deployed, and scaled independently. Objective 1.5"
    },
    {
      "q": "What does loosely coupled architecture enable?",
      "options": [
        "Services to be tightly integrated",
        "Services to function independently with minimal dependencies",
        "Faster network speeds",
        "Lower costs"
      ],
      "answer": 1,
      "explanation": "Loosely coupled services interact through well-defined interfaces, enabling independent development, deployment, and scaling while reducing cascading failures. Objective 1.5"
    },
    {
      "q": "What is fan-out in cloud architecture?",
      "options": [
        "Distributing a message to multiple subscribers",
        "Concentrating traffic to one endpoint",
        "Load balancing algorithm",
        "Storage replication"
      ],
      "answer": 0,
      "explanation": "Fan-out pattern distributes a single message/event to multiple subscribers/consumers, enabling parallel processing and event-driven architectures. Objective 1.5"
    },
    {
      "q": "Service discovery is used to:",
      "options": [
        "Monitor service health",
        "Automatically locate and connect to services",
        "Scale services",
        "Secure services"
      ],
      "answer": 1,
      "explanation": "Service discovery enables dynamic location of services in distributed systems, critical for microservices where service instances change frequently. Objective 1.5"
    },
    {
      "q": "Which cloud-native design principle reduces single points of failure?",
      "options": [
        "Centralization",
        "Loose coupling",
        "Tight integration",
        "Manual scaling"
      ],
      "answer": 1,
      "explanation": "Loose coupling ensures service failures don't cascade. Services communicate through APIs/message queues, and failures are isolated to individual components. Objective 1.5"
    },
    {
      "q": "Managed databases provide which operational benefit?",
      "options": [
        "Full OS access",
        "Automatic patching and backups",
        "Lower cost always",
        "Unlimited storage"
      ],
      "answer": 1,
      "explanation": "Managed database services handle patching, backups, replication, and scaling automatically, reducing operational burden while maintaining high availability. Objective 1.5"
    },
    {
      "q": "Microservices are BEST suited for:",
      "options": [
        "Small, simple applications",
        "Large, complex applications requiring independent scaling",
        "Legacy applications",
        "Single-function applications"
      ],
      "answer": 1,
      "explanation": "Microservices excel in large, complex applications where different components have different scaling needs, team ownership, and release cycles. Objective 1.5"
    },
    {
      "q": "What is a key challenge of microservices architecture?",
      "options": [
        "Too simple to manage",
        "Increased operational complexity",
        "Cannot scale",
        "Only works on-premises"
      ],
      "answer": 1,
      "explanation": "While microservices offer benefits, they increase complexity in deployment, monitoring, networking, and debugging compared to monolithic applications. Objective 1.5"
    },
    {
      "q": "Which pattern enables asynchronous communication between microservices?",
      "options": [
        "Direct API calls only",
        "Message queues and event buses",
        "Shared databases",
        "File sharing"
      ],
      "answer": 1,
      "explanation": "Message queues and event buses enable asynchronous, loosely coupled communication between microservices, improving resilience and scalability. Objective 1.5"
    },
    {
      "q": "What is a stand-alone container?",
      "options": [
        "A container requiring orchestration",
        "A single container running independently",
        "Multiple containers linked together",
        "A virtual machine"
      ],
      "answer": 1,
      "explanation": "Stand-alone containers run independently without orchestration, suitable for simple workloads, development, or single-purpose applications. Objective 1.6"
    },
    {
      "q": "Which tool provides workload orchestration for containers?",
      "options": [
        "Docker",
        "Kubernetes",
        "Git",
        "Ansible"
      ],
      "answer": 1,
      "explanation": "Kubernetes orchestrates container deployment, scaling, networking, and management across clusters, automating complex container operations. Objective 1.6"
    },
    {
      "q": "What does port mapping enable in containerization?",
      "options": [
        "Container networking",
        "Exposing container services on host ports",
        "Storage access",
        "Security"
      ],
      "answer": 1,
      "explanation": "Port mapping binds container ports to host ports, making containerized services accessible externally. Example: container port 80 mapped to host port 8080. Objective 1.6"
    },
    {
      "q": "Which storage type persists data beyond container lifecycle?",
      "options": [
        "Ephemeral storage",
        "Persistent volumes",
        "Container filesystem",
        "Temporary storage"
      ],
      "answer": 1,
      "explanation": "Persistent volumes exist independently of containers, preserving data when containers are deleted or restarted. Essential for databases and stateful applications. Objective 1.6"
    },
    {
      "q": "What is ephemeral storage in containers?",
      "options": [
        "Storage that persists after container deletion",
        "Temporary storage deleted with container",
        "Network-attached storage",
        "Block storage"
      ],
      "answer": 1,
      "explanation": "Ephemeral storage exists only during container lifetime. When the container is deleted, all data in ephemeral storage is lost. Suitable for temporary data only. Objective 1.6"
    },
    {
      "q": "Where are container images stored and shared?",
      "options": [
        "Local disk only",
        "Image registries",
        "Virtual machines",
        "Block storage"
      ],
      "answer": 1,
      "explanation": "Image registries (Docker Hub, ECR, ACR, GCR) store and distribute container images. Organizations use private registries for proprietary images. Objective 1.6"
    },
    {
      "q": "What is the primary purpose of container orchestration?",
      "options": [
        "Running single containers",
        "Automating deployment, scaling, and management of containerized applications",
        "Creating container images",
        "Monitoring only"
      ],
      "answer": 1,
      "explanation": "Orchestration automates container lifecycle management including deployment, scaling, networking, load balancing, health checks, and rolling updates across clusters. Objective 1.6"
    },
    {
      "q": "Which networking challenge is unique to containers?",
      "options": [
        "IP addressing",
        "DNS resolution",
        "Inter-container communication across hosts",
        "Bandwidth limitations"
      ],
      "answer": 2,
      "explanation": "Containers need networking solutions (overlay networks, service mesh) for communication across different hosts while maintaining isolation and security. Objective 1.6"
    },
    {
      "q": "A database container requires data persistence after restarts. Which storage type should be used?",
      "options": [
        "Ephemeral storage",
        "Container filesystem",
        "Persistent volumes",
        "Temporary storage"
      ],
      "answer": 2,
      "explanation": "Databases require persistent volumes that survive container restarts and deletions. Persistent volumes are mounted to containers providing durable storage. Objective 1.6"
    },
    {
      "q": "What is the relationship between container images and registries?",
      "options": [
        "Images are stored in registries",
        "Registries run inside images",
        "No relationship",
        "Registries create images"
      ],
      "answer": 0,
      "explanation": "Container registries store, version, and distribute container images. Developers push images to registries, and production systems pull images from registries. Objective 1.6"
    },
    {
      "q": "What is VM clustering used for?",
      "options": [
        "Reducing costs",
        "Providing high availability and resource pooling",
        "Faster networking",
        "Storage expansion"
      ],
      "answer": 1,
      "explanation": "VM clustering groups hosts into a resource pool, enabling high availability, load balancing, and resource sharing. VMs can be migrated between cluster hosts. Objective 1.7"
    },
    {
      "q": "What does VM cloning accomplish?",
      "options": [
        "Deletes original VM",
        "Creates an exact copy of a VM",
        "Upgrades VM",
        "Moves VM to another host"
      ],
      "answer": 1,
      "explanation": "Cloning creates identical copies of VMs including OS, applications, and configuration, enabling rapid deployment of standardized environments. Objective 1.7"
    },
    {
      "q": "What is host affinity used for?",
      "options": [
        "Keeping VMs on specific hosts",
        "Load balancing",
        "Storage allocation",
        "Network configuration"
      ],
      "answer": 0,
      "explanation": "Host affinity rules ensure VMs run on specific hosts, useful for licensing, performance optimization, or keeping related VMs together. Objective 1.7"
    },
    {
      "q": "Hardware pass-through allows:",
      "options": [
        "VMs to share CPU",
        "VMs to directly access physical hardware",
        "VMs to be cloned",
        "VMs to migrate"
      ],
      "answer": 1,
      "explanation": "Pass-through gives VMs exclusive, direct access to physical devices (GPUs, network cards), bypassing hypervisor for maximum performance. Objective 1.7"
    },
    {
      "q": "Which network type is used for VM traffic in virtualized environments?",
      "options": [
        "Physical networks only",
        "VM networks (virtual switches)",
        "Internet only",
        "Wireless networks"
      ],
      "answer": 1,
      "explanation": "VM networks use virtual switches connecting VMs to each other and physical networks. Virtual switches provide network isolation, VLANs, and traffic management. Objective 1.7"
    },
    {
      "q": "What is an overlay network in virtualization?",
      "options": [
        "Physical network cabling",
        "Virtual network built on top of physical network",
        "Storage network",
        "Management network"
      ],
      "answer": 1,
      "explanation": "Overlay networks create virtual networks on existing physical infrastructure using encapsulation, enabling network virtualization and multi-tenancy. Objective 1.7"
    },
    {
      "q": "Which storage type provides shared storage for VM clusters?",
      "options": [
        "Local storage only",
        "SAN or NAS",
        "USB drives",
        "CD-ROM"
      ],
      "answer": 1,
      "explanation": "SAN (Storage Area Network) and NAS (Network Attached Storage) provide shared storage accessible by multiple hosts, enabling VM migration and high availability. Objective 1.7"
    },
    {
      "q": "What is the primary advantage of local storage for VMs?",
      "options": [
        "Enables VM migration",
        "Lower latency and higher performance",
        "Shared across hosts",
        "Better redundancy"
      ],
      "answer": 1,
      "explanation": "Local storage (direct-attached disks) provides lowest latency and highest performance but limits VM migration and high availability features. Objective 1.7"
    },
    {
      "q": "Which billing model requires upfront commitment for cost savings?",
      "options": [
        "Pay-as-you-go",
        "Reserved instances",
        "Spot instances",
        "Free tier"
      ],
      "answer": 1,
      "explanation": "Reserved instances require 1-3 year commitments in exchange for significant discounts (up to 75%) compared to on-demand pricing. Objective 1.8"
    },
    {
      "q": "Spot instances are BEST used for:",
      "options": [
        "Production databases",
        "Mission-critical applications",
        "Fault-tolerant, interruptible workloads",
        "Real-time applications"
      ],
      "answer": 2,
      "explanation": "Spot instances offer steep discounts (up to 90%) but can be terminated with short notice. Ideal for batch processing, data analysis, and stateless workloads. Objective 1.8"
    },
    {
      "q": "What is a dedicated host?",
      "options": [
        "Shared physical server",
        "Physical server dedicated to one customer",
        "Virtual machine",
        "Container"
      ],
      "answer": 1,
      "explanation": "Dedicated hosts provide entire physical servers for single-tenant use, meeting licensing requirements and regulatory compliance but at premium cost. Objective 1.8"
    },
    {
      "q": "Pay-as-you-go billing is BEST for:",
      "options": [
        "Predictable, steady workloads",
        "Unknown or variable workloads",
        "Long-term commitments",
        "Testing only"
      ],
      "answer": 1,
      "explanation": "Pay-as-you-go (on-demand) offers flexibility without commitment, ideal for unpredictable workloads, testing, development, or short-term projects. Objective 1.8"
    },
    {
      "q": "What is resource metering used for?",
      "options": [
        "Security monitoring",
        "Tracking resource consumption for billing",
        "Performance tuning",
        "Backup"
      ],
      "answer": 1,
      "explanation": "Resource metering tracks usage of compute, storage, network, and other resources, enabling accurate billing, cost allocation, and usage analysis. Objective 1.8"
    },
    {
      "q": "Resource tagging enables:",
      "options": [
        "Faster processing",
        "Cost allocation and organization",
        "Better security",
        "Higher availability"
      ],
      "answer": 1,
      "explanation": "Tags are metadata labels (department, project, environment) enabling cost tracking, resource organization, automated policies, and chargeback/showback. Objective 1.8"
    },
    {
      "q": "What is rightsizing?",
      "options": [
        "Increasing all resources",
        "Matching resource allocation to actual needs",
        "Using largest instances",
        "Deleting all resources"
      ],
      "answer": 1,
      "explanation": "Rightsizing analyzes resource usage and adjusts instance sizes to match actual requirements, eliminating over-provisioning and reducing costs without impacting performance. Objective 1.8"
    },
    {
      "q": "A company has steady, predictable workloads running 24/7. Which billing model offers best savings?",
      "options": [
        "Pay-as-you-go",
        "Reserved instances",
        "Spot instances",
        "Free tier"
      ],
      "answer": 1,
      "explanation": "Reserved instances provide maximum savings (up to 75%) for steady, predictable workloads with long-term commitment, amortizing cost over 1-3 years. Objective 1.8"
    },
    {
      "q": "Which approach reduces costs for development environments?",
      "options": [
        "Run 24/7 like production",
        "Auto-shutdown during non-business hours",
        "Use largest instances",
        "Avoid monitoring"
      ],
      "answer": 1,
      "explanation": "Auto-shutdown schedules (nights, weekends) for non-production environments can reduce compute costs by 70% while maintaining availability during business hours. Objective 1.8"
    },
    {
      "q": "What is the primary cost benefit of spot instances?",
      "options": [
        "Better performance",
        "Up to 90% cost savings",
        "Guaranteed availability",
        "More features"
      ],
      "answer": 1,
      "explanation": "Spot instances utilize spare capacity at steep discounts (up to 90%) but can be terminated by the provider with minimal notice when capacity is needed. Objective 1.8"
    },
    {
      "q": "Resource tagging strategy should include:",
      "options": [
        "Random tag names",
        "Standardized tags for cost center, project, environment",
        "No tags needed",
        "Only one tag per resource"
      ],
      "answer": 1,
      "explanation": "Standardized tagging taxonomy (cost center, project, environment, owner) enables accurate cost allocation, policy enforcement, and resource management. Objective 1.8"
    },
    {
      "q": "Which scenario BEST justifies dedicated host usage?",
      "options": [
        "Cost savings",
        "Development environments",
        "Software with per-socket licensing",
        "Temporary workloads"
      ],
      "answer": 2,
      "explanation": "Dedicated hosts support bring-your-own-license (BYOL) scenarios where software is licensed per physical socket/core, potentially reducing overall licensing costs. Objective 1.8"
    },
    {
      "q": "Which database type uses tables with predefined schemas?",
      "options": [
        "Relational",
        "Non-relational",
        "Graph",
        "Time-series"
      ],
      "answer": 0,
      "explanation": "Relational databases use structured tables with defined schemas, relationships, and SQL for querying. Examples: MySQL, PostgreSQL, SQL Server. Objective 1.9"
    },
    {
      "q": "What is the primary advantage of non-relational databases?",
      "options": [
        "Better for complex joins",
        "Flexible schema and horizontal scalability",
        "ACID compliance always",
        "SQL support"
      ],
      "answer": 1,
      "explanation": "Non-relational (NoSQL) databases offer flexible schemas, easier horizontal scaling, and better performance for specific use cases like key-value or document storage. Objective 1.9"
    },
    {
      "q": "A self-managed database provides:",
      "options": [
        "Automatic patching",
        "Full control over configuration and management",
        "Lower operational overhead",
        "No maintenance required"
      ],
      "answer": 1,
      "explanation": "Self-managed databases give complete control but require manual patching, backups, scaling, and monitoring, increasing operational burden. Objective 1.9"
    },
    {
      "q": "Provider-managed database services handle:",
      "options": [
        "Only backup",
        "Patching, backups, replication, and scaling",
        "Only storage",
        "Application logic"
      ],
      "answer": 1,
      "explanation": "Managed database services automate patching, backups, replication, monitoring, and scaling, reducing operational overhead while maintaining high availability. Objective 1.9"
    },
    {
      "q": "Which database type is BEST for hierarchical JSON data?",
      "options": [
        "Relational",
        "Document database",
        "Graph database",
        "Key-value store"
      ],
      "answer": 1,
      "explanation": "Document databases (MongoDB, DynamoDB) natively store and query JSON/BSON documents, ideal for hierarchical, semi-structured data. Objective 1.9"
    },
    {
      "q": "When should serverless compute be chosen over VMs?",
      "options": [
        "Long-running processes",
        "Event-driven, short-duration workloads",
        "Legacy applications",
        "High memory requirements"
      ],
      "answer": 1,
      "explanation": "Serverless (FaaS) is ideal for event-driven, short-duration workloads with variable usage, eliminating infrastructure management and paying only for execution time. Objective 1.10"
    },
    {
      "q": "What metric indicates storage performance?",
      "options": [
        "Capacity only",
        "IOPS and throughput",
        "Latency only",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "IOPS (Input/Output Operations Per Second) and throughput (MB/s) measure storage performance. Different workloads require different IOPS/throughput levels. Objective 1.10"
    },
    {
      "q": "Which workload optimization improves application responsiveness?",
      "options": [
        "Increasing storage capacity",
        "Reducing network latency",
        "Adding more features",
        "Using older instances"
      ],
      "answer": 1,
      "explanation": "Network latency directly impacts application responsiveness. Optimizing network paths, using CDN, and region selection reduce latency. Objective 1.10"
    },
    {
      "q": "Workflow orchestration enables:",
      "options": [
        "Manual task execution",
        "Automated coordination of multi-step processes",
        "Single-task execution",
        "UI design"
      ],
      "answer": 1,
      "explanation": "Workflow orchestration automates complex, multi-step processes, coordinating tasks, handling dependencies, retries, and error handling across distributed systems. Objective 1.10"
    },
    {
      "q": "When should containers be chosen over VMs?",
      "options": [
        "Legacy applications requiring full OS",
        "Microservices with fast startup and scaling needs",
        "Windows-only applications",
        "GUI-intensive applications"
      ],
      "answer": 1,
      "explanation": "Containers offer faster startup, higher density, and better resource utilization for microservices and cloud-native applications compared to full VMs. Objective 1.10"
    },
    {
      "q": "Which AI/ML service converts spoken words to text?",
      "options": [
        "Text-to-voice",
        "Visual recognition",
        "Voice-to-text",
        "Sentiment analysis"
      ],
      "answer": 2,
      "explanation": "Voice-to-text (speech recognition) services transcribe audio to text, enabling voice interfaces, transcription services, and accessibility features. Objective 1.11"
    },
    {
      "q": "What does sentiment analysis determine?",
      "options": [
        "Image content",
        "Emotional tone of text",
        "Speech accuracy",
        "Translation quality"
      ],
      "answer": 1,
      "explanation": "Sentiment analysis uses NLP to determine emotional tone (positive, negative, neutral) in text, useful for customer feedback, social media monitoring, and reviews. Objective 1.11"
    },
    {
      "q": "Which component processes data from IoT sensors?",
      "options": [
        "IoT gateway",
        "Load balancer",
        "CDN",
        "Firewall"
      ],
      "answer": 0,
      "explanation": "IoT gateways aggregate, filter, and process data from sensors before sending to cloud, reducing bandwidth and enabling edge processing. Objective 1.11"
    },
    {
      "q": "Generative AI is BEST described as:",
      "options": [
        "AI that only classifies data",
        "AI that creates new content (text, images, code)",
        "AI that only analyzes data",
        "Traditional machine learning"
      ],
      "answer": 1,
      "explanation": "Generative AI creates new content based on training data, including text generation, image synthesis, code generation, and creative applications. Objective 1.11"
    },
    {
      "q": "IoT sensors typically communicate using:",
      "options": [
        "Only WiFi",
        "Low-power protocols like LoRaWAN, Zigbee, MQTT",
        "Only Ethernet",
        "Only cellular"
      ],
      "answer": 1,
      "explanation": "IoT devices use energy-efficient protocols (MQTT, CoAP, LoRaWAN, Zigbee) optimized for low power consumption, low bandwidth, and intermittent connectivity. Objective 1.11"
    }
  ],
  "2. Deployment": [
    {
      "q": "Which cloud deployment model provides exclusive resources dedicated to a single organization?",
      "options": [
        "Public cloud",
        "Private cloud",
        "Hybrid cloud",
        "Community cloud"
      ],
      "answer": 1,
      "explanation": "Private cloud provides infrastructure exclusively for one organization, offering maximum control, security, and customization but at higher cost. Can be hosted on-premises or by third parties. Objective 2.1"
    },
    {
      "q": "A company wants to keep sensitive data on-premises while using public cloud for non-sensitive workloads. Which model fits?",
      "options": [
        "Public cloud",
        "Private cloud",
        "Hybrid cloud",
        "Community cloud"
      ],
      "answer": 2,
      "explanation": "Hybrid cloud combines private (on-premises/dedicated) and public cloud, allowing organizations to keep sensitive data private while leveraging public cloud scalability for other workloads. Objective 2.1"
    },
    {
      "q": "Which deployment model shares infrastructure among organizations with common concerns (compliance, security)?",
      "options": [
        "Public cloud",
        "Private cloud",
        "Hybrid cloud",
        "Community cloud"
      ],
      "answer": 3,
      "explanation": "Community cloud shares infrastructure among organizations with shared requirements (regulatory, security, compliance), distributing costs while meeting specific industry needs. Objective 2.1"
    },
    {
      "q": "What is the primary advantage of public cloud deployment?",
      "options": [
        "Maximum control",
        "Exclusive resources",
        "Lowest cost and scalability",
        "Best for highly sensitive data"
      ],
      "answer": 2,
      "explanation": "Public cloud offers lowest cost through resource sharing, virtually unlimited scalability, and no infrastructure management, ideal for non-sensitive workloads and variable demand. Objective 2.1"
    },
    {
      "q": "An on-premises private cloud deployment provides:",
      "options": [
        "Shared multi-tenant resources",
        "Infrastructure within organization's data center",
        "Public internet access only",
        "No control over hardware"
      ],
      "answer": 1,
      "explanation": "On-premises private cloud runs in the organization's own data center, providing complete control over physical infrastructure, security, and compliance but requiring capital investment. Objective 2.1"
    },
    {
      "q": "Which deployment model is BEST for unpredictable workloads with tight budget?",
      "options": [
        "On-premises private",
        "Hosted private",
        "Public cloud",
        "Community cloud"
      ],
      "answer": 2,
      "explanation": "Public cloud's pay-as-you-go model and elastic scaling make it ideal for unpredictable workloads, avoiding over-provisioning and minimizing costs during low-demand periods. Objective 2.1"
    },
    {
      "q": "Healthcare organizations sharing HIPAA-compliant infrastructure exemplify which model?",
      "options": [
        "Public cloud",
        "Private cloud",
        "Hybrid cloud",
        "Community cloud"
      ],
      "answer": 3,
      "explanation": "Community cloud serves organizations with common compliance requirements (HIPAA, GDPR), sharing costs of specialized infrastructure while meeting regulatory standards. Objective 2.1"
    },
    {
      "q": "What is a key disadvantage of private cloud deployment?",
      "options": [
        "Less control",
        "Shared resources",
        "Higher cost and complexity",
        "Limited customization"
      ],
      "answer": 2,
      "explanation": "Private cloud requires significant capital investment, ongoing maintenance, staffing, and infrastructure management, resulting in higher total cost and operational complexity. Objective 2.1"
    },
    {
      "q": "Hybrid cloud requires which technical capability?",
      "options": [
        "Only VPN",
        "Integration and orchestration between cloud environments",
        "Single management console only",
        "No network connectivity"
      ],
      "answer": 1,
      "explanation": "Hybrid cloud requires seamless integration, orchestration, data synchronization, and consistent management across private and public environments, often using specialized tools. Objective 2.1"
    },
    {
      "q": "A startup with limited capital and no data center should choose:",
      "options": [
        "On-premises private cloud",
        "Public cloud",
        "Community cloud",
        "Build new data center"
      ],
      "answer": 1,
      "explanation": "Public cloud eliminates capital expenditure and infrastructure management, enabling startups to focus on product development while accessing enterprise-grade infrastructure on-demand. Objective 2.1"
    },
    {
      "q": "Which deployment model offers the MOST customization and control?",
      "options": [
        "Public cloud",
        "Private cloud on-premises",
        "Hybrid cloud",
        "Community cloud"
      ],
      "answer": 1,
      "explanation": "On-premises private cloud provides complete control over hardware, software, network, security, and configuration, enabling maximum customization for specific requirements. Objective 2.1"
    },
    {
      "q": "What challenge is unique to hybrid cloud deployments?",
      "options": [
        "No internet access",
        "Managing complexity across multiple environments",
        "Cannot scale",
        "No security"
      ],
      "answer": 1,
      "explanation": "Hybrid cloud introduces complexity in managing, securing, and integrating disparate environments, requiring consistent policies, identity management, and data synchronization. Objective 2.1"
    },
    {
      "q": "Public cloud multi-tenancy means:",
      "options": [
        "Single customer per server",
        "Multiple customers sharing physical infrastructure",
        "Private networks only",
        "No resource sharing"
      ],
      "answer": 1,
      "explanation": "Public cloud uses multi-tenancy where multiple customers securely share physical infrastructure through virtualization, enabling cost efficiency while maintaining isolation. Objective 2.1"
    },
    {
      "q": "A company must maintain data sovereignty in specific country. Which approach is BEST?",
      "options": [
        "Any public cloud region",
        "Private cloud in that country",
        "Community cloud anywhere",
        "No cloud usage"
      ],
      "answer": 1,
      "explanation": "Data sovereignty requires data to remain within specific geographic boundaries. Private cloud or public cloud regions within that country ensure compliance with local data laws. Objective 2.1"
    },
    {
      "q": "Which deployment model provides fastest time-to-market for new applications?",
      "options": [
        "On-premises private cloud",
        "Building new data center",
        "Public cloud",
        "Physical infrastructure"
      ],
      "answer": 2,
      "explanation": "Public cloud offers instant access to resources without procurement, installation, or configuration delays, enabling rapid application deployment and iteration. Objective 2.1"
    },
    {
      "q": "Private cloud hosted by third-party provider is called:",
      "options": [
        "Public cloud",
        "On-premises cloud",
        "Hosted private cloud",
        "Community cloud"
      ],
      "answer": 2,
      "explanation": "Hosted private cloud provides dedicated infrastructure managed by third-party provider, offering private cloud benefits without on-premises infrastructure investment. Objective 2.1"
    },
    {
      "q": "What is a primary driver for hybrid cloud adoption?",
      "options": [
        "Avoiding cloud completely",
        "Balancing control and scalability",
        "Eliminating all on-premises",
        "Single vendor lock-in"
      ],
      "answer": 1,
      "explanation": "Hybrid cloud balances private cloud control/security for sensitive workloads with public cloud scalability/cost-efficiency for variable workloads, optimizing both. Objective 2.1"
    },
    {
      "q": "Which deployment model has lowest barrier to entry?",
      "options": [
        "Private on-premises",
        "Hosted private",
        "Public cloud",
        "Community cloud"
      ],
      "answer": 2,
      "explanation": "Public cloud requires no capital investment, infrastructure, or specialized staff, allowing organizations to start immediately with pay-as-you-go pricing. Objective 2.1"
    },
    {
      "q": "Community cloud is MOST suitable for:",
      "options": [
        "Any organization",
        "Organizations with shared regulatory requirements",
        "Single large enterprise",
        "Personal use"
      ],
      "answer": 1,
      "explanation": "Community cloud suits organizations sharing compliance, security, or industry-specific requirements (finance, healthcare, government), distributing specialized infrastructure costs. Objective 2.1"
    },
    {
      "q": "What security advantage does private cloud offer over public cloud?",
      "options": [
        "No security needed",
        "Greater control over security controls and data location",
        "Automatic security",
        "No encryption needed"
      ],
      "answer": 1,
      "explanation": "Private cloud provides complete control over security architecture, configurations, physical access, and data location, critical for highly regulated industries and sensitive data. Objective 2.1"
    },
    {
      "q": "Blue-green deployment maintains two identical environments. What is the primary benefit?",
      "options": [
        "Lower cost",
        "Zero-downtime rollback",
        "Slower deployment",
        "Single environment"
      ],
      "answer": 1,
      "explanation": "Blue-green deployment enables instant rollback by switching traffic back to the previous environment if issues arise, minimizing downtime and risk. Objective 2.2"
    },
    {
      "q": "In a canary deployment, what percentage of traffic typically goes to the new version initially?",
      "options": [
        "100%",
        "50%",
        "Small percentage (5-10%)",
        "0%"
      ],
      "answer": 2,
      "explanation": "Canary deployments gradually roll out to small user subset (5-10%) first, allowing real-world testing and monitoring before full deployment, minimizing blast radius. Objective 2.2"
    },
    {
      "q": "Which deployment strategy updates instances incrementally in batches?",
      "options": [
        "Blue-green",
        "Canary",
        "Rolling",
        "In-place"
      ],
      "answer": 2,
      "explanation": "Rolling deployment updates instances in batches (e.g., 25% at a time), maintaining service availability while gradually deploying new version across entire fleet. Objective 2.2"
    },
    {
      "q": "In-place deployment updates the application on existing infrastructure. What is the main risk?",
      "options": [
        "Too fast",
        "Downtime during update",
        "Too expensive",
        "No risk"
      ],
      "answer": 1,
      "explanation": "In-place deployments update servers directly, causing downtime unless additional measures are taken. Rollback is slower and more complex than other strategies. Objective 2.2"
    },
    {
      "q": "Which deployment strategy is FASTEST for complete rollback?",
      "options": [
        "Rolling",
        "Canary",
        "Blue-green",
        "In-place"
      ],
      "answer": 2,
      "explanation": "Blue-green allows instant rollback by redirecting traffic back to the blue (previous) environment, typically taking seconds compared to minutes/hours for other strategies. Objective 2.2"
    },
    {
      "q": "A company wants to test new features with 5% of users before full release. Which strategy?",
      "options": [
        "Blue-green",
        "Rolling",
        "Canary",
        "In-place"
      ],
      "answer": 2,
      "explanation": "Canary deployment routes small percentage of traffic to new version, enabling real-user testing and gradual rollout based on metrics and feedback. Objective 2.2"
    },
    {
      "q": "What is a disadvantage of blue-green deployment?",
      "options": [
        "Slow rollback",
        "Requires double infrastructure temporarily",
        "Complex to implement",
        "Cannot monitor"
      ],
      "answer": 1,
      "explanation": "Blue-green requires maintaining two complete environments simultaneously, doubling infrastructure costs temporarily but providing safest deployment approach. Objective 2.2"
    },
    {
      "q": "Rolling deployment reduces risk by:",
      "options": [
        "Deploying to all at once",
        "Updating in incremental batches",
        "No updates",
        "Manual testing only"
      ],
      "answer": 1,
      "explanation": "Rolling deployments update small batches incrementally, limiting impact of issues to subset of infrastructure while maintaining overall service availability. Objective 2.2"
    },
    {
      "q": "Which strategy is MOST cost-effective but HIGHEST risk?",
      "options": [
        "Blue-green",
        "Canary",
        "Rolling",
        "In-place"
      ],
      "answer": 3,
      "explanation": "In-place deployment requires no additional infrastructure (lowest cost) but directly modifies production environment, increasing risk and complexity of rollback. Objective 2.2"
    },
    {
      "q": "During blue-green deployment, traffic is switched using:",
      "options": [
        "Manual file copy",
        "Load balancer or DNS update",
        "Reboot all servers",
        "Delete old servers"
      ],
      "answer": 1,
      "explanation": "Blue-green uses load balancer reconfiguration or DNS updates to instantly switch traffic between blue and green environments, enabling zero-downtime cutover. Objective 2.2"
    },
    {
      "q": "What monitoring is critical during canary deployment?",
      "options": [
        "No monitoring needed",
        "Error rates, latency, and business metrics",
        "Storage only",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Canary deployments require intensive monitoring of error rates, performance, and business KPIs to detect issues early before expanding rollout to all users. Objective 2.2"
    },
    {
      "q": "A critical bug is found 30 minutes after blue-green deployment. What is the fastest remediation?",
      "options": [
        "Fix bug in place",
        "Switch traffic back to blue environment",
        "Restart all servers",
        "Deploy new version"
      ],
      "answer": 1,
      "explanation": "Blue-green's key advantage is instant rollback - simply redirect traffic back to the previous (blue) environment in seconds, immediately restoring service. Objective 2.2"
    },
    {
      "q": "Rolling deployment batch size affects:",
      "options": [
        "Only cost",
        "Trade-off between speed and risk",
        "Security only",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Larger batches deploy faster but increase risk impact; smaller batches are safer but slower. Organizations balance based on risk tolerance and SLAs. Objective 2.2"
    },
    {
      "q": "Which deployment strategy requires no additional infrastructure?",
      "options": [
        "Blue-green",
        "Canary with separate fleet",
        "Rolling",
        "In-place"
      ],
      "answer": 3,
      "explanation": "In-place deployment updates existing infrastructure without additional resources, making it most cost-effective but requiring downtime and complicating rollback. Objective 2.2"
    },
    {
      "q": "Canary deployment is named after:",
      "options": [
        "Color yellow",
        "Canary islands",
        "Canary in coal mine (early warning)",
        "Random choice"
      ],
      "answer": 2,
      "explanation": "Named after canaries in coal mines used to detect dangerous gases early, canary deployments expose small user group first to detect problems before full rollout. Objective 2.2"
    },
    {
      "q": "What happens to blue environment after successful blue-green deployment?",
      "options": [
        "Deleted immediately",
        "Kept temporarily for rollback, then repurposed",
        "Nothing",
        "Becomes production"
      ],
      "answer": 1,
      "explanation": "Blue environment is maintained temporarily (hours/days) to enable quick rollback if issues emerge, then typically becomes the next green (staging) environment. Objective 2.2"
    },
    {
      "q": "A company has zero downtime tolerance. Which strategy is BEST?",
      "options": [
        "In-place",
        "Rolling with small batches",
        "Blue-green",
        "Manual deployment"
      ],
      "answer": 2,
      "explanation": "Blue-green provides true zero-downtime deployment by switching traffic between complete environments, maintaining full capacity throughout deployment. Objective 2.2"
    },
    {
      "q": "During rolling deployment, minimum capacity should be:",
      "options": [
        "0%",
        "Greater than minimum required for acceptable performance",
        "100%",
        "10%"
      ],
      "answer": 1,
      "explanation": "Rolling deployment must maintain sufficient capacity during updates. If batch size is 25%, remaining 75% must handle full load, requiring over-provisioning. Objective 2.2"
    },
    {
      "q": "Which strategy provides the BEST real-world testing before full rollout?",
      "options": [
        "In-place",
        "Blue-green",
        "Canary",
        "Rolling"
      ],
      "answer": 2,
      "explanation": "Canary deployment exposes new version to real users and production traffic in controlled manner, revealing issues that may not surface in pre-production testing. Objective 2.2"
    },
    {
      "q": "What is feature toggling in deployment context?",
      "options": [
        "Turning off servers",
        "Enabling/disabling features without redeployment",
        "Load balancing",
        "Backup strategy"
      ],
      "answer": 1,
      "explanation": "Feature toggles (flags) enable/disable features at runtime without deployment, supporting gradual rollout, A/B testing, and instant rollback of problematic features. Objective 2.2"
    },
    {
      "q": "Blue-green deployment database changes require:",
      "options": [
        "No special consideration",
        "Backward compatibility or data migration strategy",
        "Complete database replacement",
        "No changes allowed"
      ],
      "answer": 1,
      "explanation": "Database changes must be backward compatible with blue environment to enable rollback, or require careful migration strategy coordinated with deployment. Objective 2.2"
    },
    {
      "q": "Which deployment strategy is EASIEST to implement?",
      "options": [
        "Blue-green with automation",
        "Canary with monitoring",
        "Rolling with orchestration",
        "In-place manual"
      ],
      "answer": 3,
      "explanation": "In-place deployment is simplest to implement (update servers directly) but lacks sophisticated rollback, progressive exposure, and safety features of other strategies. Objective 2.2"
    },
    {
      "q": "Automated rollback in canary deployment is triggered by:",
      "options": [
        "Time elapsed",
        "Metrics exceeding error thresholds",
        "User complaints",
        "Random choice"
      ],
      "answer": 1,
      "explanation": "Automated canary deployments monitor metrics (error rates, latency, conversions) and automatically rollback when thresholds are exceeded, minimizing impact. Objective 2.2"
    },
    {
      "q": "Which strategy is BEST for microservices architecture?",
      "options": [
        "In-place all at once",
        "Independent rolling or canary per service",
        "Single blue-green for all",
        "Manual updates"
      ],
      "answer": 1,
      "explanation": "Microservices benefit from independent deployment strategies per service, enabling teams to deploy independently at different rates using rolling or canary approaches. Objective 2.2"
    },
    {
      "q": "Session persistence during blue-green cutover is handled by:",
      "options": [
        "Ignoring sessions",
        "Draining connections before switch or external session store",
        "Restarting all sessions",
        "No session support"
      ],
      "answer": 1,
      "explanation": "Blue-green deployments either drain existing connections gracefully before switching or use external session stores (Redis, database) accessible by both environments. Objective 2.2"
    },
    {
      "q": "What migration type moves workloads from on-premises to cloud?",
      "options": [
        "Cloud-to-cloud",
        "Cloud-to-on-premises",
        "On-premises-to-cloud",
        "Lateral migration"
      ],
      "answer": 2,
      "explanation": "On-premises-to-cloud migration moves applications and data from private infrastructure to cloud providers, the most common initial cloud migration scenario. Objective 2.3"
    },
    {
      "q": "Rehosting (lift-and-shift) migration means:",
      "options": [
        "Complete redesign",
        "Moving as-is with minimal changes",
        "Retiring application",
        "Refactoring code"
      ],
      "answer": 1,
      "explanation": "Rehosting moves applications to cloud with minimal/no code changes, fastest migration approach but doesn't leverage cloud-native features or optimization. Objective 2.3"
    },
    {
      "q": "Which migration strategy redesigns applications to use cloud-native features?",
      "options": [
        "Rehost",
        "Replatform",
        "Re-architect",
        "Retain"
      ],
      "answer": 2,
      "explanation": "Re-architecting (refactoring) involves significant code changes to leverage cloud capabilities like microservices, serverless, and managed services for maximum benefit. Objective 2.3"
    },
    {
      "q": "Replatforming involves:",
      "options": [
        "No changes",
        "Minor optimizations without full re-architecture",
        "Complete rewrite",
        "Application retirement"
      ],
      "answer": 1,
      "explanation": "Replatforming makes targeted optimizations (managed database, containerization) without full re-architecture, balancing effort and cloud benefits. Objective 2.3"
    },
    {
      "q": "When should the 'Retire' migration strategy be used?",
      "options": [
        "For all applications",
        "For applications no longer useful or redundant",
        "Never",
        "Only for new applications"
      ],
      "answer": 1,
      "explanation": "Retiring decommissions applications that are obsolete, redundant, or have low business value, reducing migration scope, costs, and future maintenance. Objective 2.3"
    },
    {
      "q": "What does 'Retain' strategy mean in migration context?",
      "options": [
        "Migrate everything",
        "Keep on-premises temporarily or permanently",
        "Delete application",
        "Rewrite application"
      ],
      "answer": 1,
      "explanation": "Retain keeps applications on-premises when migration isn't justified (regulatory, cost, risk, timing), often revisited in future migration waves. Objective 2.3"
    },
    {
      "q": "Which migration strategy is FASTEST but provides LEAST cloud benefit?",
      "options": [
        "Re-architect",
        "Replatform",
        "Rehost",
        "Refactor"
      ],
      "answer": 2,
      "explanation": "Rehosting (lift-and-shift) is fastest, moving applications as-is, but doesn't optimize for cloud, missing cost savings, scalability, and managed services benefits. Objective 2.3"
    },
    {
      "q": "What resource allocation consideration is critical for compute migration?",
      "options": [
        "Color schemes",
        "Right-sizing CPU and memory requirements",
        "Logo design",
        "Font choices"
      ],
      "answer": 1,
      "explanation": "Accurately sizing compute resources prevents over-provisioning (excess cost) or under-provisioning (performance issues), considering workload patterns and cloud instance types. Objective 2.3"
    },
    {
      "q": "Platform compatibility concerns during migration include:",
      "options": [
        "Only hardware",
        "OS compatibility, API differences, and service availability",
        "Only cost",
        "Only security"
      ],
      "answer": 1,
      "explanation": "Platform compatibility addresses OS versions, application dependencies, API differences between on-premises and cloud, and availability of required services. Objective 2.3"
    },
    {
      "q": "What storage consideration is important when migrating databases?",
      "options": [
        "Only capacity",
        "IOPS requirements, latency, and throughput",
        "Only cost",
        "Only color"
      ],
      "answer": 1,
      "explanation": "Database migration requires careful storage performance planning - IOPS, throughput, and latency directly impact application performance and user experience. Objective 2.3"
    },
    {
      "q": "Vendor lock-in risk in cloud migration refers to:",
      "options": [
        "Physical locks",
        "Difficulty switching providers due to proprietary services",
        "Security locks",
        "No risk"
      ],
      "answer": 1,
      "explanation": "Using proprietary cloud services creates dependencies making it difficult and costly to migrate to other providers, requiring careful architecture decisions. Objective 2.3"
    },
    {
      "q": "What networking consideration affects application migration?",
      "options": [
        "Only DNS",
        "Bandwidth, latency, connectivity between cloud and on-premises",
        "Only firewalls",
        "No networking needed"
      ],
      "answer": 1,
      "explanation": "Network bandwidth, latency between environments, hybrid connectivity, and data transfer requirements critically impact migration approach, timing, and application performance. Objective 2.3"
    },
    {
      "q": "Cloud migration reduces management overhead by:",
      "options": [
        "Increasing manual tasks",
        "Eliminating infrastructure management through managed services",
        "Requiring more staff",
        "No overhead change"
      ],
      "answer": 1,
      "explanation": "Managed services (databases, containers, serverless) eliminate infrastructure management tasks (patching, scaling, backups), redirecting resources to business value. Objective 2.3"
    },
    {
      "q": "What regulatory consideration can prevent cloud migration?",
      "options": [
        "Logo requirements",
        "Data residency and sovereignty requirements",
        "Color schemes",
        "No regulations apply"
      ],
      "answer": 1,
      "explanation": "Data sovereignty laws requiring data to remain in specific countries/regions may limit cloud provider/region choices or prevent migration entirely. Objective 2.3"
    },
    {
      "q": "Compliance considerations for cloud migration include:",
      "options": [
        "No compliance needed",
        "Industry standards (HIPAA, PCI-DSS) and audit requirements",
        "Only cost",
        "Only performance"
      ],
      "answer": 1,
      "explanation": "Migrating regulated workloads requires ensuring cloud environment meets industry compliance standards (HIPAA, PCI-DSS, SOC 2), potentially limiting options. Objective 2.3"
    },
    {
      "q": "What cost consideration is often overlooked in cloud migration?",
      "options": [
        "Monthly compute costs",
        "Data egress charges and data transfer costs",
        "Storage costs",
        "No hidden costs"
      ],
      "answer": 1,
      "explanation": "Data egress (transfer out of cloud) charges can be substantial and unexpected, especially for data-intensive applications or frequent data repatriation. Objective 2.3"
    },
    {
      "q": "Service availability considerations include:",
      "options": [
        "Only uptime",
        "Required SLA, multi-region failover, and disaster recovery",
        "Only cost",
        "No availability needed"
      ],
      "answer": 1,
      "explanation": "Migration planning must address availability requirements through appropriate cloud region selection, multi-region deployment, and disaster recovery architecture. Objective 2.3"
    },
    {
      "q": "Environmental considerations for cloud migration include:",
      "options": [
        "Logo colors",
        "Eliminating on-premises power and cooling costs",
        "Weather patterns",
        "No environmental impact"
      ],
      "answer": 1,
      "explanation": "Cloud migration eliminates on-premises data center power, cooling, and environmental management, reducing operational costs and often improving sustainability. Objective 2.3"
    },
    {
      "q": "A legacy application cannot be modified. Which migration strategy?",
      "options": [
        "Re-architect",
        "Refactor",
        "Rehost",
        "Must retire"
      ],
      "answer": 2,
      "explanation": "Applications that cannot be modified (legacy, vendor-provided, no source code) must be rehosted as-is, accepting limitations of non-cloud-optimized architecture. Objective 2.3"
    },
    {
      "q": "What is the 6 R's of cloud migration?",
      "options": [
        "Random strategies",
        "Rehost, Replatform, Re-architect, Retain, Retire, Refactor",
        "Only one strategy",
        "No defined strategies"
      ],
      "answer": 1,
      "explanation": "The 6 R's framework categorizes migration approaches: Rehost (lift-shift), Replatform (lift-tinker-shift), Re-architect/Refactor (modernize), Retain (keep), Retire (decommission). Objective 2.3"
    },
    {
      "q": "Cloud-to-cloud migration is driven by:",
      "options": [
        "No reason",
        "Multi-cloud strategy, cost optimization, or avoiding vendor lock-in",
        "Random choice",
        "Always required"
      ],
      "answer": 1,
      "explanation": "Organizations migrate between cloud providers for better pricing, different services, geographic presence, avoiding vendor lock-in, or disaster recovery. Objective 2.3"
    },
    {
      "q": "What tool helps assess on-premises infrastructure for cloud migration?",
      "options": [
        "Only manual spreadsheets",
        "Cloud migration assessment tools and discovery agents",
        "No tools available",
        "Only calculators"
      ],
      "answer": 1,
      "explanation": "Migration assessment tools (Azure Migrate, AWS Migration Hub) inventory infrastructure, analyze dependencies, estimate costs, and recommend migration strategies. Objective 2.3"
    },
    {
      "q": "Application dependencies must be identified before migration to:",
      "options": [
        "Increase costs",
        "Ensure all required components migrate together",
        "Slow down migration",
        "No reason"
      ],
      "answer": 1,
      "explanation": "Mapping dependencies (databases, APIs, services, storage) prevents broken functionality by ensuring related components migrate together or maintain connectivity. Objective 2.3"
    },
    {
      "q": "Data migration approach for large datasets (TB/PB) should consider:",
      "options": [
        "Only network transfer",
        "Physical data transfer appliances or hybrid sync",
        "Only FTP",
        "No special consideration"
      ],
      "answer": 1,
      "explanation": "Large datasets may require physical transfer appliances (AWS Snowball, Azure Data Box) or hybrid sync tools due to time and bandwidth constraints of network transfer. Objective 2.3"
    },
    {
      "q": "What is the pilot/wave migration approach?",
      "options": [
        "Migrate everything at once",
        "Migrate in phases, starting with low-risk applications",
        "Never migrate",
        "Random selection"
      ],
      "answer": 1,
      "explanation": "Phased migration starts with low-risk applications to build expertise and confidence, progressively migrating complex/critical applications after learning and refinement. Objective 2.3"
    },
    {
      "q": "Refactoring (re-architecting) is justified when:",
      "options": [
        "Never",
        "Significant long-term benefits outweigh re-architecture costs",
        "Always required",
        "Only for small apps"
      ],
      "answer": 1,
      "explanation": "Refactoring investment is justified when long-term benefits (cost reduction, scalability, agility) significantly outweigh re-architecture effort and costs. Objective 2.3"
    },
    {
      "q": "Cloud-to-on-premises migration (repatriation) occurs when:",
      "options": [
        "Required by law",
        "Cost, performance, or control needs aren't met in cloud",
        "Always happens",
        "Never happens"
      ],
      "answer": 1,
      "explanation": "Organizations repatriate workloads when cloud costs exceed expectations, performance requirements aren't met, or control/compliance needs favor on-premises. Objective 2.3"
    },
    {
      "q": "What testing is critical before production migration cutover?",
      "options": [
        "No testing needed",
        "End-to-end application, integration, and performance testing",
        "Only unit testing",
        "Only visual testing"
      ],
      "answer": 1,
      "explanation": "Comprehensive testing validates functionality, performance, integrations, and user experience in cloud environment before cutover, reducing production issues. Objective 2.3"
    },
    {
      "q": "Migration rollback plan should include:",
      "options": [
        "No rollback needed",
        "Steps to return to on-premises if migration fails",
        "Delete all data",
        "Forward only"
      ],
      "answer": 1,
      "explanation": "Rollback plans define steps to restore operations on-premises if critical issues emerge, maintaining on-premises environment until migration stability is proven. Objective 2.3"
    },
    {
      "q": "What is the primary benefit of replatforming over rehosting?",
      "options": [
        "Faster migration",
        "Some cloud optimization without full re-architecture",
        "Lower cost always",
        "No benefits"
      ],
      "answer": 1,
      "explanation": "Replatforming achieves meaningful cloud benefits (managed services, containers) with moderate effort, balancing quick wins against full re-architecture investment. Objective 2.3"
    },
    {
      "q": "What is Infrastructure as Code (IaC)?",
      "options": [
        "Manual server configuration",
        "Managing infrastructure through machine-readable files",
        "Only documentation",
        "Physical wiring"
      ],
      "answer": 1,
      "explanation": "IaC defines infrastructure (networks, VMs, storage) in code files that can be versioned, tested, and automatically provisioned, replacing manual configuration. Objective 2.4"
    },
    {
      "q": "Which format is commonly used for IaC templates?",
      "options": [
        "PDF",
        "JSON or YAML",
        "Word documents",
        "Images"
      ],
      "answer": 1,
      "explanation": "IaC tools use structured formats like JSON (JavaScript Object Notation) and YAML (YAML Ain't Markup Language) for human-readable, machine-parseable configuration. Objective 2.4"
    },
    {
      "q": "What is Configuration as Code (CaC)?",
      "options": [
        "Infrastructure provisioning",
        "Managing application and system configuration in version-controlled files",
        "Manual configuration",
        "No configuration"
      ],
      "answer": 1,
      "explanation": "CaC manages application settings, system configurations, and deployment parameters in code, enabling consistent, repeatable, and auditable configuration management. Objective 2.4"
    },
    {
      "q": "IaC provides repeatability by:",
      "options": [
        "Manual documentation",
        "Deploying identical infrastructure from same code",
        "Hoping for consistency",
        "One-time use"
      ],
      "answer": 1,
      "explanation": "IaC code produces identical infrastructure every execution, eliminating configuration drift and manual errors, ensuring dev, test, and prod environment consistency. Objective 2.4"
    },
    {
      "q": "What is configuration drift in infrastructure?",
      "options": [
        "Intended changes",
        "Unintended divergence from desired state",
        "Normal operations",
        "Improved configuration"
      ],
      "answer": 1,
      "explanation": "Configuration drift occurs when manual changes or ad-hoc modifications cause infrastructure to differ from defined standard, causing inconsistencies and issues. Objective 2.4"
    },
    {
      "q": "IaC enables drift detection by:",
      "options": [
        "Manual comparison",
        "Comparing current infrastructure state to code definition",
        "Guessing changes",
        "No detection possible"
      ],
      "answer": 1,
      "explanation": "IaC tools compare actual infrastructure state against code definitions, detecting unauthorized or unintended changes and enabling automated remediation. Objective 2.4"
    },
    {
      "q": "Version control for IaC provides:",
      "options": [
        "No benefits",
        "Change tracking, rollback capability, and collaboration",
        "Only storage",
        "Slower deployment"
      ],
      "answer": 1,
      "explanation": "Version control (Git) tracks all infrastructure changes, enables rollback to previous versions, facilitates team collaboration, and provides audit trail. Objective 2.4"
    },
    {
      "q": "IaC conditional logic uses which statement?",
      "options": [
        "No logic supported",
        "If-then-else statements",
        "Only hardcoded values",
        "Random selection"
      ],
      "answer": 1,
      "explanation": "IaC supports conditionals (if-then-else) to create flexible templates that deploy different resources based on environment, region, or input parameters. Objective 2.4"
    },
    {
      "q": "Variables in IaC enable:",
      "options": [
        "No flexibility",
        "Parameterizing templates for reusability across environments",
        "Only hardcoded values",
        "Slower deployment"
      ],
      "answer": 1,
      "explanation": "Variables allow single template to deploy different configurations (instance sizes, regions, counts) across dev/test/prod by passing different parameter values. Objective 2.4"
    },
    {
      "q": "What data types are common in IaC?",
      "options": [
        "Only text",
        "Strings, numbers, booleans, lists, and maps/objects",
        "No data types",
        "Only images"
      ],
      "answer": 1,
      "explanation": "IaC supports standard programming data types enabling complex configurations: strings (names), numbers (counts), booleans (flags), lists (arrays), maps (key-value pairs). Objective 2.4"
    },
    {
      "q": "Functions in IaC allow:",
      "options": [
        "No functions supported",
        "Reusable logic and transformations (concat, lookup, format)",
        "Only manual coding",
        "Deletion only"
      ],
      "answer": 1,
      "explanation": "IaC functions enable string manipulation, data lookups, arithmetic operations, and logic reuse, making templates more powerful and maintainable. Objective 2.4"
    },
    {
      "q": "IaC testing should include:",
      "options": [
        "No testing needed",
        "Syntax validation, policy checks, and deployment testing",
        "Only manual review",
        "Only production testing"
      ],
      "answer": 1,
      "explanation": "IaC testing includes syntax validation, security policy checks, cost estimation, and test deployments to catch errors before production deployment. Objective 2.4"
    },
    {
      "q": "IaC documentation should cover:",
      "options": [
        "Nothing",
        "Purpose, parameters, dependencies, and usage examples",
        "Only code comments",
        "No documentation needed"
      ],
      "answer": 1,
      "explanation": "IaC documentation explains template purpose, required/optional parameters, dependencies, usage examples, and outputs, enabling team adoption and maintenance. Objective 2.4"
    },
    {
      "q": "What is the primary advantage of IaC over manual configuration?",
      "options": [
        "Slower deployment",
        "Consistency, repeatability, and automation",
        "More errors",
        "No advantages"
      ],
      "answer": 1,
      "explanation": "IaC eliminates manual errors, ensures consistent deployments, enables rapid provisioning, and allows infrastructure to be treated like application code. Objective 2.4"
    },
    {
      "q": "IaC operators perform:",
      "options": [
        "No operations",
        "Mathematical, comparison, and logical operations",
        "Only manual tasks",
        "Physical work"
      ],
      "answer": 1,
      "explanation": "IaC supports operators for arithmetic (+, -, *, /), comparisons (>, <, ==), and logic (AND, OR, NOT), enabling complex conditional logic in templates. Objective 2.4"
    },
    {
      "q": "When provisioning storage, what requirement drives capacity planning?",
      "options": [
        "Random guess",
        "Current and projected data volume",
        "Always maximum",
        "Minimum possible"
      ],
      "answer": 1,
      "explanation": "Storage provisioning considers current data volume plus growth projections, backup requirements, and retention policies to ensure adequate capacity. Objective 2.5"
    },
    {
      "q": "Performance requirements for compute resources include:",
      "options": [
        "Only cost",
        "CPU, memory, network throughput, and IOPS needs",
        "Only color",
        "No requirements"
      ],
      "answer": 1,
      "explanation": "Compute provisioning balances CPU cores, memory, network bandwidth, and storage IOPS to meet application performance requirements within budget. Objective 2.5"
    },
    {
      "q": "Security requirements for provisioning include:",
      "options": [
        "No security needed",
        "Encryption, network isolation, access controls, and compliance",
        "Only passwords",
        "Physical locks"
      ],
      "answer": 1,
      "explanation": "Secure provisioning implements encryption (data at rest/transit), network segmentation, IAM policies, logging, and compliance controls from deployment. Objective 2.5"
    },
    {
      "q": "Cost requirements influence provisioning through:",
      "options": [
        "Ignoring cost",
        "Selecting appropriate instance types, reserved capacity, and autoscaling",
        "Always maximum resources",
        "No influence"
      ],
      "answer": 1,
      "explanation": "Cost-effective provisioning selects right-sized instances, uses reserved/spot instances where appropriate, implements autoscaling, and optimizes storage tiers. Objective 2.5"
    },
    {
      "q": "Availability requirements determine:",
      "options": [
        "Only one server",
        "Multi-zone deployment, load balancing, and failover configuration",
        "Single point of failure",
        "No redundancy"
      ],
      "answer": 1,
      "explanation": "High availability provisioning deploys across multiple availability zones, implements load balancing, auto-healing, and appropriate backup/DR strategies. Objective 2.5"
    },
    {
      "q": "Compliance requirements affect provisioning by:",
      "options": [
        "No effect",
        "Mandating specific regions, encryption, logging, and controls",
        "Only documentation",
        "Ignoring regulations"
      ],
      "answer": 1,
      "explanation": "Compliance drives region selection (data residency), encryption requirements, audit logging, access controls, and potentially dedicated infrastructure. Objective 2.5"
    },
    {
      "q": "Network requirements for provisioning include:",
      "options": [
        "Only internet",
        "Bandwidth, latency, connectivity to other resources, and isolation",
        "No networking",
        "Only WiFi"
      ],
      "answer": 1,
      "explanation": "Network provisioning considers bandwidth requirements, latency sensitivity, hybrid connectivity needs, VPC design, and security isolation requirements. Objective 2.5"
    },
    {
      "q": "What compute resource is BEST for unpredictable, event-driven workloads?",
      "options": [
        "Always-on VMs",
        "Serverless functions",
        "Physical servers",
        "Mainframes"
      ],
      "answer": 1,
      "explanation": "Serverless (FaaS) eliminates idle costs by executing only when triggered, automatically scaling from zero to thousands of concurrent executions. Objective 2.5"
    },
    {
      "q": "Storage provisioning for databases should prioritize:",
      "options": [
        "Only capacity",
        "IOPS, latency, and throughput matching database requirements",
        "Only cost",
        "Random selection"
      ],
      "answer": 1,
      "explanation": "Database storage requires careful IOPS and throughput provisioning as inadequate performance directly impacts application responsiveness and user experience. Objective 2.5"
    },
    {
      "q": "When provisioning resources, tagging enables:",
      "options": [
        "No benefit",
        "Cost allocation, automation, compliance tracking, and organization",
        "Only decoration",
        "Slower provisioning"
      ],
      "answer": 1,
      "explanation": "Resource tags enable cost attribution, automated policies, compliance reporting, resource lifecycle management, and organizational structure mapping. Objective 2.5"
    }
  ],
  "3. Security": [
    {
      "q": "What is the first step in vulnerability management?",
      "options": [
        "Remediation",
        "Assessment",
        "Defining scanning scope",
        "Reporting"
      ],
      "answer": 2,
      "explanation": "Vulnerability management begins with defining scanning scope - identifying which systems, networks, and applications will be scanned to ensure comprehensive coverage. Objective 4.1"
    },
    {
      "q": "After identifying vulnerabilities, what is the next step?",
      "options": [
        "Immediate patching",
        "Assessment of severity and impact",
        "Ignoring low-risk items",
        "Deleting systems"
      ],
      "answer": 1,
      "explanation": "Assessment evaluates vulnerability severity (CVSS scores), exploitability, business impact, and affected assets to prioritize remediation efforts effectively. Objective 4.1"
    },
    {
      "q": "What does CVE stand for in security context?",
      "options": [
        "Cloud Vulnerability Engine",
        "Common Vulnerabilities and Exposures",
        "Critical Virus Entry",
        "Certified Vulnerability Expert"
      ],
      "answer": 1,
      "explanation": "CVE (Common Vulnerabilities and Exposures) is a standardized identifier system for publicly known cybersecurity vulnerabilities, enabling consistent tracking and communication. Objective 4.1"
    },
    {
      "q": "Vulnerability remediation typically involves:",
      "options": [
        "Ignoring all findings",
        "Patching, configuration changes, or compensating controls",
        "Only documentation",
        "Deleting all systems"
      ],
      "answer": 1,
      "explanation": "Remediation addresses vulnerabilities through patching, configuration hardening, access controls, network segmentation, or compensating controls when direct fixes aren't possible. Objective 4.1"
    },
    {
      "q": "What is the purpose of vulnerability scanning?",
      "options": [
        "Create more vulnerabilities",
        "Identify security weaknesses before attackers exploit them",
        "Slow down systems",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Vulnerability scanning proactively identifies security weaknesses, misconfigurations, and missing patches, enabling remediation before attackers discover and exploit them. Objective 4.1"
    },
    {
      "q": "CVSS scoring ranges from:",
      "options": [
        "0-10",
        "0-100",
        "1-5",
        "A-F"
      ],
      "answer": 0,
      "explanation": "CVSS (Common Vulnerability Scoring System) uses 0-10 scale: 0.0 (none), 0.1-3.9 (low), 4.0-6.9 (medium), 7.0-8.9 (high), 9.0-10.0 (critical) to rate severity. Objective 4.1"
    },
    {
      "q": "Which vulnerability should be prioritized for remediation?",
      "options": [
        "Low severity, no exploits",
        "Critical severity, actively exploited, public-facing",
        "Informational findings",
        "Already patched systems"
      ],
      "answer": 1,
      "explanation": "Prioritize critical vulnerabilities that are actively exploited in the wild, on public-facing systems, or have high business impact over theoretical or low-risk issues. Objective 4.1"
    },
    {
      "q": "Authenticated vulnerability scans provide:",
      "options": [
        "Less information",
        "Deeper insight by checking internal configurations",
        "No difference",
        "Only external view"
      ],
      "answer": 1,
      "explanation": "Authenticated scans log into systems to check internal configurations, installed software, missing patches, and detailed settings that external scans cannot detect. Objective 4.1"
    },
    {
      "q": "How often should vulnerability scans be performed?",
      "options": [
        "Once per year",
        "Regularly (weekly/monthly) and after major changes",
        "Never",
        "Only when breached"
      ],
      "answer": 1,
      "explanation": "Continuous or regular scanning (weekly/monthly) plus event-driven scans after deployments, configuration changes, or new vulnerability disclosures ensure ongoing security. Objective 4.1"
    },
    {
      "q": "What is a false positive in vulnerability scanning?",
      "options": [
        "Confirmed vulnerability",
        "Scanner incorrectly reports vulnerability that doesn't exist",
        "Critical finding",
        "Patched system"
      ],
      "answer": 1,
      "explanation": "False positives occur when scanners incorrectly identify vulnerabilities due to misinterpretation, outdated signatures, or environmental factors, requiring validation before remediation. Objective 4.1"
    },
    {
      "q": "Zero-day vulnerabilities are:",
      "options": [
        "Already patched",
        "Unknown to vendor, no patch available",
        "Low severity",
        "Theoretical only"
      ],
      "answer": 1,
      "explanation": "Zero-day vulnerabilities are unknown to vendors with no patches available, making them highly dangerous and requiring compensating controls until patches are released. Objective 4.1"
    },
    {
      "q": "Compensating controls are used when:",
      "options": [
        "Patches are immediately available",
        "Direct remediation isn't possible or practical",
        "No vulnerabilities exist",
        "Systems are new"
      ],
      "answer": 1,
      "explanation": "Compensating controls (WAF, network segmentation, monitoring) mitigate risk when patching isn't possible due to legacy systems, vendor support, or business constraints. Objective 4.1"
    },
    {
      "q": "What information does a CVE entry provide?",
      "options": [
        "Only a number",
        "Vulnerability description, affected products, and severity",
        "Patch files",
        "Exploit code"
      ],
      "answer": 1,
      "explanation": "CVE entries provide standardized descriptions, affected products/versions, references, and often CVSS scores, enabling consistent vulnerability tracking across organizations. Objective 4.1"
    },
    {
      "q": "Vulnerability assessment differs from penetration testing by:",
      "options": [
        "They are identical",
        "Assessment identifies weaknesses; pen testing actively exploits them",
        "Assessment is manual only",
        "Pen testing is automated only"
      ],
      "answer": 1,
      "explanation": "Vulnerability assessment identifies and reports potential weaknesses using scanners; penetration testing actively attempts to exploit vulnerabilities to prove impact. Objective 4.1"
    },
    {
      "q": "What is vulnerability remediation SLA?",
      "options": [
        "No timeline needed",
        "Defined timeframe for fixing vulnerabilities based on severity",
        "Always 24 hours",
        "Always 1 year"
      ],
      "answer": 1,
      "explanation": "Remediation SLAs define maximum timeframes for fixing vulnerabilities based on severity (e.g., critical: 7 days, high: 30 days), ensuring timely risk reduction. Objective 4.1"
    },
    {
      "q": "Data sovereignty refers to:",
      "options": [
        "Data encryption",
        "Data subject to laws of country where it's physically located",
        "Data backup",
        "Data deletion"
      ],
      "answer": 1,
      "explanation": "Data sovereignty means data is subject to laws and regulations of the country where it physically resides, impacting cloud region selection and compliance. Objective 4.2"
    },
    {
      "q": "What is data locality?",
      "options": [
        "Data encryption method",
        "Physical location where data is stored",
        "Data format",
        "Data size"
      ],
      "answer": 1,
      "explanation": "Data locality refers to the physical geographic location where data is stored and processed, critical for compliance with data residency laws and performance. Objective 4.2"
    },
    {
      "q": "Who owns data in cloud environments?",
      "options": [
        "Cloud provider",
        "Customer (data owner)",
        "Government",
        "No one"
      ],
      "answer": 1,
      "explanation": "Customers retain data ownership in cloud environments. Providers manage infrastructure but customers own data and are responsible for its protection and compliance. Objective 4.2"
    },
    {
      "q": "Data classification categorizes data based on:",
      "options": [
        "File size",
        "Sensitivity and required protection level",
        "Color",
        "Age"
      ],
      "answer": 1,
      "explanation": "Data classification (public, internal, confidential, restricted) defines sensitivity levels, dictating appropriate security controls, access restrictions, and handling procedures. Objective 4.2"
    },
    {
      "q": "Litigation hold requires:",
      "options": [
        "Deleting all data",
        "Preserving data relevant to legal proceedings",
        "Encrypting everything",
        "No action"
      ],
      "answer": 1,
      "explanation": "Litigation hold mandates preserving potentially relevant data when legal action is anticipated or active, preventing deletion or modification until released. Objective 4.2"
    },
    {
      "q": "What drives contractual data retention requirements?",
      "options": [
        "Random choice",
        "Business agreements and SLAs defining retention periods",
        "No requirements",
        "Provider choice only"
      ],
      "answer": 1,
      "explanation": "Contracts may mandate specific retention periods for data (e.g., 7 years for financial records), requiring automated retention policies and deletion procedures. Objective 4.2"
    },
    {
      "q": "Regulatory data retention typically applies to:",
      "options": [
        "No data",
        "Specific data types per industry regulations",
        "All data forever",
        "Only logs"
      ],
      "answer": 1,
      "explanation": "Regulations mandate retention for specific data: healthcare (HIPAA), financial (SOX), personal data (GDPR), varying by industry and jurisdiction. Objective 4.2"
    },
    {
      "q": "SOC 2 compliance focuses on:",
      "options": [
        "Only security",
        "Security, availability, processing integrity, confidentiality, privacy",
        "Only backups",
        "Only cost"
      ],
      "answer": 1,
      "explanation": "SOC 2 evaluates service organizations against five Trust Service Criteria: security, availability, processing integrity, confidentiality, and privacy controls. Objective 4.2"
    },
    {
      "q": "PCI DSS applies to organizations that:",
      "options": [
        "Have any data",
        "Process, store, or transmit credit card information",
        "Only banks",
        "No organizations"
      ],
      "answer": 1,
      "explanation": "PCI DSS (Payment Card Industry Data Security Standard) mandates security requirements for any organization handling credit card data, regardless of size. Objective 4.2"
    },
    {
      "q": "ISO 27001 is a standard for:",
      "options": [
        "Building construction",
        "Information security management systems",
        "Network speeds",
        "Data center cooling"
      ],
      "answer": 1,
      "explanation": "ISO 27001 specifies requirements for establishing, implementing, maintaining, and improving information security management systems (ISMS). Objective 4.2"
    },
    {
      "q": "Cloud Security Alliance (CSA) provides:",
      "options": [
        "Cloud servers",
        "Security best practices and frameworks for cloud computing",
        "Only consulting",
        "No resources"
      ],
      "answer": 1,
      "explanation": "CSA develops cloud security guidance including Security Trust Assurance and Risk (STAR) certification, Cloud Controls Matrix (CCM), and best practices. Objective 4.2"
    },
    {
      "q": "GDPR primarily addresses:",
      "options": [
        "Only EU citizens",
        "Personal data privacy and protection",
        "Only email",
        "Only websites"
      ],
      "answer": 1,
      "explanation": "GDPR (General Data Protection Regulation) governs personal data of EU residents, requiring consent, right to erasure, breach notification, and data protection. Objective 4.2"
    },
    {
      "q": "Right to be forgotten under GDPR means:",
      "options": [
        "Never delete data",
        "Individuals can request deletion of their personal data",
        "Forget passwords",
        "No deletions allowed"
      ],
      "answer": 1,
      "explanation": "GDPR grants individuals right to have personal data erased under certain conditions, requiring organizations to implement data deletion capabilities. Objective 4.2"
    },
    {
      "q": "HIPAA compliance requires:",
      "options": [
        "No security",
        "Protecting patient health information confidentiality and integrity",
        "Only encryption",
        "Only backups"
      ],
      "answer": 1,
      "explanation": "HIPAA mandates administrative, physical, and technical safeguards to protect Protected Health Information (PHI) confidentiality, integrity, and availability. Objective 4.2"
    },
    {
      "q": "Data breach notification requirements typically mandate:",
      "options": [
        "Never notify",
        "Notifying affected parties within specific timeframe",
        "Only internal notification",
        "Ignore breaches"
      ],
      "answer": 1,
      "explanation": "Most regulations require breach notification to affected individuals, regulators, and sometimes public within specific timeframes (72 hours for GDPR). Objective 4.2"
    },
    {
      "q": "What is data residency?",
      "options": [
        "Data backup location",
        "Legal/regulatory requirement for data to remain in specific geographic location",
        "Data format",
        "Data age"
      ],
      "answer": 1,
      "explanation": "Data residency laws require certain data to be stored and processed within specific countries/regions, restricting cloud provider and region choices. Objective 4.2"
    },
    {
      "q": "Compliance audit trails require:",
      "options": [
        "No logging",
        "Comprehensive logging of access, changes, and administrative actions",
        "Only errors",
        "Manual notes"
      ],
      "answer": 1,
      "explanation": "Audit trails log who accessed what data, when, and what actions were performed, providing evidence for compliance audits and security investigations. Objective 4.2"
    },
    {
      "q": "What is a Data Protection Impact Assessment (DPIA)?",
      "options": [
        "Performance test",
        "Assessment of privacy risks in data processing",
        "Network scan",
        "Cost analysis"
      ],
      "answer": 1,
      "explanation": "DPIA evaluates privacy risks of data processing activities, required by GDPR for high-risk processing, identifying and mitigating privacy impacts. Objective 4.2"
    },
    {
      "q": "Cloud compliance responsibility follows:",
      "options": [
        "Only provider responsible",
        "Shared responsibility model",
        "Only customer responsible",
        "No responsibility"
      ],
      "answer": 1,
      "explanation": "Compliance follows shared responsibility: providers secure infrastructure and offer compliant services; customers properly configure and use services compliantly. Objective 4.2"
    },
    {
      "q": "FedRAMP applies to:",
      "options": [
        "Any organization",
        "Cloud services used by US federal agencies",
        "Only private sector",
        "Non-US only"
      ],
      "answer": 1,
      "explanation": "FedRAMP (Federal Risk and Authorization Management Program) standardizes security assessment and authorization for cloud services used by US government. Objective 4.2"
    },
    {
      "q": "Which method provides secure programmatic access to cloud management?",
      "options": [
        "Hardcoded passwords",
        "API keys or service accounts",
        "Sharing credentials",
        "No authentication"
      ],
      "answer": 1,
      "explanation": "Programmatic access uses API keys, service accounts, or OAuth tokens with least privilege permissions, enabling automated scripts and applications. Objective 4.3"
    },
    {
      "q": "Command Line Interface (CLI) access to cloud resources should use:",
      "options": [
        "Root passwords",
        "Individual user credentials with MFA",
        "Shared accounts",
        "No authentication"
      ],
      "answer": 1,
      "explanation": "CLI access requires individual user authentication, preferably with MFA, access keys with expiration, and logging for accountability and security. Objective 4.3"
    },
    {
      "q": "Web portal access should be protected with:",
      "options": [
        "Only username",
        "Username, strong password, and MFA",
        "No password",
        "Shared credentials"
      ],
      "answer": 1,
      "explanation": "Web portals require strong authentication combining passwords, multi-factor authentication, session timeouts, and IP restrictions where appropriate. Objective 4.3"
    },
    {
      "q": "Secure Shell (SSH) access to cloud VMs requires:",
      "options": [
        "No security",
        "SSH key pairs or certificates, not passwords",
        "Default passwords",
        "Anonymous access"
      ],
      "answer": 1,
      "explanation": "SSH should use key-based authentication (public/private key pairs) or certificates, disabling password authentication and implementing jump/bastion hosts. Objective 4.3"
    },
    {
      "q": "RDP (Remote Desktop Protocol) security should include:",
      "options": [
        "Open to internet",
        "Network restriction, strong auth, and MFA",
        "No password",
        "Default port 3389 exposed"
      ],
      "answer": 1,
      "explanation": "RDP requires network-level authentication, restricted network access (VPN/bastion), strong passwords, MFA, and non-default ports to prevent brute force. Objective 4.3"
    },
    {
      "q": "What is a bastion host?",
      "options": [
        "Unsecured server",
        "Hardened jump server for accessing private resources",
        "Backup server",
        "Web server"
      ],
      "answer": 1,
      "explanation": "Bastion hosts are hardened servers in public subnets that provide controlled access to private resources, centralizing and securing remote access. Objective 4.3"
    },
    {
      "q": "SAML is used for:",
      "options": [
        "Data encryption",
        "Federated single sign-on authentication",
        "Network routing",
        "Storage allocation"
      ],
      "answer": 1,
      "explanation": "SAML (Security Assertion Markup Language) enables federated SSO, allowing users to authenticate once with identity provider and access multiple services. Objective 4.3"
    },
    {
      "q": "Token-based authentication provides:",
      "options": [
        "Permanent passwords",
        "Time-limited access tokens after initial authentication",
        "No security",
        "Shared credentials"
      ],
      "answer": 1,
      "explanation": "Token-based authentication (JWT, OAuth) issues temporary tokens after authentication, enabling stateless sessions and API access without exposing credentials. Objective 4.3"
    },
    {
      "q": "Directory-based authentication integrates with:",
      "options": [
        "No directories",
        "Active Directory, LDAP, or cloud directories",
        "Only local accounts",
        "No integration"
      ],
      "answer": 1,
      "explanation": "Directory integration (AD, Azure AD, LDAP) centralizes user management, enabling consistent authentication, authorization, and access control across resources. Objective 4.3"
    },
    {
      "q": "Multi-Factor Authentication (MFA) requires:",
      "options": [
        "Only password",
        "Two or more authentication factors",
        "Only biometrics",
        "No authentication"
      ],
      "answer": 1,
      "explanation": "MFA requires multiple authentication factors: knowledge (password), possession (phone/token), or inherence (biometrics), significantly improving security. Objective 4.3"
    },
    {
      "q": "OpenID Connect is primarily used for:",
      "options": [
        "Network routing",
        "Authentication and identity layer on OAuth 2.0",
        "Data encryption",
        "Storage management"
      ],
      "answer": 1,
      "explanation": "OpenID Connect adds identity layer to OAuth 2.0, enabling authentication and obtaining user profile information in addition to authorization. Objective 4.3"
    },
    {
      "q": "Role-Based Access Control (RBAC) assigns permissions based on:",
      "options": [
        "Individual users",
        "User roles and job functions",
        "Random assignment",
        "No structure"
      ],
      "answer": 1,
      "explanation": "RBAC assigns permissions to roles (admin, developer, auditor) rather than individuals, simplifying management and ensuring consistent access based on responsibilities. Objective 4.3"
    },
    {
      "q": "Group-based access control:",
      "options": [
        "No groups",
        "Assigns permissions to groups, users inherit group permissions",
        "Only individual permissions",
        "No inheritance"
      ],
      "answer": 1,
      "explanation": "Group-based access assigns permissions to groups; users inherit group permissions automatically, simplifying management of users with similar access needs. Objective 4.3"
    },
    {
      "q": "OAuth 2.0 is primarily used for:",
      "options": [
        "Encryption",
        "Authorization and delegated access",
        "Authentication only",
        "Data storage"
      ],
      "answer": 1,
      "explanation": "OAuth 2.0 provides authorization framework enabling applications to obtain limited access to user resources without exposing credentials (delegated access). Objective 4.3"
    },
    {
      "q": "Discretionary Access Control (DAC) means:",
      "options": [
        "No control",
        "Resource owners control access to their resources",
        "Mandatory rules",
        "No flexibility"
      ],
      "answer": 1,
      "explanation": "DAC allows resource owners to grant/revoke access at their discretion, providing flexibility but potentially less consistent than mandatory access control. Objective 4.3"
    },
    {
      "q": "What does an audit trail in IAM provide?",
      "options": [
        "No information",
        "Comprehensive logging of authentication, authorization, and access events",
        "Only errors",
        "Cost data"
      ],
      "answer": 1,
      "explanation": "IAM audit trails log authentication attempts, authorization decisions, access to resources, and administrative changes, enabling security investigation and compliance. Objective 4.3"
    },
    {
      "q": "Local users in cloud environments:",
      "options": [
        "Are always best practice",
        "Should be minimized; prefer directory integration",
        "Required for all access",
        "No limitations"
      ],
      "answer": 1,
      "explanation": "Local users create management overhead and inconsistency. Prefer centralized directory authentication for consistent management and access control. Objective 4.3"
    },
    {
      "q": "Federation in IAM enables:",
      "options": [
        "No benefits",
        "Single sign-on across multiple organizations/domains",
        "Only local access",
        "No integration"
      ],
      "answer": 1,
      "explanation": "Federation establishes trust relationships between identity providers, enabling users to authenticate once and access resources across multiple organizations. Objective 4.3"
    },
    {
      "q": "Service accounts should:",
      "options": [
        "Use personal credentials",
        "Have specific permissions limited to service needs",
        "Have full admin rights",
        "Share credentials"
      ],
      "answer": 1,
      "explanation": "Service accounts require minimum permissions for specific service functions, credential rotation, logging, and should never use personal user credentials. Objective 4.3"
    },
    {
      "q": "Just-in-Time (JIT) access provides:",
      "options": [
        "Permanent access",
        "Temporary elevated permissions for specific timeframe",
        "No access",
        "Unlimited access"
      ],
      "answer": 1,
      "explanation": "JIT access grants elevated permissions temporarily when needed, then automatically revokes them, reducing standing privileges and attack surface. Objective 4.3"
    },
    {
      "q": "What is the purpose of access reviews?",
      "options": [
        "No purpose",
        "Regularly verify users have appropriate access and remove excess permissions",
        "Grant more access",
        "Never review"
      ],
      "answer": 1,
      "explanation": "Periodic access reviews ensure users retain only necessary permissions, identifying and removing orphaned accounts and excessive privileges (access creep). Objective 4.3"
    },
    {
      "q": "Privileged Access Management (PAM) controls:",
      "options": [
        "Regular user access",
        "Elevated administrative and privileged account access",
        "Public access",
        "Guest access"
      ],
      "answer": 1,
      "explanation": "PAM specifically manages highly privileged accounts (admin, root), requiring additional authentication, session recording, and time-limited elevation. Objective 4.3"
    },
    {
      "q": "API authentication should use:",
      "options": [
        "No authentication",
        "API keys, OAuth tokens, or certificates with rate limiting",
        "Hardcoded passwords",
        "Public access"
      ],
      "answer": 1,
      "explanation": "API security requires authentication (keys, OAuth, mutual TLS), authorization, rate limiting, input validation, and logging to prevent abuse. Objective 4.3"
    },
    {
      "q": "Conditional access policies enforce:",
      "options": [
        "Unconditional access",
        "Access requirements based on context (location, device, risk)",
        "No policies",
        "Always deny"
      ],
      "answer": 1,
      "explanation": "Conditional access evaluates context (user location, device compliance, risk score) and enforces appropriate controls (MFA, block, restricted access). Objective 4.3"
    },
    {
      "q": "What is credential rotation?",
      "options": [
        "Never changing credentials",
        "Periodically changing passwords, keys, and secrets",
        "Using same password forever",
        "Sharing credentials"
      ],
      "answer": 1,
      "explanation": "Credential rotation regularly changes passwords, API keys, and secrets, limiting exposure window if credentials are compromised and enforcing security hygiene. Objective 4.3"
    },
    {
      "q": "Zero Trust security model assumes:",
      "options": [
        "Trust everyone inside network",
        "Never trust, always verify all access",
        "Only external threats exist",
        "No verification needed"
      ],
      "answer": 1,
      "explanation": "Zero Trust assumes breach and verifies every access request regardless of location, requiring continuous authentication, authorization, and least privilege. Objective 4.4"
    },
    {
      "q": "CIS Benchmarks provide:",
      "options": [
        "Random configurations",
        "Security configuration best practices for systems and applications",
        "Only documentation",
        "No guidance"
      ],
      "answer": 1,
      "explanation": "CIS (Center for Internet Security) Benchmarks offer consensus-based security configuration guidelines for operating systems, applications, and cloud services. Objective 4.4"
    },
    {
      "q": "Hardening a system involves:",
      "options": [
        "Installing all software",
        "Removing unnecessary services, closing ports, and applying security configurations",
        "Opening all access",
        "No changes"
      ],
      "answer": 1,
      "explanation": "Hardening reduces attack surface by removing unnecessary services, disabling default accounts, closing unused ports, and applying security baselines. Objective 4.4"
    },
    {
      "q": "Patching should be performed:",
      "options": [
        "Never",
        "Regularly based on criticality and testing",
        "Only when breached",
        "Random schedule"
      ],
      "answer": 1,
      "explanation": "Regular patching based on criticality (critical: days, high: weeks) after appropriate testing prevents exploitation of known vulnerabilities. Objective 4.4"
    },
    {
      "q": "Data encryption at rest protects:",
      "options": [
        "Data in motion",
        "Stored data from unauthorized access if physical media is compromised",
        "Network traffic",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Encryption at rest protects stored data (disks, databases, backups) from unauthorized access if physical storage is stolen, lost, or improperly disposed. Objective 4.4"
    },
    {
      "q": "Data encryption in transit protects:",
      "options": [
        "Stored data",
        "Data being transmitted across networks from interception",
        "Backups",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Encryption in transit (TLS/SSL, VPN, IPSec) protects data moving between systems from interception, eavesdropping, and tampering. Objective 4.4"
    },
    {
      "q": "Secrets management involves:",
      "options": [
        "Hardcoding passwords",
        "Secure storage, rotation, and access control for credentials",
        "Sharing passwords",
        "No management"
      ],
      "answer": 1,
      "explanation": "Secrets management uses vaults (AWS Secrets Manager, Azure Key Vault) to securely store, rotate, audit, and control access to credentials and encryption keys. Objective 4.4"
    },
    {
      "q": "API security requires:",
      "options": [
        "Public access to all",
        "Authentication, authorization, rate limiting, and input validation",
        "No security",
        "Only encryption"
      ],
      "answer": 1,
      "explanation": "API security includes authentication (keys, OAuth), authorization, rate limiting, input validation, encryption, logging, and protection against injection attacks. Objective 4.4"
    },
    {
      "q": "Principle of Least Privilege means:",
      "options": [
        "Grant maximum permissions",
        "Grant minimum permissions necessary for function",
        "No permissions",
        "Everyone is admin"
      ],
      "answer": 1,
      "explanation": "Least privilege grants users/services only minimum permissions required for their job functions, reducing potential impact of compromised accounts. Objective 4.4"
    },
    {
      "q": "Privileged containers run with:",
      "options": [
        "Minimum permissions",
        "Elevated host access and capabilities",
        "No access",
        "Standard permissions only"
      ],
      "answer": 1,
      "explanation": "Privileged containers have elevated host access (root capabilities, host devices) and should be avoided unless absolutely necessary due to security risks. Objective 4.4"
    },
    {
      "q": "Unprivileged containers:",
      "options": [
        "Have full host access",
        "Run with restricted permissions and limited capabilities",
        "Cannot function",
        "Always privileged"
      ],
      "answer": 1,
      "explanation": "Unprivileged containers run with restricted capabilities, no root access, read-only filesystems where possible, improving security through isolation. Objective 4.4"
    },
    {
      "q": "Container file access permissions should:",
      "options": [
        "Be world-writable",
        "Follow least privilege with minimal write access",
        "Grant full access",
        "No permissions"
      ],
      "answer": 1,
      "explanation": "Container files should have minimal permissions (no world-write), use non-root users, and implement read-only root filesystems where feasible. Objective 4.4"
    },
    {
      "q": "Object storage security includes:",
      "options": [
        "Public access to all",
        "Access controls, encryption, versioning, and logging",
        "No security",
        "Only passwords"
      ],
      "answer": 1,
      "explanation": "Object storage security requires IAM policies, bucket policies, encryption (at rest/transit), versioning, MFA delete, and access logging. Objective 4.4"
    },
    {
      "q": "File storage security should implement:",
      "options": [
        "Public shares",
        "Access controls, encryption, and audit logging",
        "No security",
        "Anonymous access"
      ],
      "answer": 1,
      "explanation": "File storage requires network-level access controls, encryption at rest/transit, appropriate share/NTFS permissions, and comprehensive audit logging. Objective 4.4"
    },
    {
      "q": "Vendor-specific security benchmarks:",
      "options": [
        "Should be ignored",
        "Provide platform-specific hardening guidance",
        "Are unnecessary",
        "Only for documentation"
      ],
      "answer": 1,
      "explanation": "Cloud provider security benchmarks (AWS, Azure, GCP) offer platform-specific configuration recommendations addressing unique services and security controls. Objective 4.4"
    },
    {
      "q": "Regular security patching should prioritize:",
      "options": [
        "Random patches",
        "Critical and high-severity patches, especially for public-facing systems",
        "Only annual updates",
        "No priority"
      ],
      "answer": 1,
      "explanation": "Patching prioritizes critical vulnerabilities, actively exploited issues, and public-facing systems, balanced with testing requirements and business impact. Objective 4.4"
    },
    {
      "q": "Defense in depth means:",
      "options": [
        "Single security control",
        "Multiple layered security controls",
        "No security",
        "Only perimeter security"
      ],
      "answer": 1,
      "explanation": "Defense in depth implements multiple security layers (network, host, application, data) so compromise of one layer doesn't compromise entire system. Objective 4.4"
    },
    {
      "q": "Security baselines define:",
      "options": [
        "Random configurations",
        "Minimum security configuration standards",
        "Maximum settings",
        "No standards"
      ],
      "answer": 1,
      "explanation": "Security baselines establish minimum required security configurations for systems, ensuring consistent security posture across infrastructure. Objective 4.4"
    },
    {
      "q": "Immutable infrastructure means:",
      "options": [
        "Constantly changing servers",
        "Servers are never updated, only replaced",
        "Manual updates",
        "No infrastructure"
      ],
      "answer": 1,
      "explanation": "Immutable infrastructure replaces servers rather than updating them, ensuring consistency, eliminating configuration drift, and improving security. Objective 4.4"
    },
    {
      "q": "Security automation provides:",
      "options": [
        "Manual processes",
        "Consistent, rapid security controls and response",
        "Slower security",
        "No benefits"
      ],
      "answer": 1,
      "explanation": "Automation ensures consistent security implementation, rapid incident response, automated compliance checks, and scaling security operations efficiently. Objective 4.4"
    },
    {
      "q": "Endpoint protection provides:",
      "options": [
        "No protection",
        "Antivirus, anti-malware, and threat detection on devices",
        "Only backups",
        "Only updates"
      ],
      "answer": 1,
      "explanation": "Endpoint protection combines antivirus, anti-malware, behavioral analysis, and threat detection protecting devices from malware, exploits, and unauthorized access. Objective 4.5"
    },
    {
      "q": "Data Loss Prevention (DLP) prevents:",
      "options": [
        "All data flow",
        "Unauthorized data exfiltration and leakage",
        "Only viruses",
        "Internet access"
      ],
      "answer": 1,
      "explanation": "DLP identifies, monitors, and blocks unauthorized transmission of sensitive data through email, web, removable media, and cloud applications. Objective 4.5"
    },
    {
      "q": "IPS (Intrusion Prevention System) differs from IDS by:",
      "options": [
        "They are identical",
        "IPS actively blocks threats; IDS only detects and alerts",
        "IPS is passive",
        "IDS blocks traffic"
      ],
      "answer": 1,
      "explanation": "IDS detects and alerts on suspicious activity; IPS actively blocks/prevents malicious traffic inline, providing automated threat response. Objective 4.5"
    },
    {
      "q": "DDoS protection mitigates:",
      "options": [
        "Data encryption",
        "Distributed denial of service attacks overwhelming resources",
        "Malware",
        "Insider threats"
      ],
      "answer": 1,
      "explanation": "DDoS protection detects and mitigates volumetric attacks, protocol attacks, and application-layer attacks attempting to overwhelm infrastructure. Objective 4.5"
    },
    {
      "q": "IAM policies define:",
      "options": [
        "Network routes",
        "Who can access what resources and under what conditions",
        "Storage capacity",
        "Compute sizes"
      ],
      "answer": 1,
      "explanation": "IAM policies specify permissions (actions, resources, conditions) controlling access to cloud resources based on users, roles, and service accounts. Objective 4.5"
    },
    {
      "q": "Network ACLs (Access Control Lists) operate at:",
      "options": [
        "Instance level",
        "Subnet level as stateless firewall",
        "Application level",
        "User level"
      ],
      "answer": 1,
      "explanation": "Network ACLs are stateless firewalls at subnet boundaries, controlling inbound/outbound traffic with ordered allow/deny rules evaluated sequentially. Objective 4.5"
    },
    {
      "q": "Web Application Firewall (WAF) protects against:",
      "options": [
        "Network attacks only",
        "Application-layer attacks (SQL injection, XSS, CSRF)",
        "Only DDoS",
        "Physical attacks"
      ],
      "answer": 1,
      "explanation": "WAF filters HTTP/HTTPS traffic protecting web applications from SQL injection, cross-site scripting, CSRF, and other OWASP Top 10 vulnerabilities. Objective 4.5"
    },
    {
      "q": "Network Security Groups control:",
      "options": [
        "Subnet traffic",
        "Instance-level firewall rules",
        "Application code",
        "User permissions"
      ],
      "answer": 1,
      "explanation": "Security groups are stateful firewalls controlling inbound/outbound traffic at instance level, acting as virtual firewalls for VMs and resources. Objective 4.5"
    },
    {
      "q": "SIEM (Security Information and Event Management) provides:",
      "options": [
        "Only backups",
        "Centralized logging, correlation, and security analytics",
        "Only monitoring",
        "No security"
      ],
      "answer": 1,
      "explanation": "SIEM aggregates logs from multiple sources, correlates events, detects threats, enables investigation, and provides compliance reporting. Objective 4.5"
    },
    {
      "q": "Cloud-native firewalls provide:",
      "options": [
        "No protection",
        "Integrated network security with cloud platform features",
        "Only on-premises protection",
        "Physical security only"
      ],
      "answer": 1,
      "explanation": "Cloud-native firewalls integrate with cloud platforms providing automated scaling, API integration, managed rules, and cloud-specific threat intelligence. Objective 4.5"
    },
    {
      "q": "Event monitoring should track:",
      "options": [
        "Nothing",
        "Authentication, access, configuration changes, and security events",
        "Only errors",
        "Only cost"
      ],
      "answer": 1,
      "explanation": "Comprehensive event monitoring logs authentication attempts, resource access, configuration changes, security events, and admin actions for threat detection. Objective 4.6"
    },
    {
      "q": "Deviation from baseline behavior indicates:",
      "options": [
        "Normal operations",
        "Potential security incident or anomaly",
        "Better performance",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Significant deviations from established baselines (traffic patterns, access times, data transfers) may indicate compromised accounts or malicious activity. Objective 4.6"
    },
    {
      "q": "Unnecessary open ports should be:",
      "options": [
        "Left open",
        "Closed or filtered to reduce attack surface",
        "All opened",
        "Ignored"
      ],
      "answer": 1,
      "explanation": "Unused ports increase attack surface and should be closed, with necessary ports restricted to specific sources using firewalls and security groups. Objective 4.6"
    },
    {
      "q": "Vulnerability exploitation often results from:",
      "options": [
        "Good security",
        "Human error or outdated software",
        "Proper patching",
        "Strong authentication"
      ],
      "answer": 1,
      "explanation": "Exploitation typically stems from human error (misconfigurations, weak passwords), unpatched systems, or outdated software with known vulnerabilities. Objective 4.6"
    },
    {
      "q": "Social engineering attacks like phishing target:",
      "options": [
        "Network infrastructure",
        "Human psychology and trust",
        "Only technical systems",
        "Firewalls"
      ],
      "answer": 1,
      "explanation": "Social engineering exploits human psychology (urgency, authority, trust) to trick users into revealing credentials, clicking links, or executing malware. Objective 4.6"
    },
    {
      "q": "Ransomware is malware that:",
      "options": [
        "Monitors only",
        "Encrypts data and demands payment for decryption",
        "Improves performance",
        "Provides backups"
      ],
      "answer": 1,
      "explanation": "Ransomware encrypts victim data making it inaccessible and demands payment (often cryptocurrency) for decryption keys, requiring robust backups for recovery. Objective 4.6"
    },
    {
      "q": "DDoS attacks aim to:",
      "options": [
        "Improve performance",
        "Overwhelm resources making services unavailable",
        "Enhance security",
        "Backup data"
      ],
      "answer": 1,
      "explanation": "Distributed Denial of Service attacks flood targets with traffic from multiple sources, exhausting resources and preventing legitimate user access. Objective 4.6"
    },
    {
      "q": "Cryptojacking involves:",
      "options": [
        "Data encryption",
        "Unauthorized use of resources for cryptocurrency mining",
        "Improving performance",
        "Data backup"
      ],
      "answer": 1,
      "explanation": "Cryptojacking hijacks compute resources to mine cryptocurrency without authorization, consuming CPU, increasing costs, and degrading performance. Objective 4.6"
    },
    {
      "q": "Zombie instances are:",
      "options": [
        "Normal resources",
        "Unused or forgotten resources that remain active and incur costs",
        "High-performance systems",
        "Required systems"
      ],
      "answer": 1,
      "explanation": "Zombie instances are abandoned or forgotten resources left running, incurring unnecessary costs and potential security risks through lack of maintenance. Objective 4.6"
    },
    {
      "q": "Instance metadata exploitation can reveal:",
      "options": [
        "Nothing",
        "Credentials, configuration details, and sensitive information",
        "Only public data",
        "No useful information"
      ],
      "answer": 1,
      "explanation": "Instance metadata services expose configuration, IAM credentials, and sensitive data accessible from within instances, requiring proper network restrictions. Objective 4.6"
    }
  ],
  "4. Operations": [
    {
      "q": "What is the primary purpose of log collection in cloud environments?",
      "options": [
        "Increase costs",
        "Centralize data for analysis, troubleshooting, and security monitoring",
        "Slow down systems",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Log collection aggregates logs from distributed resources into centralized storage, enabling comprehensive analysis, troubleshooting, security investigation, and compliance. Objective 3.1"
    },
    {
      "q": "Log aggregation combines logs from:",
      "options": [
        "Single source only",
        "Multiple sources into unified view",
        "No sources",
        "Only errors"
      ],
      "answer": 1,
      "explanation": "Log aggregation collects logs from applications, infrastructure, network devices, and security tools into single platform, enabling correlation and comprehensive monitoring. Objective 3.1"
    },
    {
      "q": "What should log retention policies consider?",
      "options": [
        "Only cost",
        "Compliance requirements, storage costs, and analysis needs",
        "Keep forever",
        "Delete immediately"
      ],
      "answer": 1,
      "explanation": "Retention policies balance compliance mandates (30-90 days minimum), security investigation needs, storage costs, and data lifecycle requirements. Objective 3.1"
    },
    {
      "q": "Application tracing helps identify:",
      "options": [
        "Only errors",
        "Request flow, latency bottlenecks, and dependencies across services",
        "Storage usage",
        "Costs only"
      ],
      "answer": 1,
      "explanation": "Distributed tracing tracks requests across microservices, identifying performance bottlenecks, latency issues, failed calls, and service dependencies. Objective 3.1"
    },
    {
      "q": "Which metric type measures system performance?",
      "options": [
        "Logs only",
        "CPU, memory, disk I/O, network throughput",
        "Only errors",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Performance metrics track CPU utilization, memory consumption, disk IOPS/throughput, network bandwidth, and latency for capacity planning and troubleshooting. Objective 3.1"
    },
    {
      "q": "Custom application metrics should track:",
      "options": [
        "Nothing",
        "Business KPIs, user actions, and application-specific performance",
        "Only infrastructure",
        "Random data"
      ],
      "answer": 1,
      "explanation": "Custom metrics measure business outcomes (transactions, conversions), user experience, application errors, and domain-specific performance indicators. Objective 3.1"
    },
    {
      "q": "Alerting should be configured to:",
      "options": [
        "Alert on everything",
        "Notify appropriate teams when thresholds exceed acceptable limits",
        "Never alert",
        "Only email"
      ],
      "answer": 1,
      "explanation": "Effective alerting notifies relevant teams when metrics exceed thresholds, with appropriate severity, context, and actionable information to reduce noise. Objective 3.1"
    },
    {
      "q": "Alert triage involves:",
      "options": [
        "Ignoring alerts",
        "Prioritizing and assessing severity, impact, and required response",
        "Deleting all alerts",
        "No action"
      ],
      "answer": 1,
      "explanation": "Triage evaluates alert severity, business impact, affected users, and urgency to prioritize response efforts and allocate appropriate resources. Objective 3.1"
    },
    {
      "q": "Alert response procedures should include:",
      "options": [
        "No procedures",
        "Escalation paths, runbooks, and clear responsibilities",
        "Random actions",
        "Ignore problems"
      ],
      "answer": 1,
      "explanation": "Response procedures define responsibilities, escalation paths, investigation steps, communication plans, and remediation actions for different alert types. Objective 3.1"
    },
    {
      "q": "What is the purpose of structured logging?",
      "options": [
        "Random text",
        "Consistent format enabling automated parsing and analysis",
        "Human-only readability",
        "No structure"
      ],
      "answer": 1,
      "explanation": "Structured logs (JSON, key-value pairs) enable automated parsing, filtering, aggregation, and correlation, improving analysis efficiency and accuracy. Objective 3.1"
    },
    {
      "q": "Log levels typically include:",
      "options": [
        "Only errors",
        "DEBUG, INFO, WARN, ERROR, FATAL",
        "Random levels",
        "No levels"
      ],
      "answer": 1,
      "explanation": "Log levels categorize message severity: DEBUG (detailed), INFO (general), WARN (potential issues), ERROR (failures), FATAL (critical), enabling filtering and prioritization. Objective 3.1"
    },
    {
      "q": "Centralized logging platforms provide:",
      "options": [
        "Local storage only",
        "Search, analysis, visualization, and alerting across all logs",
        "No benefits",
        "Only storage"
      ],
      "answer": 1,
      "explanation": "Platforms like ELK, Splunk, CloudWatch Logs enable searching, filtering, correlation, visualization, alerting, and long-term retention of distributed logs. Objective 3.1"
    },
    {
      "q": "What should be logged for security purposes?",
      "options": [
        "Nothing",
        "Authentication attempts, access to sensitive data, and administrative actions",
        "Only successes",
        "Random events"
      ],
      "answer": 1,
      "explanation": "Security logs capture authentication (success/failure), authorization decisions, data access, config changes, and admin actions for investigation and compliance. Objective 3.1"
    },
    {
      "q": "Application Performance Monitoring (APM) tracks:",
      "options": [
        "Only errors",
        "Response times, throughput, error rates, and resource usage",
        "Only logs",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "APM monitors application performance metrics including response times, transaction volumes, error rates, database queries, and external service calls. Objective 3.1"
    },
    {
      "q": "Synthetic monitoring involves:",
      "options": [
        "Real user monitoring only",
        "Simulated transactions testing availability and performance",
        "No monitoring",
        "Manual testing only"
      ],
      "answer": 1,
      "explanation": "Synthetic monitoring executes scripted transactions from multiple locations, proactively testing availability, functionality, and performance before users are affected. Objective 3.1"
    },
    {
      "q": "Real User Monitoring (RUM) captures:",
      "options": [
        "Simulated data",
        "Actual end-user experience, page load times, and interactions",
        "Only server metrics",
        "No data"
      ],
      "answer": 1,
      "explanation": "RUM collects actual user experience data including page load times, JavaScript errors, user flows, and geographic performance variations. Objective 3.1"
    },
    {
      "q": "Metric aggregation enables:",
      "options": [
        "No benefits",
        "Summarizing data points into meaningful statistics over time",
        "Raw data only",
        "Data deletion"
      ],
      "answer": 1,
      "explanation": "Aggregation calculates averages, percentiles, min/max, and sums over time intervals, reducing storage while maintaining statistical significance. Objective 3.1"
    },
    {
      "q": "What is the difference between metrics and logs?",
      "options": [
        "They are identical",
        "Metrics are numeric time-series; logs are event records with context",
        "No difference",
        "Only format"
      ],
      "answer": 1,
      "explanation": "Metrics are numeric measurements over time (CPU 45%); logs are timestamped event records with detailed context (error messages, stack traces). Objective 3.1"
    },
    {
      "q": "Distributed tracing requires:",
      "options": [
        "Nothing",
        "Correlation IDs propagated across service calls",
        "Single service only",
        "No requirements"
      ],
      "answer": 1,
      "explanation": "Tracing uses correlation/trace IDs passed between services, tracking request flow through distributed systems and identifying latency sources. Objective 3.1"
    },
    {
      "q": "Alert fatigue occurs when:",
      "options": [
        "No alerts exist",
        "Excessive or poorly configured alerts cause teams to ignore them",
        "Alerts work perfectly",
        "Only one alert"
      ],
      "answer": 1,
      "explanation": "Alert fatigue results from too many alerts, false positives, or unclear severity, causing teams to ignore/dismiss important notifications. Objective 3.1"
    },
    {
      "q": "Reducing alert fatigue requires:",
      "options": [
        "More alerts",
        "Proper thresholds, aggregation, and actionable alerts only",
        "Disabling monitoring",
        "Random changes"
      ],
      "answer": 1,
      "explanation": "Reduce fatigue by setting appropriate thresholds, aggregating related alerts, eliminating false positives, and ensuring alerts are actionable. Objective 3.1"
    },
    {
      "q": "Log sampling involves:",
      "options": [
        "Logging everything",
        "Collecting subset of logs to reduce volume and cost",
        "No logging",
        "Deleting all logs"
      ],
      "answer": 1,
      "explanation": "Sampling collects representative log subset (e.g., 10% of requests) for high-volume applications, balancing cost and visibility for analysis. Objective 3.1"
    },
    {
      "q": "What are Service Level Indicators (SLIs)?",
      "options": [
        "Random metrics",
        "Quantitative measures of service level (latency, availability, error rate)",
        "Cost metrics only",
        "No indicators"
      ],
      "answer": 1,
      "explanation": "SLIs are specific metrics measuring service quality: request latency percentiles, availability percentage, error rate, throughput. Objective 3.1"
    },
    {
      "q": "Service Level Objectives (SLOs) define:",
      "options": [
        "No targets",
        "Target values for SLIs (e.g., 99.9% availability)",
        "Cost goals only",
        "Random numbers"
      ],
      "answer": 1,
      "explanation": "SLOs set target values for SLIs: 99.9% availability, p95 latency <200ms, error rate <0.1%, providing measurable service quality goals. Objective 3.1"
    },
    {
      "q": "Error budgets are:",
      "options": [
        "Unlimited errors",
        "Acceptable amount of downtime/errors before SLO violation",
        "Zero tolerance",
        "No concept"
      ],
      "answer": 1,
      "explanation": "Error budgets (100% - SLO) define acceptable failure: 99.9% SLO = 0.1% error budget, balancing innovation velocity with reliability. Objective 3.1"
    },
    {
      "q": "Dashboard design should prioritize:",
      "options": [
        "All possible metrics",
        "Most important metrics for specific audience and use case",
        "Random data",
        "Pretty colors only"
      ],
      "answer": 1,
      "explanation": "Effective dashboards display relevant metrics for specific roles (executives, operations, developers) with clear visualizations and actionable insights. Objective 3.1"
    },
    {
      "q": "Anomaly detection identifies:",
      "options": [
        "Normal behavior",
        "Deviations from expected patterns indicating potential issues",
        "All data",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Anomaly detection uses machine learning or statistical methods to identify unusual patterns deviating from baselines, flagging potential problems. Objective 3.1"
    },
    {
      "q": "Log correlation enables:",
      "options": [
        "No benefits",
        "Connecting related events across systems for root cause analysis",
        "Random linking",
        "Data deletion"
      ],
      "answer": 1,
      "explanation": "Correlation links related log events using request IDs, sessions, or timestamps across services, enabling comprehensive incident investigation. Objective 3.1"
    },
    {
      "q": "What information should alerts include?",
      "options": [
        "Just error message",
        "Severity, affected resources, impact, and remediation guidance",
        "No information",
        "Only timestamp"
      ],
      "answer": 1,
      "explanation": "Actionable alerts include severity, impacted services/users, current vs threshold values, runbook links, and context for rapid response. Objective 3.1"
    },
    {
      "q": "Observability differs from monitoring by:",
      "options": [
        "They are identical",
        "Observability infers internal state from external outputs",
        "No difference",
        "Only terminology"
      ],
      "answer": 1,
      "explanation": "Observability enables understanding system internal state through logs, metrics, and traces without predefined dashboards, answering unexpected questions. Objective 3.1"
    },
    {
      "q": "Horizontal scaling involves:",
      "options": [
        "Increasing instance size",
        "Adding more instances to distribute load",
        "Removing instances",
        "No changes"
      ],
      "answer": 1,
      "explanation": "Horizontal scaling (scale out) adds more instances of same size, distributing load across multiple servers, providing better fault tolerance than vertical. Objective 3.2"
    },
    {
      "q": "Vertical scaling involves:",
      "options": [
        "Adding more instances",
        "Increasing size/capacity of existing instances",
        "Removing resources",
        "No changes"
      ],
      "answer": 1,
      "explanation": "Vertical scaling (scale up) increases CPU, memory, or other resources of existing instance, simpler but limited by maximum instance size. Objective 3.2"
    },
    {
      "q": "Which scaling type provides better fault tolerance?",
      "options": [
        "Vertical only",
        "Horizontal scaling across multiple instances",
        "Neither",
        "Both equal"
      ],
      "answer": 1,
      "explanation": "Horizontal scaling improves fault tolerance as failure of one instance affects smaller percentage of capacity, while vertical scaling creates single point of failure. Objective 3.2"
    },
    {
      "q": "Auto-scaling based on trending means:",
      "options": [
        "Random scaling",
        "Scaling based on predicted demand patterns",
        "No scaling",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Trend-based scaling analyzes historical patterns to predict future demand, scaling proactively before load increases to prevent performance degradation. Objective 3.2"
    },
    {
      "q": "Load-based auto-scaling triggers when:",
      "options": [
        "Scheduled time",
        "Metrics like CPU or requests exceed thresholds",
        "Never",
        "Random"
      ],
      "answer": 1,
      "explanation": "Load-based scaling monitors metrics (CPU, memory, requests/second) and adds/removes instances when thresholds are crossed, responding to actual demand. Objective 3.2"
    },
    {
      "q": "Event-driven scaling responds to:",
      "options": [
        "No events",
        "Specific events or conditions triggering scale actions",
        "Only time",
        "Manual requests only"
      ],
      "answer": 1,
      "explanation": "Event-driven scaling reacts to specific events (queue depth, message rate, custom metrics), scaling in response to actual workload conditions. Objective 3.2"
    },
    {
      "q": "Scheduled scaling is BEST for:",
      "options": [
        "Unpredictable load",
        "Predictable, time-based demand patterns",
        "Steady load",
        "Random patterns"
      ],
      "answer": 1,
      "explanation": "Scheduled scaling pre-scales resources for known patterns (business hours, end-of-month processing), avoiding reactive delays during predictable demand. Objective 3.2"
    },
    {
      "q": "Manual scaling involves:",
      "options": [
        "Automatic adjustments",
        "Operator-initiated resource adjustments",
        "No scaling",
        "System decides only"
      ],
      "answer": 1,
      "explanation": "Manual scaling requires human intervention to add/remove resources, suitable for planned events or when automatic scaling isn't appropriate. Objective 3.2"
    },
    {
      "q": "Scale-in (reducing instances) should consider:",
      "options": [
        "Immediate termination",
        "Connection draining and graceful shutdown",
        "No consideration",
        "Force stop"
      ],
      "answer": 1,
      "explanation": "Safe scale-in drains existing connections, completes in-flight requests, and ensures no data loss before terminating instances. Objective 3.2"
    },
    {
      "q": "Scaling cooldown periods prevent:",
      "options": [
        "All scaling",
        "Rapid, unstable scaling oscillations",
        "Manual scaling",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Cooldown periods delay subsequent scaling actions after scale event, preventing instability from rapid oscillations while new resources stabilize. Objective 3.2"
    },
    {
      "q": "Minimum instance count ensures:",
      "options": [
        "No instances",
        "Baseline capacity always available",
        "Maximum cost",
        "No scaling"
      ],
      "answer": 1,
      "explanation": "Minimum instance count maintains baseline capacity for availability and performance, preventing scale-to-zero scenarios affecting service. Objective 3.2"
    },
    {
      "q": "Maximum instance count prevents:",
      "options": [
        "Scaling up",
        "Uncontrolled costs and resource exhaustion",
        "All instances",
        "Availability"
      ],
      "answer": 1,
      "explanation": "Maximum limits protect against runaway scaling costs, misconfigurations, or DDoS attacks causing excessive resource consumption. Objective 3.2"
    },
    {
      "q": "Desired capacity in auto-scaling represents:",
      "options": [
        "Random number",
        "Target number of instances system aims to maintain",
        "Minimum only",
        "Maximum only"
      ],
      "answer": 1,
      "explanation": "Desired capacity is target instance count auto-scaling maintains, adjusted dynamically within min/max bounds based on scaling policies. Objective 3.2"
    },
    {
      "q": "Target tracking scaling maintains:",
      "options": [
        "Random values",
        "Specific metric value (e.g., 70% CPU utilization)",
        "No target",
        "Maximum always"
      ],
      "answer": 1,
      "explanation": "Target tracking automatically adjusts capacity to maintain specified metric target (CPU 70%, request count), simplifying configuration. Objective 3.2"
    },
    {
      "q": "Step scaling adjusts capacity by:",
      "options": [
        "Fixed amount always",
        "Different amounts based on alarm breach magnitude",
        "No adjustment",
        "Random amounts"
      ],
      "answer": 1,
      "explanation": "Step scaling uses multiple thresholds with different scaling adjustments (e.g., +1 instance at 70% CPU, +3 at 90%), responding proportionally. Objective 3.2"
    },
    {
      "q": "Predictive scaling uses:",
      "options": [
        "No data",
        "Machine learning to forecast demand and pre-scale",
        "Random guesses",
        "Manual input only"
      ],
      "answer": 1,
      "explanation": "Predictive scaling analyzes historical patterns with ML to forecast demand, scaling proactively before load increases improve performance. Objective 3.2"
    },
    {
      "q": "What metric is commonly used for web application auto-scaling?",
      "options": [
        "Disk space only",
        "CPU utilization, requests per second, or response time",
        "Color schemes",
        "No metrics"
      ],
      "answer": 1,
      "explanation": "Web apps typically scale on CPU, requests/second, active connections, or response latency, selecting metrics that reflect actual user impact. Objective 3.2"
    },
    {
      "q": "Queue-based scaling triggers when:",
      "options": [
        "No messages",
        "Queue depth or message age exceeds thresholds",
        "Random times",
        "Never"
      ],
      "answer": 1,
      "explanation": "Queue-based scaling monitors message queue depth or age, adding workers when backlog grows to maintain processing throughput. Objective 3.2"
    },
    {
      "q": "Health checks in auto-scaling determine:",
      "options": [
        "Nothing",
        "Whether instances are healthy and should receive traffic",
        "Only startup",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Health checks verify instance health; unhealthy instances are replaced automatically, maintaining fleet health and availability. Objective 3.2"
    },
    {
      "q": "What happens when an instance fails health checks?",
      "options": [
        "Nothing",
        "Removed from load balancer and replaced with new instance",
        "Continues serving",
        "Manual intervention required"
      ],
      "answer": 1,
      "explanation": "Failed instances are automatically removed from load balancer rotation and terminated, with auto-scaling launching replacements to maintain desired capacity. Objective 3.2"
    },
    {
      "q": "Scaling policies should be tested during:",
      "options": [
        "Never",
        "Load testing and chaos engineering exercises",
        "Production only",
        "No testing needed"
      ],
      "answer": 1,
      "explanation": "Testing scaling under load validates thresholds, response times, and stability before production traffic, identifying tuning needs. Objective 3.2"
    },
    {
      "q": "Blue-green deployment facilitates scaling by:",
      "options": [
        "No impact",
        "Allowing instant traffic shifts to pre-scaled environment",
        "Slowing scaling",
        "Preventing scaling"
      ],
      "answer": 1,
      "explanation": "Blue-green enables pre-scaling green environment to required capacity before cutover, avoiding performance issues during traffic migration. Objective 3.2"
    },
    {
      "q": "Stateful applications present scaling challenges because:",
      "options": [
        "No challenges",
        "Session data must be preserved or shared across instances",
        "They scale easily",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Stateful apps require session persistence, shared storage, or session replication, complicating horizontal scaling compared to stateless apps. Objective 3.2"
    },
    {
      "q": "Stateless application design enables:",
      "options": [
        "No scaling",
        "Easy horizontal scaling with any instance handling requests",
        "Only vertical scaling",
        "Complexity"
      ],
      "answer": 1,
      "explanation": "Stateless apps store session data externally, allowing any instance to handle requests, simplifying horizontal scaling and load balancing. Objective 3.2"
    },
    {
      "q": "Database scaling typically uses:",
      "options": [
        "Only horizontal",
        "Read replicas for reads, vertical scaling for writes",
        "No scaling",
        "Random approach"
      ],
      "answer": 1,
      "explanation": "Databases commonly scale reads horizontally with replicas while scaling writes vertically, though some use sharding for horizontal write scaling. Objective 3.2"
    },
    {
      "q": "Full backups capture:",
      "options": [
        "Only changed data",
        "Complete copy of all data",
        "No data",
        "Metadata only"
      ],
      "answer": 1,
      "explanation": "Full backups copy all data regardless of previous backups, providing complete standalone backup but consuming most storage and time. Objective 3.3"
    },
    {
      "q": "Incremental backups capture:",
      "options": [
        "All data",
        "Only changes since last backup (full or incremental)",
        "No data",
        "Random data"
      ],
      "answer": 1,
      "explanation": "Incremental backups save only data changed since previous backup (any type), minimizing backup time and storage but requiring all backups for restoration. Objective 3.3"
    },
    {
      "q": "Differential backups capture:",
      "options": [
        "Only changed today",
        "All changes since last full backup",
        "All data",
        "No data"
      ],
      "answer": 1,
      "explanation": "Differential backups save all changes since last full backup, balancing backup size/time against restoration complexity (need full + latest differential). Objective 3.3"
    },
    {
      "q": "Which backup type requires the LEAST time to restore?",
      "options": [
        "Incremental",
        "Differential",
        "Full backup",
        "All equal"
      ],
      "answer": 2,
      "explanation": "Full backups restore fastest as they contain all data in single backup, while incremental requires full + all incremental backups. Objective 3.3"
    },
    {
      "q": "Which backup type consumes LEAST storage space?",
      "options": [
        "Full",
        "Differential",
        "Incremental",
        "All equal"
      ],
      "answer": 2,
      "explanation": "Incremental backups use least storage by capturing only changes since previous backup, though requiring more backups for full restoration. Objective 3.3"
    },
    {
      "q": "On-site backup provides:",
      "options": [
        "Geographic redundancy",
        "Fast restoration but no disaster recovery",
        "Slow restoration",
        "No benefits"
      ],
      "answer": 1,
      "explanation": "On-site backups enable fast restoration due to proximity but don't protect against site-wide disasters (fire, flood, ransomware). Objective 3.3"
    },
    {
      "q": "Off-site backup provides:",
      "options": [
        "No benefits",
        "Geographic redundancy for disaster recovery",
        "Fastest restoration",
        "Local access only"
      ],
      "answer": 1,
      "explanation": "Off-site backups protect against site disasters by storing data in different geographic location, essential for comprehensive DR strategy. Objective 3.3"
    },
    {
      "q": "The 3-2-1 backup rule recommends:",
      "options": [
        "One backup only",
        "3 copies, 2 media types, 1 off-site",
        "Random approach",
        "No backups"
      ],
      "answer": 1,
      "explanation": "3-2-1 rule ensures resilience: 3 data copies, on 2 different media types, with 1 copy off-site protecting against various failure scenarios. Objective 3.3"
    },
    {
      "q": "Backup scheduling should consider:",
      "options": [
        "Random times",
        "RPO requirements, system load, and change frequency",
        "Only weekends",
        "No schedule"
      ],
      "answer": 1,
      "explanation": "Schedule frequency must meet RPO (acceptable data loss), balance system impact, and align with change patterns (more frequent for critical data). Objective 3.3"
    },
    {
      "q": "Backup retention policies define:",
      "options": [
        "No limits",
        "How long backups are kept based on requirements",
        "Keep forever",
        "Delete immediately"
      ],
      "answer": 1,
      "explanation": "Retention balances compliance requirements, recovery needs, and storage costs (daily: 30 days, weekly: 12 weeks, monthly: 12 months). Objective 3.3"
    },
    {
      "q": "Backup replication provides:",
      "options": [
        "No benefits",
        "Multiple copies in different locations for redundancy",
        "Single copy only",
        "Slower backups"
      ],
      "answer": 1,
      "explanation": "Replication creates copies across regions/clouds, protecting against regional failures and improving disaster recovery capabilities. Objective 3.3"
    },
    {
      "q": "Why should backups be encrypted?",
      "options": [
        "Not necessary",
        "Protect data confidentiality if backup media is compromised",
        "Slows backups",
        "No benefit"
      ],
      "answer": 1,
      "explanation": "Encryption protects backup data from unauthorized access if storage is compromised, lost, or stolen, meeting compliance and security requirements. Objective 3.3"
    },
    {
      "q": "Backup testing should verify:",
      "options": [
        "Nothing",
        "Recoverability and data integrity through regular restoration tests",
        "Only size",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Regular restoration testing validates backups are complete, uncorrupted, and restorable, ensuring they'll work during actual recovery. Objective 3.3"
    },
    {
      "q": "How often should backup restoration be tested?",
      "options": [
        "Never",
        "Regularly (monthly/quarterly) and after major changes",
        "Once only",
        "During disaster only"
      ],
      "answer": 1,
      "explanation": "Regular testing (quarterly minimum) plus event-driven testing validates backup integrity and team preparedness, identifying issues before emergencies. Objective 3.3"
    },
    {
      "q": "Backup integrity checks verify:",
      "options": [
        "Nothing",
        "Backup files are complete and not corrupted",
        "Only size",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Integrity checks use checksums, hashes, or test restores to verify backups are complete and uncorrupted, ensuring reliable recovery. Objective 3.3"
    },
    {
      "q": "In-place recovery restores data to:",
      "options": [
        "Different location",
        "Original location overwriting existing data",
        "No location",
        "Random location"
      ],
      "answer": 1,
      "explanation": "In-place recovery restores data to original location/system, replacing current state with backup version, typical for disaster recovery. Objective 3.3"
    },
    {
      "q": "Parallel recovery restores data to:",
      "options": [
        "Original location only",
        "Alternate location while production continues",
        "No location",
        "Random location"
      ],
      "answer": 1,
      "explanation": "Parallel recovery restores to alternate environment, enabling validation before cutover and supporting legal discovery without disrupting production. Objective 3.3"
    },
    {
      "q": "Bulk recovery restores:",
      "options": [
        "Single file",
        "Entire system or large data set",
        "No data",
        "Metadata only"
      ],
      "answer": 1,
      "explanation": "Bulk recovery restores complete systems, volumes, or databases, used for disaster recovery or major failures requiring comprehensive restoration. Objective 3.3"
    },
    {
      "q": "Granular recovery enables:",
      "options": [
        "Only full restoration",
        "Restoring individual files, emails, or database records",
        "No recovery",
        "System restoration only"
      ],
      "answer": 1,
      "explanation": "Granular recovery restores specific items (single file, email, database table) without full restoration, enabling targeted recovery faster. Objective 3.3"
    },
    {
      "q": "Backup windows should:",
      "options": [
        "Run during peak hours",
        "Complete within maintenance window with minimal impact",
        "Never complete",
        "Take weeks"
      ],
      "answer": 1,
      "explanation": "Backups should complete within available maintenance window, minimizing production impact while ensuring adequate data protection. Objective 3.3"
    },
    {
      "q": "Immutable backups prevent:",
      "options": [
        "Recovery",
        "Deletion or modification, protecting against ransomware",
        "All access",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Immutable backups cannot be deleted or modified after creation, protecting against ransomware and insider threats for specified retention period. Objective 3.3"
    },
    {
      "q": "Application-consistent backups ensure:",
      "options": [
        "No consistency",
        "All application components backed up in consistent state",
        "Random data",
        "File-level only"
      ],
      "answer": 1,
      "explanation": "Application-consistent backups coordinate with applications to quiesce I/O and capture all components in synchronized state for clean recovery. Objective 3.3"
    },
    {
      "q": "Crash-consistent backups:",
      "options": [
        "Are never useful",
        "Capture point-in-time snapshot similar to crash recovery",
        "Are always best",
        "Have no data"
      ],
      "answer": 1,
      "explanation": "Crash-consistent backups capture snapshot without coordinating with applications, like recovery after crash; applications must replay logs during restoration. Objective 3.3"
    },
    {
      "q": "Backup deduplication reduces:",
      "options": [
        "Backup quality",
        "Storage requirements by eliminating duplicate data",
        "Recovery speed",
        "Retention"
      ],
      "answer": 1,
      "explanation": "Deduplication identifies and eliminates duplicate data blocks across backups, significantly reducing storage consumption especially for incremental backups. Objective 3.3"
    },
    {
      "q": "Cloud-to-cloud backup protects against:",
      "options": [
        "All disasters",
        "Accidental deletion, corruption, and provider issues",
        "Physical disasters only",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Cloud-to-cloud backup protects SaaS data (Office 365, Salesforce) from accidental deletion, ransomware, retention gaps, and provider issues. Objective 3.3"
    },
    {
      "q": "Major version updates typically include:",
      "options": [
        "Only bug fixes",
        "Significant changes, new features, potential breaking changes",
        "No changes",
        "Minor fixes only"
      ],
      "answer": 1,
      "explanation": "Major updates (2.0, 3.0) include significant functionality changes, architectural updates, and potentially breaking changes requiring testing and planning. Objective 3.4"
    },
    {
      "q": "Minor version updates typically include:",
      "options": [
        "Breaking changes",
        "New features, improvements, backward-compatible changes",
        "No changes",
        "Major redesign"
      ],
      "answer": 1,
      "explanation": "Minor updates (2.1, 2.2) add features and improvements while maintaining backward compatibility, lower risk than major updates. Objective 3.4"
    },
    {
      "q": "Patch updates primarily address:",
      "options": [
        "New features",
        "Security vulnerabilities and bug fixes",
        "Major redesign",
        "No changes"
      ],
      "answer": 1,
      "explanation": "Patches (2.1.1, 2.1.2) fix security vulnerabilities, bugs, and stability issues without adding features, requiring prompt deployment. Objective 3.4"
    },
    {
      "q": "Before applying updates in production, you should:",
      "options": [
        "Apply immediately",
        "Test in non-production environment",
        "Skip testing",
        "Hope for best"
      ],
      "answer": 1,
      "explanation": "Updates must be tested in dev/test environments to identify compatibility issues, performance impacts, or regressions before production deployment. Objective 3.4"
    },
    {
      "q": "Ephemeral data is:",
      "options": [
        "Permanent storage",
        "Temporary data deleted when resource terminates",
        "Backed up daily",
        "Long-term retention"
      ],
      "answer": 1,
      "explanation": "Ephemeral data exists only during resource lifetime (container, instance session), deleted upon termination, suitable for caches and temporary processing. Objective 3.4"
    },
    {
      "q": "Persistent data requires:",
      "options": [
        "No storage",
        "Durable storage independent of compute resources",
        "Deletion on shutdown",
        "Temporary storage only"
      ],
      "answer": 1,
      "explanation": "Persistent data survives resource termination using volumes, object storage, or databases that exist independently of compute instances. Objective 3.4"
    },
    {
      "q": "End of Life (EOL) for software means:",
      "options": [
        "Just beginning",
        "Vendor discontinues product, no further updates",
        "New version released",
        "Peak support"
      ],
      "answer": 1,
      "explanation": "EOL means vendor ceases all updates, patches, and support; continued use creates security and compliance risks requiring migration planning. Objective 3.4"
    },
    {
      "q": "End of Support (EOS) means:",
      "options": [
        "Full support continues",
        "Vendor stops technical support, may continue security patches",
        "New features added",
        "No impact"
      ],
      "answer": 1,
      "explanation": "EOS discontinues technical support and feature updates; critical security patches may continue temporarily before full EOL, signaling migration need. Objective 3.4"
    },
    {
      "q": "When software reaches EOL, you should:",
      "options": [
        "Continue using indefinitely",
        "Plan migration to supported version or alternative",
        "Ignore it",
        "No action needed"
      ],
      "answer": 1,
      "explanation": "EOL requires migration planning to supported versions or alternatives, as continued use without patches creates security vulnerabilities and compliance issues. Objective 3.4"
    },
    {
      "q": "Decommissioning process should include:",
      "options": [
        "Immediate deletion",
        "Data backup, dependency checks, documentation, and gradual shutdown",
        "No process",
        "Random deletion"
      ],
      "answer": 1,
      "explanation": "Proper decommissioning backs up data, verifies no dependencies, updates documentation, archives configurations, and gradually removes resources. Objective 3.4"
    },
    {
      "q": "Before decommissioning a resource, verify:",
      "options": [
        "Nothing",
        "No dependencies exist and data is backed up or migrated",
        "Delete immediately",
        "Random check"
      ],
      "answer": 1,
      "explanation": "Pre-decommissioning checks identify dependencies, ensure data preservation/migration, document reason, and coordinate with stakeholders. Objective 3.4"
    },
    {
      "q": "Resource tagging for lifecycle management enables:",
      "options": [
        "No benefits",
        "Automated policies for updates, archiving, and decommissioning",
        "Manual only",
        "Random management"
      ],
      "answer": 1,
      "explanation": "Lifecycle tags (environment, owner, expiration date) enable automated policies for updates, cost allocation, retention, and decommissioning. Objective 3.4"
    },
    {
      "q": "Automated patch management provides:",
      "options": [
        "No automation",
        "Consistent, timely patching across resources",
        "Manual intervention always",
        "Random updates"
      ],
      "answer": 1,
      "explanation": "Automated patching (AWS Systems Manager, Azure Update Management) ensures consistent, timely updates across fleet reducing manual effort and exposure. Objective 3.4"
    },
    {
      "q": "Patch compliance reporting shows:",
      "options": [
        "Nothing",
        "Which resources need patches and compliance status",
        "Random data",
        "Cost only"
      ],
      "answer": 1,
      "explanation": "Compliance reporting identifies unpatched systems, missing security updates, and policy violations, enabling targeted remediation and audit evidence. Objective 3.4"
    },
    {
      "q": "Change windows for updates should be:",
      "options": [
        "Any time",
        "Scheduled during low-traffic periods with approval",
        "Random",
        "Peak hours"
      ],
      "answer": 1,
      "explanation": "Change windows schedule updates during maintenance periods, require approvals, communicate to stakeholders, and prepare rollback plans. Objective 3.4"
    },
    {
      "q": "Rollback procedures for failed updates should:",
      "options": [
        "Not exist",
        "Be documented and tested before deployment",
        "Improvise",
        "No planning"
      ],
      "answer": 1,
      "explanation": "Rollback procedures define steps to revert failed updates, should be tested beforehand, and include data restoration if necessary. Objective 3.4"
    },
    {
      "q": "Configuration management tools help with lifecycle by:",
      "options": [
        "No help",
        "Automating consistent configuration across resource lifecycle",
        "Manual work only",
        "Random changes"
      ],
      "answer": 1,
      "explanation": "Tools like Ansible, Puppet, Chef automate configuration, ensure consistency, manage updates, and enforce desired state throughout lifecycle. Objective 3.4"
    },
    {
      "q": "Version pinning in deployments:",
      "options": [
        "Uses latest always",
        "Locks to specific versions for stability and testing",
        "Random versions",
        "No versions"
      ],
      "answer": 1,
      "explanation": "Pinning dependencies to specific versions prevents unexpected breaking changes from automatic updates, ensuring stability and predictable behavior. Objective 3.4"
    },
    {
      "q": "Blue-green deployment facilitates updates by:",
      "options": [
        "No benefit",
        "Enabling testing and instant rollback of new versions",
        "Slowing updates",
        "Complicating process"
      ],
      "answer": 1,
      "explanation": "Blue-green allows testing new version in green environment before traffic switch, with instant rollback to blue if issues arise. Objective 3.4"
    },
    {
      "q": "Sunset process for applications includes:",
      "options": [
        "Immediate shutdown",
        "User notification, data migration, gradual shutdown, and documentation",
        "No process",
        "Random deletion"
      ],
      "answer": 1,
      "explanation": "Application sunset notifies users, migrates data, provides alternatives, maintains access temporarily, documents process, and archives data. Objective 3.4"
    }
  ],
  "5. DevOps Fundamentals": [
    {
      "q": "What is the primary purpose of version control systems?",
      "options": [
        "Slow down development",
        "Track changes, enable collaboration, and maintain code history",
        "Delete code",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Version control systems (Git, SVN) track every change to code, enable team collaboration, maintain complete history, and support rollback to previous versions. Objective 5.1"
    },
    {
      "q": "Git branching allows developers to:",
      "options": [
        "Delete all code",
        "Work on features independently without affecting main codebase",
        "Only view code",
        "No changes allowed"
      ],
      "answer": 1,
      "explanation": "Branches enable parallel development, allowing multiple developers to work on different features/fixes simultaneously without interfering with each other. Objective 5.1"
    },
    {
      "q": "The main/master branch typically contains:",
      "options": [
        "Experimental code",
        "Stable, production-ready code",
        "Deleted code",
        "Random files"
      ],
      "answer": 1,
      "explanation": "Main/master branch maintains stable, tested, production-ready code; all deployments typically come from this branch to ensure reliability. Objective 5.1"
    },
    {
      "q": "Feature branches are used for:",
      "options": [
        "Production deployments",
        "Developing new features in isolation",
        "Deleting features",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Feature branches isolate new feature development from main branch, enabling testing and review before merging into production code. Objective 5.1"
    },
    {
      "q": "A pull request (PR) enables:",
      "options": [
        "Automatic deployment",
        "Code review before merging changes",
        "Deleting branches",
        "No review"
      ],
      "answer": 1,
      "explanation": "Pull requests facilitate code review, discussion, automated testing, and approval before merging changes, ensuring code quality and knowledge sharing. Objective 5.1"
    },
    {
      "q": "Git merge combines:",
      "options": [
        "Nothing",
        "Changes from one branch into another",
        "Deletes branches",
        "Only conflicts"
      ],
      "answer": 1,
      "explanation": "Merge integrates changes from source branch into target branch, combining commit histories and code changes into unified codebase. Objective 5.1"
    },
    {
      "q": "Merge conflicts occur when:",
      "options": [
        "No issues",
        "Same code lines modified in different branches",
        "Successful merge",
        "No changes made"
      ],
      "answer": 1,
      "explanation": "Conflicts arise when same code sections are modified differently in branches being merged, requiring manual resolution to determine final version. Objective 5.1"
    },
    {
      "q": "Git commit creates:",
      "options": [
        "Branch",
        "Snapshot of changes with descriptive message",
        "Repository",
        "Conflict"
      ],
      "answer": 1,
      "explanation": "Commits save snapshots of code changes with messages describing what changed and why, building complete history of project evolution. Objective 5.1"
    },
    {
      "q": "Git push sends:",
      "options": [
        "Nothing",
        "Local commits to remote repository",
        "Deletes remote code",
        "Only conflicts"
      ],
      "answer": 1,
      "explanation": "Push uploads local commits to remote repository (GitHub, GitLab), sharing changes with team and backing up work to central location. Objective 5.1"
    },
    {
      "q": "Git pull retrieves:",
      "options": [
        "Nothing",
        "Latest changes from remote repository to local",
        "Deletes local code",
        "Only conflicts"
      ],
      "answer": 1,
      "explanation": "Pull fetches and merges latest changes from remote repository into local branch, keeping local copy synchronized with team changes. Objective 5.1"
    },
    {
      "q": "A Git tag is used to:",
      "options": [
        "Delete commits",
        "Mark specific points in history like releases",
        "Create branches",
        "Cause conflicts"
      ],
      "answer": 1,
      "explanation": "Tags create immutable references to specific commits, typically marking release versions (v1.0, v2.0) for easy identification and deployment. Objective 5.1"
    },
    {
      "q": "Git clone creates:",
      "options": [
        "New branch",
        "Complete local copy of remote repository",
        "Tag",
        "Conflict"
      ],
      "answer": 1,
      "explanation": "Clone downloads entire repository including all history, branches, and files to local machine, enabling independent work on copy. Objective 5.1"
    },
    {
      "q": ".gitignore file specifies:",
      "options": [
        "Required files",
        "Files to exclude from version control",
        "Merge strategy",
        "Branch names"
      ],
      "answer": 1,
      "explanation": ".gitignore lists files/patterns to exclude from Git (build artifacts, credentials, IDE files), preventing unnecessary or sensitive files from being tracked. Objective 5.1"
    },
    {
      "q": "Git rebase differs from merge by:",
      "options": [
        "No difference",
        "Rewriting commit history to create linear progression",
        "Deleting commits",
        "Only creating conflicts"
      ],
      "answer": 1,
      "explanation": "Rebase moves/combines commits to create linear history without merge commits, resulting in cleaner history but rewriting commit chronology. Objective 5.1"
    },
    {
      "q": "Code review best practices include:",
      "options": [
        "Skip all reviews",
        "Reviewing for logic, security, style, and maintainability",
        "Only syntax check",
        "Auto-approve everything"
      ],
      "answer": 1,
      "explanation": "Effective reviews check correctness, security vulnerabilities, code style, test coverage, documentation, and maintainability before approval. Objective 5.1"
    },
    {
      "q": "What should commit messages include?",
      "options": [
        "Nothing",
        "Clear description of what changed and why",
        "Random text",
        "Only emoji"
      ],
      "answer": 1,
      "explanation": "Good commit messages clearly explain what changed, why it changed, and any relevant context, making history understandable and useful. Objective 5.1"
    },
    {
      "q": "Branch protection rules can enforce:",
      "options": [
        "No restrictions",
        "Required reviews, status checks, and merge restrictions",
        "Anyone can delete",
        "No rules"
      ],
      "answer": 1,
      "explanation": "Protection rules require PR approvals, passing tests, up-to-date branches, and prevent force pushes/deletions, ensuring code quality. Objective 5.1"
    },
    {
      "q": "Git stash temporarily:",
      "options": [
        "Deletes changes",
        "Saves uncommitted changes for later retrieval",
        "Creates branch",
        "Pushes to remote"
      ],
      "answer": 1,
      "explanation": "Stash saves current uncommitted changes without committing, allowing branch switching or pulling updates, then reapplying changes later. Objective 5.1"
    },
    {
      "q": "Forking a repository creates:",
      "options": [
        "Branch",
        "Independent copy under your account",
        "Tag",
        "Merge"
      ],
      "answer": 1,
      "explanation": "Fork creates personal copy of repository where you can freely experiment, then submit changes back via pull request to original. Objective 5.1"
    },
    {
      "q": "Git cherry-pick allows:",
      "options": [
        "Deleting commits",
        "Applying specific commits from one branch to another",
        "Creating repository",
        "No action"
      ],
      "answer": 1,
      "explanation": "Cherry-pick selectively applies specific commits to current branch without merging entire branch history, useful for hotfixes. Objective 5.1"
    },
    {
      "q": "Git diff shows:",
      "options": [
        "Nothing",
        "Differences between commits, branches, or working directory",
        "Merge commits only",
        "Tags"
      ],
      "answer": 1,
      "explanation": "Diff displays line-by-line changes between versions, helping review modifications before committing or understanding what changed. Objective 5.1"
    },
    {
      "q": "Release branches are used for:",
      "options": [
        "Feature development",
        "Preparing and stabilizing code for production release",
        "Experimentation",
        "Deleting code"
      ],
      "answer": 1,
      "explanation": "Release branches isolate release preparation (bug fixes, documentation, version bumps) while allowing continued feature development on main. Objective 5.1"
    },
    {
      "q": "Hotfix branches address:",
      "options": [
        "New features",
        "Critical production bugs requiring immediate fixes",
        "Regular updates",
        "Experiments"
      ],
      "answer": 1,
      "explanation": "Hotfix branches branch from production, fix critical bugs, and merge back to production and development, bypassing normal release cycle. Objective 5.1"
    },
    {
      "q": "Git workflow strategies include:",
      "options": [
        "No strategy",
        "GitFlow, GitHub Flow, trunk-based development",
        "Random commits",
        "No branching"
      ],
      "answer": 1,
      "explanation": "Workflows define branching strategies: GitFlow (multiple long-lived branches), GitHub Flow (feature branches), trunk-based (frequent main commits). Objective 5.1"
    },
    {
      "q": "Trunk-based development emphasizes:",
      "options": [
        "Long-lived branches",
        "Frequent small commits directly to main branch",
        "No commits",
        "Annual merges"
      ],
      "answer": 1,
      "explanation": "Trunk-based development commits small changes frequently to main, reducing merge conflicts and enabling continuous integration/deployment. Objective 5.1"
    },
    {
      "q": "Configuration management tools ensure:",
      "options": [
        "Manual configuration",
        "Consistent, automated infrastructure and application configuration",
        "Random settings",
        "No management"
      ],
      "answer": 1,
      "explanation": "CM tools (Ansible, Puppet, Chef) automate configuration deployment, ensure consistency across environments, and enforce desired state. Objective 5.2"
    },
    {
      "q": "Ansible playbooks are written in:",
      "options": [
        "Binary",
        "YAML format describing desired state",
        "Assembly",
        "No format"
      ],
      "answer": 1,
      "explanation": "Ansible playbooks use human-readable YAML to define tasks, configurations, and orchestration steps for automated infrastructure management. Objective 5.2"
    },
    {
      "q": "Idempotency in configuration management means:",
      "options": [
        "Different results each run",
        "Running multiple times produces same result",
        "Always fails",
        "Random outcomes"
      ],
      "answer": 1,
      "explanation": "Idempotent operations produce consistent results regardless of repetition - applying same configuration multiple times achieves same state safely. Objective 5.2"
    },
    {
      "q": "Infrastructure as Code (IaC) treats infrastructure as:",
      "options": [
        "Physical only",
        "Code that can be versioned, tested, and automated",
        "Manual documentation",
        "Random hardware"
      ],
      "answer": 1,
      "explanation": "IaC defines infrastructure in code files enabling version control, automated deployment, testing, and consistent reproducible environments. Objective 5.2"
    },
    {
      "q": "Configuration drift occurs when:",
      "options": [
        "Desired state maintained",
        "Actual configuration diverges from defined state",
        "Perfect alignment",
        "No configuration"
      ],
      "answer": 1,
      "explanation": "Drift happens when manual changes or errors cause actual state to differ from defined configuration, causing inconsistencies and issues. Objective 5.2"
    },
    {
      "q": "Desired state configuration defines:",
      "options": [
        "Current state",
        "Target configuration systems should maintain",
        "Historical state",
        "Random state"
      ],
      "answer": 1,
      "explanation": "Desired state specifies target configuration; CM tools continuously enforce this state, automatically correcting deviations. Objective 5.2"
    },
    {
      "q": "Ansible is agentless, meaning:",
      "options": [
        "No functionality",
        "No software required on managed nodes, uses SSH",
        "Requires agents everywhere",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Ansible doesn't require agent installation on managed nodes; it uses SSH (Linux) or WinRM (Windows) for communication, simplifying deployment. Objective 5.2"
    },
    {
      "q": "Puppet uses agent-based architecture where:",
      "options": [
        "No agents",
        "Agents on managed nodes communicate with Puppet master",
        "Manual only",
        "No master"
      ],
      "answer": 1,
      "explanation": "Puppet agents run on managed nodes, periodically checking with Puppet master for configuration updates and reporting status. Objective 5.2"
    },
    {
      "q": "Configuration management advantages include:",
      "options": [
        "More manual work",
        "Consistency, repeatability, faster deployment, reduced errors",
        "Slower processes",
        "Random results"
      ],
      "answer": 1,
      "explanation": "CM provides consistent configurations, eliminates manual errors, enables rapid deployment, ensures compliance, and documents infrastructure as code. Objective 5.2"
    },
    {
      "q": "Ansible inventory files define:",
      "options": [
        "Nothing",
        "Managed hosts and their groupings",
        "Only local machine",
        "Random servers"
      ],
      "answer": 1,
      "explanation": "Inventory lists managed hosts, organizes them into groups (web, database), and defines connection parameters for automation. Objective 5.2"
    },
    {
      "q": "Ansible roles provide:",
      "options": [
        "No structure",
        "Reusable, organized collections of tasks and configurations",
        "Random code",
        "Single use only"
      ],
      "answer": 1,
      "explanation": "Roles organize playbooks into reusable components with standardized directory structure (tasks, handlers, templates, files) for modularity. Objective 5.2"
    },
    {
      "q": "Variables in configuration management enable:",
      "options": [
        "No flexibility",
        "Parameterizing configurations for different environments",
        "Hardcoded values only",
        "No reuse"
      ],
      "answer": 1,
      "explanation": "Variables allow same code to deploy different configurations (dev/prod) by changing values, improving reusability and maintainability. Objective 5.2"
    },
    {
      "q": "Templates in CM tools allow:",
      "options": [
        "Static files only",
        "Dynamic file generation using variables and logic",
        "No customization",
        "Manual editing only"
      ],
      "answer": 1,
      "explanation": "Templates (Jinja2 in Ansible) generate configuration files dynamically, inserting variables and applying logic for environment-specific configs. Objective 5.2"
    },
    {
      "q": "Handlers in Ansible execute:",
      "options": [
        "Always",
        "Only when notified by tasks that make changes",
        "Never",
        "Randomly"
      ],
      "answer": 1,
      "explanation": "Handlers run only when triggered by task notifications (e.g., restart service only if config changed), avoiding unnecessary actions. Objective 5.2"
    },
    {
      "q": "Configuration management testing should verify:",
      "options": [
        "Nothing",
        "Configurations apply correctly and achieve desired state",
        "Only syntax",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Testing validates syntax, idempotency, desired state achievement, and absence of unintended side effects before production deployment. Objective 5.2"
    },
    {
      "q": "Chef cookbooks contain:",
      "options": [
        "Food recipes",
        "Reusable configuration code and resources",
        "Random files",
        "No content"
      ],
      "answer": 1,
      "explanation": "Chef cookbooks package related recipes, resources, templates, and files into reusable units for infrastructure configuration management. Objective 5.2"
    },
    {
      "q": "Configuration management supports compliance by:",
      "options": [
        "No support",
        "Enforcing security policies and audit requirements consistently",
        "Random configs",
        "Manual tracking"
      ],
      "answer": 1,
      "explanation": "CM enforces security baselines, compliance policies, and audit requirements consistently across infrastructure with documented, versioned configurations. Objective 5.2"
    },
    {
      "q": "Secrets management in configuration code should:",
      "options": [
        "Hardcode passwords",
        "Use encrypted vaults or external secret stores",
        "Plain text files",
        "No secrets"
      ],
      "answer": 1,
      "explanation": "Secrets require encryption (Ansible Vault, HashiCorp Vault) or external stores, never hardcoded in configuration code or version control. Objective 5.2"
    },
    {
      "q": "Configuration as Code benefits from:",
      "options": [
        "No version control",
        "Version control, peer review, and automated testing",
        "Manual only",
        "Random changes"
      ],
      "answer": 1,
      "explanation": "Treating configuration as code enables version control, change tracking, code review, automated testing, and rollback capabilities. Objective 5.2"
    },
    {
      "q": "Ansible check mode:",
      "options": [
        "Makes changes",
        "Simulates execution without making actual changes",
        "Deletes configuration",
        "No function"
      ],
      "answer": 1,
      "explanation": "Check mode (dry run) shows what would change without applying modifications, enabling safe testing before actual execution. Objective 5.2"
    },
    {
      "q": "Configuration management scales by:",
      "options": [
        "Manual work only",
        "Applying same configurations consistently across thousands of nodes",
        "Single server only",
        "No scaling"
      ],
      "answer": 1,
      "explanation": "CM tools efficiently manage configurations across thousands of systems simultaneously, maintaining consistency at scale impossible manually. Objective 5.2"
    },
    {
      "q": "Orchestration in configuration management:",
      "options": [
        "Single task only",
        "Coordinates complex workflows across multiple systems",
        "No coordination",
        "Random execution"
      ],
      "answer": 1,
      "explanation": "Orchestration sequences tasks across multiple systems in specific order (deploy DB first, then app), managing complex multi-tier deployments. Objective 5.2"
    },
    {
      "q": "Configuration management documentation should include:",
      "options": [
        "Nothing",
        "Purpose, variables, dependencies, and usage examples",
        "Only code",
        "No documentation"
      ],
      "answer": 1,
      "explanation": "Documentation explains configuration purpose, required variables, system dependencies, usage examples, and expected outcomes for team understanding. Objective 5.2"
    },
    {
      "q": "Continuous configuration management:",
      "options": [
        "One-time setup",
        "Regularly enforces desired state and corrects drift",
        "Manual checks only",
        "No monitoring"
      ],
      "answer": 1,
      "explanation": "Continuous enforcement regularly checks and corrects configuration drift, maintaining desired state automatically without manual intervention. Objective 5.2"
    },
    {
      "q": "Configuration baselines define:",
      "options": [
        "Random settings",
        "Standard, approved configurations for systems",
        "Maximum settings",
        "No standards"
      ],
      "answer": 1,
      "explanation": "Baselines establish approved standard configurations ensuring security, compliance, and consistency across similar systems and environments. Objective 5.2"
    },
    {
      "q": "Continuous Integration (CI) involves:",
      "options": [
        "Monthly code merges",
        "Frequently merging code changes with automated testing",
        "Manual testing only",
        "No integration"
      ],
      "answer": 1,
      "explanation": "CI automatically builds and tests code when developers commit changes, detecting integration issues early through frequent merging and automated validation. Objective 5.3"
    },
    {
      "q": "Continuous Delivery (CD) ensures:",
      "options": [
        "Manual deployment",
        "Code is always in deployable state, ready for production",
        "No deployments",
        "Annual releases"
      ],
      "answer": 1,
      "explanation": "Continuous Delivery automates testing and staging so code is always production-ready; deployment to production requires manual approval. Objective 5.3"
    },
    {
      "q": "Continuous Deployment differs from Continuous Delivery by:",
      "options": [
        "No difference",
        "Automatically deploying to production without manual approval",
        "Manual only",
        "No automation"
      ],
      "answer": 1,
      "explanation": "Continuous Deployment automatically releases all passing changes to production without human intervention, while Delivery requires manual approval. Objective 5.3"
    },
    {
      "q": "CI/CD pipelines automate:",
      "options": [
        "Nothing",
        "Build, test, and deployment processes",
        "Manual work only",
        "Random tasks"
      ],
      "answer": 1,
      "explanation": "Pipelines automate entire software delivery workflow: code checkout, building, testing, security scanning, and deployment to various environments. Objective 5.3"
    },
    {
      "q": "Build automation compiles:",
      "options": [
        "Nothing",
        "Source code into executable artifacts automatically",
        "Manual compilation",
        "Documentation only"
      ],
      "answer": 1,
      "explanation": "Build automation compiles code, resolves dependencies, packages artifacts, and ensures consistent, repeatable builds without manual steps. Objective 5.3"
    },
    {
      "q": "Unit tests verify:",
      "options": [
        "Entire application",
        "Individual code units/functions work correctly in isolation",
        "Infrastructure",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Unit tests validate individual functions/methods in isolation, running quickly and frequently to catch logic errors early in development. Objective 5.3"
    },
    {
      "q": "Integration tests verify:",
      "options": [
        "Single functions",
        "Multiple components work together correctly",
        "Infrastructure only",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Integration tests validate interactions between components (application and database), ensuring modules integrate correctly. Objective 5.3"
    },
    {
      "q": "End-to-end tests validate:",
      "options": [
        "Single function",
        "Complete user workflows through entire application",
        "Code syntax",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "E2E tests simulate real user scenarios testing entire application stack from UI to database, validating complete functionality. Objective 5.3"
    },
    {
      "q": "Smoke tests provide:",
      "options": [
        "Comprehensive testing",
        "Quick validation that basic functionality works after deployment",
        "No testing",
        "Only security"
      ],
      "answer": 1,
      "explanation": "Smoke tests quickly verify critical functionality works after deployment (app starts, key endpoints respond), catching major issues immediately. Objective 5.3"
    },
    {
      "q": "Code coverage measures:",
      "options": [
        "Code size",
        "Percentage of code executed by tests",
        "Only errors",
        "Deployment frequency"
      ],
      "answer": 1,
      "explanation": "Code coverage shows what percentage of code is tested, identifying untested paths that may contain undetected bugs. Objective 5.3"
    },
    {
      "q": "Static code analysis checks:",
      "options": [
        "Runtime behavior",
        "Code quality, style, and potential bugs without execution",
        "Performance only",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Static analysis examines code without running it, identifying style violations, potential bugs, security issues, and code smells. Objective 5.3"
    },
    {
      "q": "Security scanning in CI/CD should detect:",
      "options": [
        "Nothing",
        "Vulnerabilities, dependency issues, and security misconfigurations",
        "Only performance",
        "Code style"
      ],
      "answer": 1,
      "explanation": "Security scanning identifies vulnerable dependencies, security misconfigurations, secrets in code, and OWASP vulnerabilities before production. Objective 5.3"
    },
    {
      "q": "Artifact repositories store:",
      "options": [
        "Nothing",
        "Built packages, containers, and dependencies",
        "Source code only",
        "Logs only"
      ],
      "answer": 1,
      "explanation": "Artifact repositories (Artifactory, Nexus, ECR) store built artifacts, container images, libraries, and packages for versioning and deployment. Objective 5.3"
    },
    {
      "q": "Pipeline stages typically execute in:",
      "options": [
        "Random order",
        "Sequential order with gates between stages",
        "Parallel always",
        "No order"
      ],
      "answer": 1,
      "explanation": "Pipeline stages run sequentially (build \u2192 test \u2192 deploy), with each stage gating next; failures stop progression preventing bad code deployment. Objective 5.3"
    },
    {
      "q": "Deployment environments typically include:",
      "options": [
        "Production only",
        "Development, testing, staging, and production",
        "One environment",
        "No environments"
      ],
      "answer": 1,
      "explanation": "Multiple environments enable progressive testing: dev (rapid iteration), test (integration), staging (production-like validation), production (live users). Objective 5.3"
    },
    {
      "q": "Staging environment should:",
      "options": [
        "Differ from production",
        "Mirror production configuration as closely as possible",
        "Be much smaller",
        "Not exist"
      ],
      "answer": 1,
      "explanation": "Staging mirrors production (infrastructure, data, configuration) enabling realistic testing and catching environment-specific issues before production. Objective 5.3"
    },
    {
      "q": "Pipeline failure should:",
      "options": [
        "Continue anyway",
        "Stop progression and notify team for investigation",
        "Deploy automatically",
        "Be ignored"
      ],
      "answer": 1,
      "explanation": "Failed pipelines halt deployment, notify responsible teams, and provide failure details, preventing defective code from reaching production. Objective 5.3"
    },
    {
      "q": "Rollback automation enables:",
      "options": [
        "No rollback",
        "Quick reversion to previous version if issues arise",
        "Manual only",
        "Random versions"
      ],
      "answer": 1,
      "explanation": "Automated rollback quickly restores previous working version when deployment issues occur, minimizing downtime and user impact. Objective 5.3"
    },
    {
      "q": "Blue-green deployment maintains:",
      "options": [
        "Single environment",
        "Two identical environments for zero-downtime switching",
        "No environments",
        "Random setup"
      ],
      "answer": 1,
      "explanation": "Blue-green keeps two production environments; deploy to inactive, test, then switch traffic instantly with immediate rollback capability. Objective 5.3"
    },
    {
      "q": "Canary deployment gradually:",
      "options": [
        "Deploys to all at once",
        "Routes small traffic percentage to new version",
        "No deployment",
        "Deletes old version"
      ],
      "answer": 1,
      "explanation": "Canary releases new version to small user subset (5-10%), monitoring metrics before full rollout, limiting impact of issues. Objective 5.3"
    },
    {
      "q": "Feature flags allow:",
      "options": [
        "No features",
        "Enabling/disabling features without redeployment",
        "Only deletions",
        "Random behavior"
      ],
      "answer": 1,
      "explanation": "Feature flags toggle features at runtime, enabling gradual rollout, A/B testing, and instant disabling without code deployment. Objective 5.3"
    },
    {
      "q": "Deployment approval gates require:",
      "options": [
        "No approvals",
        "Manual authorization before critical stage progression",
        "Automatic always",
        "Random decision"
      ],
      "answer": 1,
      "explanation": "Approval gates require designated approvers to authorize progression to sensitive stages (production), adding human oversight. Objective 5.3"
    },
    {
      "q": "Pipeline notifications should inform:",
      "options": [
        "No one",
        "Relevant teams of build status, failures, and deployments",
        "Everyone always",
        "Random people"
      ],
      "answer": 1,
      "explanation": "Notifications alert appropriate teams (Slack, email, SMS) of pipeline status, failures, and deployment completions for rapid response. Objective 5.3"
    },
    {
      "q": "Build triggers can be:",
      "options": [
        "Nothing",
        "Code commits, schedules, manual starts, or other events",
        "Random only",
        "Never trigger"
      ],
      "answer": 1,
      "explanation": "Pipelines trigger on git commits, pull requests, schedules (nightly builds), manual requests, or external events (image updates). Objective 5.3"
    },
    {
      "q": "Pipeline as Code defines:",
      "options": [
        "No code",
        "Pipeline configuration in version-controlled files",
        "Manual GUI only",
        "Random setup"
      ],
      "answer": 1,
      "explanation": "Pipeline as Code (Jenkinsfile, GitLab CI YAML) version-controls pipeline definitions, enabling review, testing, and rollback of pipeline changes. Objective 5.3"
    },
    {
      "q": "Parallel execution in pipelines:",
      "options": [
        "Always sequential",
        "Runs independent stages simultaneously to reduce time",
        "Never parallel",
        "Random execution"
      ],
      "answer": 1,
      "explanation": "Parallelization runs independent tasks simultaneously (multiple test suites, different environments), significantly reducing total pipeline duration. Objective 5.3"
    },
    {
      "q": "Container images in CI/CD provide:",
      "options": [
        "No benefits",
        "Consistent runtime environment across pipeline stages",
        "Random environments",
        "No consistency"
      ],
      "answer": 1,
      "explanation": "Container images ensure identical runtime environment from build through production, eliminating works-on-my-machine issues. Objective 5.3"
    },
    {
      "q": "Deployment frequency in mature DevOps organizations is:",
      "options": [
        "Annually",
        "Multiple times per day",
        "Never",
        "Decades"
      ],
      "answer": 1,
      "explanation": "High-performing teams deploy multiple times daily through automated pipelines, small changes, and strong testing, reducing risk and improving velocity. Objective 5.3"
    },
    {
      "q": "Lead time for changes measures:",
      "options": [
        "Nothing",
        "Time from code commit to production deployment",
        "Only development time",
        "Random duration"
      ],
      "answer": 1,
      "explanation": "Lead time tracks duration from commit to production, indicating efficiency of delivery process and automation maturity. Objective 5.3"
    },
    {
      "q": "Mean time to recovery (MTTR) measures:",
      "options": [
        "Development speed",
        "Time to restore service after failure",
        "Build time only",
        "No measurement"
      ],
      "answer": 1,
      "explanation": "MTTR measures how quickly teams detect, respond to, and recover from failures, indicating incident response effectiveness. Objective 5.3"
    },
    {
      "q": "Change failure rate tracks:",
      "options": [
        "All changes",
        "Percentage of deployments causing failures",
        "No failures",
        "Random events"
      ],
      "answer": 1,
      "explanation": "Change failure rate measures deployment quality by tracking percentage requiring remediation (hotfix, rollback, patch), indicating testing effectiveness. Objective 5.3"
    },
    {
      "q": "Infrastructure testing in pipelines validates:",
      "options": [
        "Only code",
        "Infrastructure configuration and provisioning",
        "Nothing",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Infrastructure testing validates IaC provisions correctly, configurations are secure, and compliance requirements are met before deployment. Objective 5.3"
    },
    {
      "q": "Database migration in CI/CD should:",
      "options": [
        "Manual only",
        "Be automated, versioned, and tested with rollback capability",
        "Never migrate",
        "Random changes"
      ],
      "answer": 1,
      "explanation": "Database migrations are versioned, tested in pipeline, applied automatically during deployment with rollback scripts for failures. Objective 5.3"
    },
    {
      "q": "Pipeline caching improves:",
      "options": [
        "Nothing",
        "Build speed by reusing dependencies and artifacts",
        "Slows builds",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Caching stores dependencies, build artifacts, and intermediate results, avoiding redundant downloads/builds and significantly speeding pipelines. Objective 5.3"
    },
    {
      "q": "GitOps manages infrastructure by:",
      "options": [
        "Manual changes",
        "Using Git as single source of truth for declarative infrastructure",
        "Random changes",
        "No version control"
      ],
      "answer": 1,
      "explanation": "GitOps stores desired infrastructure state in Git; automated systems continuously reconcile actual state with Git, enabling auditable infrastructure changes. Objective 5.3"
    },
    {
      "q": "Orchestration differs from automation by:",
      "options": [
        "No difference",
        "Coordinating multiple automated tasks into workflows",
        "Single task focus",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Automation handles individual tasks; orchestration coordinates multiple automated tasks across systems in specific sequence for complex workflows. Objective 5.4"
    },
    {
      "q": "Workflow automation sequences:",
      "options": [
        "Nothing",
        "Multiple tasks in defined order with dependencies",
        "Single task only",
        "Random execution"
      ],
      "answer": 1,
      "explanation": "Workflows define task sequences, dependencies, conditions, and error handling, automating complex multi-step processes across systems. Objective 5.4"
    },
    {
      "q": "Event-driven automation triggers when:",
      "options": [
        "Never",
        "Specific events occur in infrastructure or applications",
        "Scheduled only",
        "Manual only"
      ],
      "answer": 1,
      "explanation": "Event-driven automation responds to events (alerts, resource changes, log patterns), executing responses automatically without human intervention. Objective 5.4"
    },
    {
      "q": "Runbooks provide:",
      "options": [
        "Nothing",
        "Documented procedures for operational tasks",
        "Random instructions",
        "No documentation"
      ],
      "answer": 1,
      "explanation": "Runbooks document step-by-step procedures for routine operations, troubleshooting, and incident response, enabling consistent execution and automation. Objective 5.4"
    },
    {
      "q": "Automated remediation responds to:",
      "options": [
        "Nothing",
        "Known issues by executing predefined corrective actions",
        "Random events",
        "Manual intervention only"
      ],
      "answer": 1,
      "explanation": "Auto-remediation detects issues (service down, high CPU) and automatically executes fixes (restart service, scale resources) without human involvement. Objective 5.4"
    },
    {
      "q": "Self-healing infrastructure automatically:",
      "options": [
        "Breaks systems",
        "Detects and recovers from failures without intervention",
        "Requires manual fixes",
        "No recovery"
      ],
      "answer": 1,
      "explanation": "Self-healing systems detect failures, automatically replace failed components, and restore service without human intervention, improving reliability. Objective 5.4"
    },
    {
      "q": "Chaos engineering intentionally:",
      "options": [
        "Fixes everything",
        "Introduces failures to test system resilience",
        "Avoids all issues",
        "Manual testing only"
      ],
      "answer": 1,
      "explanation": "Chaos engineering deliberately injects failures (instance termination, network latency) to validate resilience, identify weaknesses, and improve recovery. Objective 5.4"
    },
    {
      "q": "API-driven automation enables:",
      "options": [
        "Manual work only",
        "Programmatic control of infrastructure and services",
        "No automation",
        "Random actions"
      ],
      "answer": 1,
      "explanation": "APIs enable programmatic infrastructure management, integration between systems, and building custom automation tools and workflows. Objective 5.4"
    },
    {
      "q": "Scheduled automation executes tasks:",
      "options": [
        "Randomly",
        "At predefined times or intervals",
        "Never",
        "Only manually"
      ],
      "answer": 1,
      "explanation": "Scheduled automation runs tasks at specific times (cron jobs, scheduled scaling) for maintenance, backups, reports, and predictable operations. Objective 5.4"
    },
    {
      "q": "Automation testing validates:",
      "options": [
        "Nothing",
        "Automation scripts work correctly and handle errors",
        "Manual steps only",
        "No validation"
      ],
      "answer": 1,
      "explanation": "Automation scripts require testing for correctness, idempotency, error handling, and edge cases before production use to prevent automated failures. Objective 5.4"
    },
    {
      "q": "Monitoring-driven automation uses:",
      "options": [
        "No data",
        "Metrics and alerts to trigger automated responses",
        "Manual decisions only",
        "Random triggers"
      ],
      "answer": 1,
      "explanation": "Monitoring data triggers automation (high CPU scales out, failed health check replaces instance), creating feedback loops for optimization. Objective 5.4"
    },
    {
      "q": "Automation scope should be:",
      "options": [
        "Automate everything immediately",
        "Gradually expanded based on value and risk",
        "Never automate",
        "Random selection"
      ],
      "answer": 1,
      "explanation": "Start automating high-value, low-risk tasks; gradually expand to complex operations after gaining confidence and refining processes. Objective 5.4"
    },
    {
      "q": "Automation documentation should include:",
      "options": [
        "Nothing",
        "Purpose, triggers, actions, error handling, and dependencies",
        "Only code",
        "No documentation"
      ],
      "answer": 1,
      "explanation": "Document automation purpose, trigger conditions, executed actions, error handling, dependencies, and runbook references for team understanding. Objective 5.4"
    },
    {
      "q": "Exception handling in automation should:",
      "options": [
        "Crash scripts",
        "Gracefully handle errors and notify appropriate teams",
        "Ignore errors",
        "Delete everything"
      ],
      "answer": 1,
      "explanation": "Robust automation includes error handling, logging, notifications, and graceful degradation preventing cascading failures from automation errors. Objective 5.4"
    },
    {
      "q": "Immutable infrastructure automation:",
      "options": [
        "Updates servers in place",
        "Replaces entire servers rather than updating them",
        "Manual updates",
        "Never changes"
      ],
      "answer": 1,
      "explanation": "Immutable infrastructure automation deploys new servers with updated configuration rather than modifying existing ones, ensuring consistency and enabling rollback. Objective 5.4"
    }
  ],
  "6. Troubleshooting": [
    {
      "q": "The first step in troubleshooting methodology is:",
      "options": [
        "Apply solution immediately",
        "Identify the problem and gather information",
        "Reboot everything",
        "Escalate to management"
      ],
      "answer": 1,
      "explanation": "Troubleshooting begins with identifying problem symptoms, gathering information about when it started, affected systems, and recent changes before jumping to solutions. Objective 6.1"
    },
    {
      "q": "After identifying the problem, the next step is:",
      "options": [
        "Random fixes",
        "Establish a theory of probable cause",
        "Give up",
        "Reboot server"
      ],
      "answer": 1,
      "explanation": "After gathering information, establish theories about the cause based on symptoms, starting with most likely/simplest explanations before complex ones. Objective 6.1"
    },
    {
      "q": "Before implementing a solution, you should:",
      "options": [
        "Just do it",
        "Test the theory and establish an action plan",
        "Delete everything",
        "Ignore the plan"
      ],
      "answer": 1,
      "explanation": "Test theories in non-production when possible, develop action plan with steps and rollback procedures before implementing in production. Objective 6.1"
    },
    {
      "q": "After implementing a fix, you should:",
      "options": [
        "Move on immediately",
        "Verify functionality and prevent recurrence",
        "Delete logs",
        "No follow-up needed"
      ],
      "answer": 1,
      "explanation": "Verify the fix resolves the issue, ensure no side effects, document the solution, and implement preventive measures to avoid recurrence. Objective 6.1"
    },
    {
      "q": "Documentation in troubleshooting should include:",
      "options": [
        "Nothing",
        "Problem, cause, solution, and prevention steps",
        "Only time spent",
        "Random notes"
      ],
      "answer": 1,
      "explanation": "Document problem symptoms, root cause, solution implemented, verification results, and preventive measures for knowledge sharing and future reference. Objective 6.1"
    },
    {
      "q": "When should you escalate an issue?",
      "options": [
        "Immediately always",
        "When beyond your expertise or requires higher authority",
        "Never escalate",
        "After weeks of trying"
      ],
      "answer": 1,
      "explanation": "Escalate when problem exceeds your skill/authority, requires vendor support, impacts business significantly, or after reasonable troubleshooting attempts. Objective 6.1"
    },
    {
      "q": "Recent changes are important because:",
      "options": [
        "Not relevant",
        "They often correlate with new problems",
        "Random information",
        "Only for documentation"
      ],
      "answer": 1,
      "explanation": "Recent changes (deployments, configurations, updates) frequently cause issues; identifying timing correlation helps pinpoint root cause quickly. Objective 6.1"
    },
    {
      "q": "Isolating the problem involves:",
      "options": [
        "Testing everything",
        "Narrowing scope to identify affected components",
        "Random testing",
        "No isolation needed"
      ],
      "answer": 1,
      "explanation": "Isolation determines which components are affected vs working, reducing scope and focusing troubleshooting efforts on relevant systems. Objective 6.1"
    },
    {
      "q": "The divide-and-conquer approach:",
      "options": [
        "Tests randomly",
        "Systematically eliminates half of possibilities each test",
        "Tests everything",
        "No strategy"
      ],
      "answer": 1,
      "explanation": "Divide-and-conquer tests middle of system to eliminate half possibilities each iteration, efficiently narrowing down problem location. Objective 6.1"
    },
    {
      "q": "Multiple simultaneous changes during troubleshooting:",
      "options": [
        "Speed up resolution",
        "Make it unclear which change fixed the issue",
        "Always best",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Change one variable at a time so you know exactly what fixed the issue, enabling proper documentation and understanding of root cause. Objective 6.1"
    },
    {
      "q": "Reproducing the problem helps:",
      "options": [
        "Waste time",
        "Validate theories and verify solutions",
        "Create more issues",
        "No benefit"
      ],
      "answer": 1,
      "explanation": "Reproducing issues validates understanding, tests theories, and confirms solutions actually fix the problem rather than coincidental timing. Objective 6.1"
    },
    {
      "q": "When initial theory is disproven, you should:",
      "options": [
        "Give up",
        "Develop new theory based on new information",
        "Implement anyway",
        "Escalate immediately"
      ],
      "answer": 1,
      "explanation": "If theory is disproven, gather additional information, reassess symptoms, and develop new theory based on expanded understanding. Objective 6.1"
    },
    {
      "q": "Baseline configurations help troubleshooting by:",
      "options": [
        "No help",
        "Providing comparison point to identify deviations",
        "Slowing process",
        "Only documentation"
      ],
      "answer": 1,
      "explanation": "Baselines show known-good state; comparing current to baseline quickly identifies configuration drift or changes causing issues. Objective 6.1"
    },
    {
      "q": "Post-implementation review should:",
      "options": [
        "Skip it",
        "Analyze what happened and improve processes",
        "Blame individuals",
        "Ignore lessons"
      ],
      "answer": 1,
      "explanation": "Post-mortems identify root cause, improve processes, document lessons learned, and implement preventive measures without blame. Objective 6.1"
    },
    {
      "q": "Troubleshooting in production requires:",
      "options": [
        "No caution",
        "Extra care, change control, and rollback plans",
        "Random changes",
        "No planning"
      ],
      "answer": 1,
      "explanation": "Production troubleshooting requires change approval, communication, rollback plans, and careful testing to avoid making problems worse. Objective 6.1"
    },
    {
      "q": "If ping fails but telnet to port 80 succeeds, the issue is likely:",
      "options": [
        "Total network failure",
        "ICMP blocked but TCP working",
        "DNS failure",
        "Everything broken"
      ],
      "answer": 1,
      "explanation": "Successful TCP connection (telnet) but failed ping indicates ICMP protocol is blocked by firewall while TCP traffic passes normally. Objective 6.2"
    },
    {
      "q": "DNS resolution failure results in:",
      "options": [
        "Successful connections",
        "Inability to resolve names to IP addresses",
        "Faster connections",
        "No impact"
      ],
      "answer": 1,
      "explanation": "DNS issues prevent hostname resolution but direct IP connections may work, indicating name resolution rather than network connectivity problem. Objective 6.2"
    },
    {
      "q": "To test DNS specifically, use:",
      "options": [
        "ping only",
        "nslookup or dig commands",
        "telnet",
        "ssh"
      ],
      "answer": 1,
      "explanation": "nslookup and dig query DNS servers directly, showing resolution results, response times, and authoritative servers for DNS troubleshooting. Objective 6.2"
    },
    {
      "q": "Intermittent connectivity often indicates:",
      "options": [
        "Total failure",
        "Network congestion, failing hardware, or load balancer issues",
        "Everything perfect",
        "DNS problems"
      ],
      "answer": 1,
      "explanation": "Intermittent issues suggest capacity problems, degrading hardware, packet loss, or load balancing problems rather than complete failures. Objective 6.2"
    },
    {
      "q": "Traceroute shows:",
      "options": [
        "DNS records",
        "Network path and latency to each hop",
        "Open ports",
        "Disk usage"
      ],
      "answer": 1,
      "explanation": "Traceroute displays each router hop between source and destination with response times, identifying where delays or failures occur. Objective 6.2"
    },
    {
      "q": "Security group misconfiguration typically causes:",
      "options": [
        "Slow performance",
        "Blocked connections even with network connectivity",
        "No impact",
        "DNS issues"
      ],
      "answer": 1,
      "explanation": "Security groups block specific ports/protocols at instance level; incorrect rules prevent connections even when network path is valid. Objective 6.2"
    },
    {
      "q": "Network ACL issues differ from security groups by:",
      "options": [
        "No difference",
        "ACLs are stateless and affect entire subnet",
        "Only protocol",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Network ACLs operate at subnet level with stateless rules requiring explicit inbound and outbound rules, unlike stateful security groups. Objective 6.2"
    },
    {
      "q": "If connection times out rather than being refused:",
      "options": [
        "Port is open",
        "Firewall is dropping packets or service not listening",
        "DNS issue",
        "Perfect connection"
      ],
      "answer": 1,
      "explanation": "Timeout suggests packets aren't reaching destination (firewall drop) or no service listening; refused means connection reached but was rejected. Objective 6.2"
    },
    {
      "q": "Connection refused error indicates:",
      "options": [
        "Firewall block",
        "Connection reached destination but port is closed",
        "DNS failure",
        "Network down"
      ],
      "answer": 1,
      "explanation": "Connection refused means packets reached host but no service listening on that port, indicating service or configuration issue. Objective 6.2"
    },
    {
      "q": "Routing table issues cause:",
      "options": [
        "Fast connections",
        "Packets sent to wrong destination or dropped",
        "No impact",
        "DNS problems"
      ],
      "answer": 1,
      "explanation": "Incorrect routing tables misdirect traffic to wrong paths or drop packets when no route exists to destination network. Objective 6.2"
    },
    {
      "q": "Subnet mask misconfiguration results in:",
      "options": [
        "Perfect networking",
        "Incorrect network/host boundaries and routing",
        "Faster speeds",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Wrong subnet masks cause systems to misidentify local vs remote hosts, sending traffic incorrectly to router or attempting direct connections. Objective 6.2"
    },
    {
      "q": "To verify listening ports on a server, use:",
      "options": [
        "ping",
        "netstat or ss commands",
        "traceroute",
        "nslookup"
      ],
      "answer": 1,
      "explanation": "netstat -an or ss -tuln show listening ports, active connections, and bound addresses for service verification and troubleshooting. Objective 6.2"
    },
    {
      "q": "MTU mismatch can cause:",
      "options": [
        "No issues",
        "Packet fragmentation or connection failures",
        "Faster speeds",
        "DNS problems"
      ],
      "answer": 1,
      "explanation": "MTU mismatches cause fragmentation or black hole scenarios where large packets are dropped, affecting specific applications differently. Objective 6.2"
    },
    {
      "q": "Asymmetric routing may cause:",
      "options": [
        "Better performance",
        "Stateful firewall drops or inspection issues",
        "No impact",
        "Faster connections"
      ],
      "answer": 1,
      "explanation": "Asymmetric routing (forward and return paths differ) causes problems with stateful firewalls expecting both directions through same path. Objective 6.2"
    },
    {
      "q": "VPN connectivity issues often involve:",
      "options": [
        "Only speed",
        "Authentication, encryption, routing, or firewall rules",
        "No common issues",
        "DNS only"
      ],
      "answer": 1,
      "explanation": "VPN problems stem from authentication failures, encryption mismatches, incorrect routing tables, or firewall blocking VPN protocols/ports. Objective 6.2"
    },
    {
      "q": "Load balancer health check failures result in:",
      "options": [
        "Better distribution",
        "Instances removed from rotation",
        "No impact",
        "Faster responses"
      ],
      "answer": 1,
      "explanation": "Failed health checks cause load balancers to stop routing traffic to unhealthy instances, potentially causing capacity issues if widespread. Objective 6.2"
    },
    {
      "q": "Peering connection issues between VPCs can involve:",
      "options": [
        "No issues",
        "Route table configurations or overlapping CIDR blocks",
        "Only speed",
        "DNS only"
      ],
      "answer": 1,
      "explanation": "VPC peering requires non-overlapping CIDR blocks and route table entries in both VPCs pointing to peering connection. Objective 6.2"
    },
    {
      "q": "NAT gateway problems affect:",
      "options": [
        "Internal communication",
        "Outbound internet connectivity from private subnets",
        "Nothing",
        "Inbound only"
      ],
      "answer": 1,
      "explanation": "NAT gateway failures prevent private subnet resources from initiating outbound internet connections while not affecting inbound or internal traffic. Objective 6.2"
    },
    {
      "q": "Internet gateway issues impact:",
      "options": [
        "Internal only",
        "Public internet connectivity to/from VPC",
        "Private subnets",
        "No connectivity"
      ],
      "answer": 1,
      "explanation": "Internet gateway problems affect all internet-bound traffic; resources can't reach internet and internet can't reach public IPs in VPC. Objective 6.2"
    },
    {
      "q": "Bandwidth saturation causes:",
      "options": [
        "Better performance",
        "Packet loss, high latency, and slow transfers",
        "No impact",
        "Faster speeds"
      ],
      "answer": 1,
      "explanation": "Saturated bandwidth causes queuing delays, packet drops, and degraded performance for all traffic competing for limited capacity. Objective 6.2"
    },
    {
      "q": "High CPU utilization may indicate:",
      "options": [
        "Perfect health",
        "Insufficient capacity, inefficient code, or runaway processes",
        "Low usage",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Sustained high CPU suggests undersized instances, inefficient algorithms, memory leaks causing excessive GC, or runaway processes consuming resources. Objective 6.3"
    },
    {
      "q": "Memory pressure symptoms include:",
      "options": [
        "Fast performance",
        "High swap usage, OOM errors, and application crashes",
        "No impact",
        "Better caching"
      ],
      "answer": 1,
      "explanation": "Memory exhaustion causes swapping (severe slowdown), out-of-memory kills, application crashes, and system instability. Objective 6.3"
    },
    {
      "q": "Disk I/O bottlenecks manifest as:",
      "options": [
        "CPU issues",
        "High latency, queue depths, and slow response times",
        "Network problems",
        "No symptoms"
      ],
      "answer": 1,
      "explanation": "Storage bottlenecks show high IOPS utilization, queue depths, elevated latency, and slow application response times for disk operations. Objective 6.3"
    },
    {
      "q": "Network latency issues affect:",
      "options": [
        "Only local operations",
        "Distributed applications and user experience",
        "Disk only",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "High latency increases response times, particularly impacting chatty applications, microservices, and geographically distributed systems. Objective 6.3"
    },
    {
      "q": "Database query performance issues often result from:",
      "options": [
        "Too much memory",
        "Missing indexes, poor query design, or lock contention",
        "Perfect optimization",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Slow queries stem from missing indexes, inefficient query patterns, full table scans, locking, or outdated statistics. Objective 6.3"
    },
    {
      "q": "Application response time degradation could indicate:",
      "options": [
        "Everything perfect",
        "Resource constraints, code issues, or dependency problems",
        "Better performance",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Degraded response times suggest resource saturation, inefficient code, external dependency slowness, or increasing data volumes. Objective 6.3"
    },
    {
      "q": "Memory leaks gradually cause:",
      "options": [
        "Better performance",
        "Increasing memory usage until exhaustion and crashes",
        "Freed memory",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Memory leaks progressively consume memory as applications fail to release allocated memory, eventually causing OOM errors and crashes. Objective 6.3"
    },
    {
      "q": "Caching can improve performance by:",
      "options": [
        "Slowing requests",
        "Reducing database queries and external API calls",
        "Increasing latency",
        "No benefit"
      ],
      "answer": 1,
      "explanation": "Caching stores frequently accessed data in fast storage (Redis, Memcached), reducing expensive database/API calls and improving response times. Objective 6.3"
    },
    {
      "q": "N+1 query problem causes:",
      "options": [
        "Better performance",
        "Excessive database queries degrading performance",
        "Fewer queries",
        "No impact"
      ],
      "answer": 1,
      "explanation": "N+1 queries execute one query for list then separate queries for each item, causing hundreds of queries where one would suffice. Objective 6.3"
    },
    {
      "q": "Connection pool exhaustion results in:",
      "options": [
        "Faster connections",
        "Applications unable to get database connections",
        "More connections",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Exhausted connection pools cause applications to wait or fail when requesting database connections, degrading performance or causing errors. Objective 6.3"
    },
    {
      "q": "Proper instance sizing requires considering:",
      "options": [
        "Random selection",
        "CPU, memory, network, storage needs, and growth",
        "Smallest always",
        "Largest always"
      ],
      "answer": 1,
      "explanation": "Right-sizing analyzes workload requirements across all dimensions (CPU, RAM, network, IOPS) plus growth projections for optimal cost-performance. Objective 6.3"
    },
    {
      "q": "Auto-scaling helps performance by:",
      "options": [
        "Reducing capacity",
        "Adding resources during demand spikes",
        "Fixed capacity only",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Auto-scaling dynamically adds capacity during high demand and removes it during low demand, maintaining performance while optimizing costs. Objective 6.3"
    },
    {
      "q": "Load balancer algorithms distribute traffic based on:",
      "options": [
        "Random only",
        "Round-robin, least connections, or response time",
        "No distribution",
        "Single server"
      ],
      "answer": 1,
      "explanation": "Algorithms like round-robin, least connections, IP hash, or least response time distribute requests optimizing for different scenarios. Objective 6.3"
    },
    {
      "q": "CDN usage improves performance by:",
      "options": [
        "Centralizing content",
        "Caching content closer to users geographically",
        "Slowing delivery",
        "No benefit"
      ],
      "answer": 1,
      "explanation": "CDNs cache static content at edge locations worldwide, reducing latency by serving users from geographically nearby servers. Objective 6.3"
    },
    {
      "q": "Database read replicas improve:",
      "options": [
        "Write performance",
        "Read scalability by distributing queries",
        "Nothing",
        "Write capacity only"
      ],
      "answer": 1,
      "explanation": "Read replicas offload read queries from primary database, improving read scalability and performance while primary handles writes. Objective 6.3"
    },
    {
      "q": "Index fragmentation causes:",
      "options": [
        "Better performance",
        "Degraded query performance over time",
        "Faster queries",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Fragmented indexes require more I/O operations to traverse, degrading query performance; regular maintenance rebuilds indexes for optimal structure. Objective 6.3"
    },
    {
      "q": "API rate limiting prevents:",
      "options": [
        "All requests",
        "Single clients from overwhelming service",
        "Performance issues",
        "No purpose"
      ],
      "answer": 1,
      "explanation": "Rate limiting protects services from abuse or unintentional overload by restricting request rates per client, maintaining performance for all. Objective 6.3"
    },
    {
      "q": "Monitoring 95th percentile latency shows:",
      "options": [
        "Average only",
        "Performance experienced by most users excluding outliers",
        "Worst case",
        "Best case"
      ],
      "answer": 1,
      "explanation": "P95 latency indicates 95% of requests complete within this time, better representing typical user experience than average (affected by outliers). Objective 6.3"
    },
    {
      "q": "Thread pool sizing affects:",
      "options": [
        "Nothing",
        "Concurrent request handling capacity",
        "Disk space",
        "Network only"
      ],
      "answer": 1,
      "explanation": "Thread pool size determines concurrent processing capacity; too small limits throughput, too large causes overhead and resource contention. Objective 6.3"
    },
    {
      "q": "Async processing improves performance by:",
      "options": [
        "Slowing responses",
        "Not blocking on long-running operations",
        "Synchronous only",
        "No benefit"
      ],
      "answer": 1,
      "explanation": "Asynchronous processing queues long operations (email, reports) and returns immediately, improving response times and resource utilization. Objective 6.3"
    },
    {
      "q": "Database connection timeout issues suggest:",
      "options": [
        "Perfect performance",
        "Database overload or network problems",
        "Fast queries",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Connection timeouts indicate database can't accept connections (overloaded, max connections reached) or network connectivity issues. Objective 6.3"
    },
    {
      "q": "Static content compression reduces:",
      "options": [
        "Transfer speed",
        "Bandwidth usage and transfer time",
        "Performance",
        "File quality"
      ],
      "answer": 1,
      "explanation": "Gzip/Brotli compression reduces file sizes significantly, decreasing bandwidth consumption and accelerating page load times. Objective 6.3"
    },
    {
      "q": "Session affinity (sticky sessions) may be needed when:",
      "options": [
        "Stateless apps",
        "Applications store session data locally",
        "No sessions",
        "Distributed cache"
      ],
      "answer": 1,
      "explanation": "Sticky sessions route users to same instance when applications store session data locally rather than in shared store. Objective 6.3"
    },
    {
      "q": "Microservices performance issues often involve:",
      "options": [
        "Single point",
        "Service-to-service communication overhead and latency",
        "No communication",
        "Perfect performance"
      ],
      "answer": 1,
      "explanation": "Microservices introduce network latency for inter-service calls; proper service mesh, caching, and async patterns mitigate this. Objective 6.3"
    },
    {
      "q": "Container resource limits prevent:",
      "options": [
        "All usage",
        "Single container consuming all host resources",
        "Performance",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "CPU and memory limits prevent noisy neighbor problems where one container starves others of resources on shared host. Objective 6.3"
    },
    {
      "q": "Unauthorized access attempts should trigger:",
      "options": [
        "No action",
        "Security alerts and investigation",
        "Ignore completely",
        "Disable monitoring"
      ],
      "answer": 1,
      "explanation": "Failed authentication attempts, especially multiple rapid attempts, indicate potential brute force attacks requiring investigation and response. Objective 6.4"
    },
    {
      "q": "Compromised credentials typically lead to:",
      "options": [
        "Nothing",
        "Unauthorized access and potential data breaches",
        "Better security",
        "No risk"
      ],
      "answer": 1,
      "explanation": "Stolen credentials enable attackers to access systems legitimately, bypass security controls, and exfiltrate data or deploy malware. Objective 6.4"
    },
    {
      "q": "Certificate validation errors indicate:",
      "options": [
        "Perfect security",
        "Expired, invalid, or untrusted certificates",
        "No issues",
        "Better encryption"
      ],
      "answer": 1,
      "explanation": "Certificate errors suggest expiration, self-signed certs, hostname mismatch, or untrusted CA, preventing secure encrypted connections. Objective 6.4"
    },
    {
      "q": "Unusual outbound traffic may indicate:",
      "options": [
        "Normal operations",
        "Data exfiltration or command-and-control communication",
        "Better performance",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Unexpected outbound connections, especially to unusual destinations or ports, suggest compromised systems communicating with attackers. Objective 6.4"
    },
    {
      "q": "IAM permission issues cause:",
      "options": [
        "No impact",
        "Access denied errors for legitimate operations",
        "Better security always",
        "Performance issues"
      ],
      "answer": 1,
      "explanation": "Overly restrictive IAM policies prevent authorized actions; too permissive policies create security risks - proper balance required. Objective 6.4"
    },
    {
      "q": "Encryption in transit failures result in:",
      "options": [
        "Better performance",
        "Unencrypted data transmission exposing sensitive information",
        "Faster speeds",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Missing TLS/SSL encryption exposes data to interception on network, violating compliance and enabling man-in-middle attacks. Objective 6.4"
    },
    {
      "q": "Security group rules should follow:",
      "options": [
        "Allow all traffic",
        "Principle of least privilege",
        "Block everything",
        "Random rules"
      ],
      "answer": 1,
      "explanation": "Security groups should allow only necessary traffic to/from specific sources on required ports, minimizing attack surface. Objective 6.4"
    },
    {
      "q": "Public S3 buckets with sensitive data represent:",
      "options": [
        "Best practice",
        "Critical security misconfiguration and data exposure",
        "Performance optimization",
        "No issue"
      ],
      "answer": 1,
      "explanation": "Publicly accessible buckets with sensitive data cause data breaches; proper bucket policies and ACLs are essential for data protection. Objective 6.4"
    },
    {
      "q": "Failed MFA challenges indicate:",
      "options": [
        "No issue",
        "Potential unauthorized access attempts",
        "User error only",
        "Better security"
      ],
      "answer": 1,
      "explanation": "Multiple failed MFA attempts suggest attackers have passwords but not second factor, warranting investigation and potential account lockout. Objective 6.4"
    },
    {
      "q": "Privilege escalation attacks attempt to:",
      "options": [
        "Reduce access",
        "Gain higher-level permissions than authorized",
        "Improve security",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Privilege escalation exploits vulnerabilities or misconfigurations to gain administrative access from regular user accounts. Objective 6.4"
    },
    {
      "q": "API authentication failures could result from:",
      "options": [
        "Perfect security",
        "Invalid keys, expired tokens, or incorrect permissions",
        "No authentication",
        "Better performance"
      ],
      "answer": 1,
      "explanation": "API authentication fails due to invalid/expired keys, incorrect token format, missing permissions, or revoked credentials. Objective 6.4"
    },
    {
      "q": "Ransomware attacks typically:",
      "options": [
        "Improve security",
        "Encrypt data and demand payment for decryption",
        "Delete nothing",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Ransomware encrypts victim data making it inaccessible, demanding ransom payment; requires robust backups and security controls for prevention. Objective 6.4"
    },
    {
      "q": "SQL injection vulnerabilities allow attackers to:",
      "options": [
        "Nothing",
        "Execute arbitrary database queries",
        "Improve performance",
        "Enhance security"
      ],
      "answer": 1,
      "explanation": "SQL injection exploits inadequate input validation, enabling attackers to execute malicious queries, access data, or modify databases. Objective 6.4"
    },
    {
      "q": "Cross-site scripting (XSS) enables:",
      "options": [
        "Better security",
        "Injecting malicious scripts into web pages",
        "Performance gains",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "XSS injects malicious JavaScript into pages viewed by other users, stealing cookies, credentials, or performing actions as victim. Objective 6.4"
    },
    {
      "q": "Security audit logs should be:",
      "options": [
        "Deleted immediately",
        "Protected from modification and retained appropriately",
        "Public",
        "Ignored"
      ],
      "answer": 1,
      "explanation": "Audit logs require protection against tampering, appropriate retention periods, and monitoring to support investigations and compliance. Objective 6.4"
    },
    {
      "q": "Insider threats are difficult because:",
      "options": [
        "They don't exist",
        "Authorized users with legitimate access misuse privileges",
        "Easy to detect",
        "No risk"
      ],
      "answer": 1,
      "explanation": "Insider threats exploit legitimate access making detection harder; require monitoring user behavior, access patterns, and data exfiltration. Objective 6.4"
    },
    {
      "q": "Zero-day vulnerabilities are dangerous because:",
      "options": [
        "Already patched",
        "No patch available and actively exploited",
        "Well-known",
        "Low risk"
      ],
      "answer": 1,
      "explanation": "Zero-days have no vendor patches available yet; organizations must use compensating controls (WAF, IPS, segmentation) until patches release. Objective 6.4"
    },
    {
      "q": "Security incident response should include:",
      "options": [
        "Ignore incidents",
        "Containment, eradication, recovery, and lessons learned",
        "Delete everything",
        "No response"
      ],
      "answer": 1,
      "explanation": "Incident response contains threat spread, eradicates attacker presence, recovers systems, and documents lessons for improvement. Objective 6.4"
    },
    {
      "q": "Security group rules showing 0.0.0.0/0 should be:",
      "options": [
        "Always used",
        "Carefully reviewed as they allow all sources",
        "Ignored",
        "Preferred"
      ],
      "answer": 1,
      "explanation": "0.0.0.0/0 allows traffic from anywhere on internet; should be restricted to specific sources except for intended public services. Objective 6.4"
    },
    {
      "q": "Failed deployments often result from:",
      "options": [
        "Perfect code",
        "Dependency conflicts, configuration errors, or resource constraints",
        "No issues",
        "Better performance"
      ],
      "answer": 1,
      "explanation": "Deployment failures stem from missing dependencies, configuration mismatches between environments, insufficient resources, or breaking changes. Objective 6.5"
    },
    {
      "q": "Version mismatch issues occur when:",
      "options": [
        "Versions match",
        "Incompatible component versions are deployed together",
        "Everything updated",
        "No versions"
      ],
      "answer": 1,
      "explanation": "Incompatible library, API, or service versions cause integration failures; version pinning and testing prevent mismatches. Objective 6.5"
    },
    {
      "q": "Configuration drift between environments causes:",
      "options": [
        "Identical behavior",
        "Works in test but fails in production scenarios",
        "Better reliability",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Environment differences (configs, dependencies, resources) cause code working in dev/test to fail in production unexpectedly. Objective 6.5"
    },
    {
      "q": "Rollback procedures should be:",
      "options": [
        "Never planned",
        "Tested and documented before deployment",
        "Improvised",
        "Ignored"
      ],
      "answer": 1,
      "explanation": "Pre-tested rollback procedures enable quick recovery from failed deployments, minimizing downtime and user impact. Objective 6.5"
    },
    {
      "q": "Deployment health checks verify:",
      "options": [
        "Nothing",
        "Application functionality after deployment",
        "Only startup",
        "Disk space"
      ],
      "answer": 1,
      "explanation": "Post-deployment health checks validate application responds correctly, dependencies work, and no regressions before routing traffic. Objective 6.5"
    },
    {
      "q": "Container image issues may involve:",
      "options": [
        "Perfect images",
        "Wrong base image, missing dependencies, or outdated versions",
        "No problems",
        "Better performance"
      ],
      "answer": 1,
      "explanation": "Container problems include wrong base OS, missing libraries, incorrect versions, or vulnerabilities in base images. Objective 6.5"
    },
    {
      "q": "Infrastructure provisioning failures could result from:",
      "options": [
        "Perfect templates",
        "Resource limits, quotas, or syntax errors in IaC",
        "No issues",
        "Better capacity"
      ],
      "answer": 1,
      "explanation": "IaC failures stem from quota limits, insufficient permissions, syntax errors, or dependency issues in templates. Objective 6.5"
    },
    {
      "q": "Database migration issues during deployment may cause:",
      "options": [
        "No impact",
        "Schema incompatibilities or data corruption",
        "Better performance",
        "Faster queries"
      ],
      "answer": 1,
      "explanation": "Migration problems include schema changes incompatible with code, failed migrations leaving inconsistent state, or data loss. Objective 6.5"
    },
    {
      "q": "Deployment in wrong order can cause:",
      "options": [
        "No issues",
        "Dependency failures when consumers deploy before providers",
        "Better results",
        "Performance gains"
      ],
      "answer": 1,
      "explanation": "Deploying services in wrong order causes failures when consumers expect new API that providers haven't deployed yet. Objective 6.5"
    },
    {
      "q": "Failed smoke tests after deployment indicate:",
      "options": [
        "Success",
        "Critical functionality broken by deployment",
        "Performance gains",
        "Nothing"
      ],
      "answer": 1,
      "explanation": "Failed smoke tests show deployment broke basic functionality, requiring immediate rollback or hotfix to restore service. Objective 6.5"
    },
    {
      "q": "Resource quota limits cause:",
      "options": [
        "No issues",
        "Inability to provision additional resources",
        "Better performance",
        "Unlimited capacity"
      ],
      "answer": 1,
      "explanation": "Cloud quotas limit resource creation (VMs, IPs, storage); hitting limits prevents scaling, requiring quota increase requests. Objective 6.6"
    },
    {
      "q": "Unexpected cloud costs often result from:",
      "options": [
        "Perfect optimization",
        "Untagged resources, data egress, or forgotten resources",
        "No impact",
        "Free services"
      ],
      "answer": 1,
      "explanation": "Cost surprises come from orphaned resources, data transfer charges, oversized instances, or resources left running unnecessarily. Objective 6.6"
    },
    {
      "q": "API rate limiting errors indicate:",
      "options": [
        "No usage",
        "Exceeding allowed request rates",
        "Better performance",
        "Unlimited access"
      ],
      "answer": 1,
      "explanation": "Rate limit errors show application exceeding API call quotas, requiring implementation of backoff, caching, or quota increase. Objective 6.6"
    },
    {
      "q": "Service availability issues may stem from:",
      "options": [
        "Perfect design",
        "Single availability zone deployment or lack of redundancy",
        "Better reliability",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Availability problems result from single AZ/region deployment, lack of redundancy, or not utilizing cloud high-availability features. Objective 6.6"
    },
    {
      "q": "Multi-region deployment issues include:",
      "options": [
        "No challenges",
        "Data replication lag, latency, and consistency challenges",
        "Perfect synchronization",
        "No differences"
      ],
      "answer": 1,
      "explanation": "Multi-region architectures face replication delays, cross-region latency, data consistency complexities, and increased costs. Objective 6.6"
    },
    {
      "q": "Vendor lock-in concerns involve:",
      "options": [
        "Easy migration",
        "Difficulty moving to other providers due to proprietary services",
        "No issues",
        "Better portability"
      ],
      "answer": 1,
      "explanation": "Using proprietary services creates dependencies making migration costly/difficult; portable solutions using standards reduce lock-in. Objective 6.6"
    },
    {
      "q": "Cloud service outages impact:",
      "options": [
        "Nothing",
        "All resources depending on affected service",
        "Single resource",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Cloud provider outages affect all dependent services in region/zone; multi-region and multi-cloud strategies mitigate this risk. Objective 6.6"
    },
    {
      "q": "Insufficient monitoring leads to:",
      "options": [
        "Better visibility",
        "Delayed problem detection and resolution",
        "Perfect awareness",
        "No issues"
      ],
      "answer": 1,
      "explanation": "Inadequate monitoring causes problems to go undetected, delaying response and potentially impacting users before awareness. Objective 6.6"
    },
    {
      "q": "Shadow IT in cloud creates:",
      "options": [
        "Better control",
        "Unmanaged resources, security risks, and cost issues",
        "No problems",
        "Perfect governance"
      ],
      "answer": 1,
      "explanation": "Unmanaged cloud usage creates security vulnerabilities, compliance issues, cost surprises, and lack of visibility. Objective 6.6"
    },
    {
      "q": "Cloud skills gap results in:",
      "options": [
        "Perfect operations",
        "Misconfigurations, inefficient designs, and security issues",
        "Better architecture",
        "No impact"
      ],
      "answer": 1,
      "explanation": "Insufficient cloud expertise causes suboptimal architectures, security misconfigurations, cost inefficiencies, and operational difficulties. Objective 6.6"
    }
  ]
}